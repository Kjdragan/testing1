## Universality of Layer-Level Entropy-Weighted Quantization Beyond Model Architecture and Size

**Alireza Behtash** **Marijan Fofonjka** **Ethan Baird**
**Tyler Mauer** **Hossein Moghimifam** **David Stout** **Joel Dennison**


webAI
```
{alireza.behtash,marijan.fofonjka,ethan}@webai.com
 {tyler,hossein,David,joel.dennison}@webai.com

```

### Abstract


We present a novel approach to selective model quantization that transcends the
limitations of architecture-specific and size-dependent compression methods for
Large Language Models (LLMs) using Entropy-Weighted Quantization (EWQ).
By analyzing the entropy distribution across transformer blocks, EWQ determines
which blocks can be safely quantized without causing significant performance
degradation, independent of model architecture or size. Our method outperforms
uniform quantization approaches, maintaining Massive Multitask Language Understanding (MMLU) accuracy scores within 0.5% of unquantized models while
reducing memory usage by up to 18%. We demonstrate the effectiveness of EWQ
across multiple architectures—from 1.6B to 70B parameters—showcasing consistent improvements in the quality-compression trade-off regardless of model
scale or architectural design. A surprising finding of EWQ is its ability to reduce
perplexity compared to unquantized models, suggesting the presence of beneficial regularization through selective precision reduction. This improvement holds
across different model families, indicating a fundamental relationship between
layer-level entropy and optimal precision requirements. Additionally, we introduce FastEWQ, a rapid method for entropy distribution analysis that eliminates
the need for loading model weights. This technique leverages universal characteristics of entropy distribution that persist across various architectures and scales,
enabling near-instantaneous quantization decisions while maintaining 80% classification accuracy with full entropy analysis. Our results demonstrate that effective
quantization strategies can be developed independently of specific architectural
choices or model sizes, opening new possibilities for efficient LLM deployment.

### 1 Introduction


The widespread adoption of LLMs has been constrained by their substantial computational and
memory requirements, particularly as model sizes continue to grow exponentially [Brown et al.,
2020, Chowdhery et al., 2022, Hoffmann et al., 2022]. As LLMs become increasingly integral to
various applications, from natural language processing to automated reasoning, the need for efficient
deployment solutions becomes more critical. A typical LLM with 7-70B parameters requires 14140GB of memory in full precision, making deployment challenging even in well-resourced data
centers and practically impossible on edge devices or consumer hardware.

The memory bottleneck is particularly acute when serving multiple users or handling long context
windows. For instance, the key-value cache required for processing long sequences can consume
several gigabytes of additional memory per request. This challenge is compounded in production


-----

environments where multiple model instances must run concurrently to handle user traffic, leading
to substantial infrastructure costs and deployment complexity.

Neural network quantization has emerged as a promising approach to address these deployment challenges by reducing resource usage and accelerating inference through lowering the bit precision of
model parameters [Choi et al., 2018, Hubara et al., 2021, Yao et al., 2022, Park et al., 2022, Gholami
et al., 2022]. We can define quantization as a way of reducing data precision from typical 32-bit or
bfloat16 to smaller bits (<= 8-bit), which in turn lowers the size of the model and speeds up matrix
multiplications involved in the attention mechanism. While quantization offers significant memory
savings—potentially reducing model size by 75% or more—maintaining model performance under
reduced precision remains a fundamental challenge.

Quantization techniques can be broadly categorized into uniform precision quantization and mixed
_precision quantization. While uniform precision quantization is widely applied to reduce the size_
of transformer layers in LLMs, its indiscriminate application often leads to significant performance
degradation. This degradation occurs because different layers in transformer-based models exhibit
varying sensitivity to quantization, necessitating more nuanced approaches [Wang et al., 2018, Cai
and Vasconcelos, 2020, Zadeh and Moshovos, 2020, Ganesh et al., 2021]. For example, early layers processing raw input tokens and final layers producing output logits typically require higher
precision than intermediate layers.

Recent research has focused on addressing the challenges posed by outlier activations, which represent a key impediment to effective uniform low-precision quantization. Mixed-precision quantization has shown promise in mitigating this issue by maintaining outlier channels at higher precision

[Dettmers et al., 2022a, Zhao et al., 2024, Ashkboos et al., 2023, Zhao et al., 2024]. Another emerging approach is invariant random rotation, which aims to suppress outliers and enable more effective
uniform low-precision quantization [Ashkboos et al., 2024, Liu et al., 2024, Wei et al., 2023]. While
both methods improve the signal-to-quantization noise ratio and reduce quantization errors locally,
they have yet to demonstrate substantial performance advantages over 16-bit precision models. For
instance, SpinQuant [Liu et al., 2024] applied at 4-bit precision to Llama-3-8b [Meta, 2024] shows
approximately 20% higher perplexity compared to the 16-bit baseline, despite significant optimization efforts.

Alternative approaches such as SmoothQuant [Xiao et al., 2024] and Activation-Aware Quantization
_(AWQ) [Yuan et al., 2023, Lin et al., 2024] have been developed to enable effective 8-bit quantization_
of both weights and activations. These methods employ sophisticated techniques, including offline
migration of quantization difficulty from activations to weights and channel equalization. However,
they typically require access to the entire model for activation distribution analysis, making them impractical in resource-constrained environments [Kim et al., 2023a]. This is why weight-only quantization represents a more suitable use case for model compression, exemplified by methods like
_GPTQ [Frantar et al., 2022] or FineQuant [Kim et al., 2023c]. GPTQ converts quantized weights_
to float16 during inference for matrix multiplication operations and FineQuant uses a fine-grained
quantization algorithm that incorporates group-wise quantization and adaptive selection of granularity. While these approaches can achieve performance gains through reduced data loading with
minimal accuracy loss, they present practical limitations in production environments specially in
distributed settings. For example, handling long context lengths and batch processing increases the
memory footprint of key-value (KV) cache substantially. Or uniform quantization of all transformer
blocks happen to increase perplexity significantly. Still, even with these challenges the weight-only
quantization seems to be a promising approach for the purposes of this paper. So we would like to
pose the following important question.

**Question: Is it possible to devise an architecture-agnostic, optimal post-training weight-only quan-**
_tization method that, given resource constraints, produces an on-the-fly[1]_ _quantized model that re-_
_mains competitive with the original-precision model while delivering fast inference and memory_
_efficiency?_

To provide a production-grade answer, we introduce Entropy-Weighted Quantization (EWQ), a systematic framework for selective model compression that preserves performance while substantially
reducing memory requirements. Our approach extends the theoretical foundations of informationtheoretic neural network compression [Park et al., 2017, Xu et al., 2018, Dong et al., 2019c] to

1By ‘on-the-fly’ we mean O(1) time complexity.


-----

address the unique challenges of large language models. While entropy-based quantization has
proven effective for traditional machine learning architectures [Park et al., 2017] and has recently
shown promise in quantization-aware training of LLMs [Shen et al., 2024], its application to runtime
weight-only quantization remains unexplored. This gap is particularly significant given that LLMs
exhibit markedly heterogeneous entropy distributions across their transformer layers—a characteristic that distinguishes them from smaller neural architectures and necessitates more sophisticated
quantization strategies.

Our approach’s distinguishing feature is its ability to facilitate efficient on-the-fly quantization, making it particularly well-suited for deployment on consumer-grade hardware with limited resources.
Unlike traditional mixed-precision methods that demand significant computational overhead for activation analysis [Dettmers et al., 2022a, Kim et al., 2023b], EWQ’s weight-centric approach allows
for rapid deployment while maintaining adaptability to different hardware constraints. By intelligently mapping the entropy characteristics of LLM weights to appropriate precision levels, EWQ
achieves an optimal balance between model performance and resource efficiency. This ensures that
quantized models maintain competitive performance with their full-precision counterparts while significantly reducing memory footprint and accelerating inference speed. The architecture-agnostic
nature of EWQ is further demonstrated by the emergence of a universal approximator for the quantization of transformer layers, enabling rapid quantization—termed FastEWQ—without the need to
load weights.

### 2 Background and Related Work

**2.1** **Model Quantization**

In more technical terms, model (neural network) quantization refers to the process of reducing the
numerical precision of weights and activations from 32-bit floating point to lower bit-width representations (typically 8-bit or 4-bit), quantization achieves significant reductions in memory footprint
and computational requirements while preserving model functionality [Choi et al., 2018, Hubara
et al., 2021]. Modern quantization approaches can be broadly categorized into two paradigms:

   - Uniform quantization: Applies identical precision reduction across all model components, enabling straightforward implementation but often resulting in significant accuracy
degradation for sensitive layers [Yao et al., 2022].

    - Mixed-precision quantization: Allocates higher precision to critical layers identified
through sensitivity analysis, achieving better accuracy preservation at the cost of increased
implementation complexity [Dettmers et al., 2022a, Zhao et al., 2024].

Recent advances in post-training quantization (PTQ) have demonstrated particular promise for LLM
deployment. Dettmers et al. [2022a] introduced layer-wise adaptive mixed precision for GPT-3
models, maintaining 16-bit precision only for outlier-dominated attention heads. Frantar et al. [2022]
developed a second-order quantization approach that minimizes layer-wise reconstruction errors,
enabling 4-bit quantization of LLaMA models with minimal accuracy loss. The BitLinear layer
proposed by Ashkboos et al. [2023] achieves extreme 1.58-bit quantization through entropy-driven
logarithmic representations, though with increased computational overhead.

**2.2** **Block Sensitivity Analysis**

The transformer architecture’s layered structure exhibits significant heterogeneity in quantization
sensitivity across blocks. Early work by Devlin et al. [2019] demonstrated that initial encoder layers in BERT models capture fundamental syntactic features highly sensitive to precision reduction,
while deeper layers encode semantic relationships more tolerant of quantization. This phenomenon
was formalized by Dong et al. [2019a,b], Shen et al. [2019] through Hessian-based sensitivity analysis, establishing that attention blocks typically require 2-4× higher precision than feedforward layers.

Three key strategies have emerged for leveraging this sensitivity gradient:

    - Progressive quantization: Gradually increases quantization intensity from output to input
layers, preserving early layer precision [Zadeh and Moshovos, 2020].


-----

   - Attention-aware allocation: Assigns higher precision to query/key matrices than value
projections to maintain attention fidelity [Passban et al., 2021].

    - Task-adaptive thresholds: Dynamically adjusts layer precision based on downstream task
gradients [Kim et al., 2023a].

In particular, Yuan et al. [2023] introduced activation-aware singular value decomposition (ASVD),
a post-training compression that addresses challenges in low-rank factorization by managing activation outliers through the scaling of the weight matrix based on the activation distributions, enhancing the accuracy of decomposition. Additionally, it employs an iterative calibration process to
optimize layer-specific decomposition, considering the varying sensitivity of different LLM layers.
Experiments demonstrate that ASVD can compress networks by 10-20% without compromising
performance. Furthermore, by applying low-rank decomposition to projection matrices in the selfattention module, ASVD achieves up to 50% reduction in KV cache memory requirements without
performance degradation.

These developments underscore the importance of understanding and leveraging block sensitivity in
transformer architectures to inform effective compression strategies.

**2.3** **Information-Theoretic Approaches**

The relationship between parameter entropy and quantization robustness originates from fundamental rate-distortion theory [Cover and Thomas, 2006], where entropy establishes theoretical bounds
on lossy compression. In deep learning, Park et al. [2017] first operationalized this connection by
demonstrating that weight matrices with Shannon entropy H(W ) ≤ 4 bits/parameter could withstand 4-bit quantization with less than 1% accuracy drop. Their analysis revealed that entropy correlates with parameter redundancy—layers learning simpler patterns (e.g., smooth feature detectors)
naturally exhibit lower entropy and greater quantization tolerance [Jin et al., 2021].

Recent advances extended this framework to activation entropy. By leveraging activation entropy,
methods optimally balance computational efficiency and model accuracy for edge deployment. Using 4-/8-bit multipliers, they employ entropy-driven thresholds to assign 8-bit quantization to highentropy activations and 4-bit to low-entropy regions, maintaining distortion under 0.5 KL divergence. Adaptive 4/8-bit quantization with 4-bit weights achieves superior accuracy compared to
static non-power-of-two baselines. The entropy-regularized objective prioritizes high-information
activations, improving performance by 1.2–3.4% across containment ratios (ρ = 0-100). This enables a 2.37× on-device speedup, bridging the efficiency-accuracy trade-off. Our method advances
these foundations while addressing three persistent challenges discussed next.

Calculating the full entropy for all n transformer layers leads to a complexity of O(n), which becomes prohibitive in billion-parameter models. We mitigate this by focusing solely on weightonly quantization, maintaining O(n) time and space complexity. Another challenge is architecturespecific sensitivity. Existing thresholds, such as Park’s 4-bit boundary [Park et al., 2017], fail for
heterogeneous architectures like Mixture of Experts (MoE) models. Instead, we derive architectureagnostic criteria using FastEWQ to generalize across multiple model families.

A crucial step for any post-training quantization is downloading the model weights, which significantly limits access to hard disk resources. Static quantization policies [Ashkboos et al., 2023] cannot adapt to varying cluster resources. Our on-the-fly optimization framework operates in O(n) time
per resource update for EWQ and O(1) time with FastEWQ, maintaining Pareto-optimal accuracyefficiency trade-offs [Abdolrashidi et al., 2021]. This synthesis enables the first informationtheoretic quantization system that simultaneously achieves sublinear time entropy estimation, crossarchitecture validity, and real-time adaptation to deployment constraints.

**2.4** **Challenges in Uniform Quantization**

Uniform quantization techniques apply the same precision reductions across all layers and blocks
of the model, presenting several significant challenges in practice. Different components of a model
exhibit varying levels of sensitivity to precision reductions, making uniform approaches particularly
problematic. When all layers are quantized uniformly, the performance of sensitive layers often degrades substantially, leading to reduced model accuracy, coherence, and perplexity. While uniform
quantization successfully reduces memory usage and model size, these benefits come at the cost


-----

of significantly impacting downstream performance metrics. For example, fully quantized models
using int8 (8-bit) or nf4 (4-bit) precision frequently demonstrate notable declines in crucial metrics
such as MMLU scores. Furthermore, the rigid nature of uniform quantization provides minimal flexibility for optimization based on task-specific or architecture/size-specific requirements, rendering
it particularly suboptimal for specialized applications such as question-answering (QA) or natural
language inference.

**2.5** **The Case for Mixed Quantization**

Our preliminary analysis of entropy distributions across transformer blocks, combined with benchmarking the effects of quantization precision on model performance, has revealed several fundamental insights that challenge existing assumptions about model compression. As depicted in Figure 1,
transformer blocks exhibit variability in their entropy values, which has profound implications for
quantization strategies. Specifically, lower-entropy blocks demonstrate reduced sensitivity to quantization, whereas high-entropy blocks are crucial for maintaining overall model performance. This
phenomenon can be attributed to the natural development of hierarchical information structures
within transformer blocks during the training process.

Garnier-Brun et al. (2024) introduced a hierarchical filtering procedure for generative models of
sequences on trees, allowing for controlled tuning of positional correlations in the data. Their
study provides evidence that vanilla encoder-only transformers can approximate exact inference
algorithms when trained on root classification and masked language modeling tasks. They observed
that correlations at larger distances, corresponding to increasing layers of the hierarchy, are sequentially incorporated by the network during training. This suggests that transformer models inherently
develop hierarchical structures, with some layers capturing local information (lower entropy) and
others capturing more complex, global information (higher entropy).

Our analysis indicates that the entropy distribution does not follow a universal pattern. Crucially, a
layer’s entropy—regardless of its position—relates to its handling of global versus local information.
Understanding this relationship is important for developing effective quantization strategies with
selective precision that preserve model performance while reducing computational complexity.

Our initial motivation was backed by utilizing the Tonic Validate library to conduct accuracy
benchmarks on QA datasets, including MMLU, with various quantization configurations. Mixed
precision approaches, utilizing 8-bit precision for 60% of blocks and 4-bit for the remaining 40%
randomly, achieve the highest answer-similarity at 52% while maintaining competitive answer consistency at 22%. Full 8-bit quantization shows improved answer consistency at 26% but demonstrates lower similarity metrics, suggesting that a mixed quantization can edge out a higher precision
global quantization. Complete 4-bit quantization performs poorest across all metrics, emphasizing
its unsuitability for precision-demanding tasks. Table 1 shows a summary of these comparisons.

Figure 1: Entropy distribution of Meta-Llama-3.1-8B-Instruct model weights with block number. The optimal quantization requires those blocks with lower entropy to be quantized first.


-----

|Configuration|Similarity|Consistency|Remarks|
|---|---|---|---|
|Mixed Precision (8-bit: 60%, 4-bit: 40%)|52%|22%|High similarity with competitive consistency.|
|Fully 8-bit Quantization|<52%|26%|Better consistency but lower simi- larity than mixed precision.|
|Fully 4-bit Quantization|<10%|<10%|Poor performance on both metrics; unsuitable for high-precision tasks.|


Table 1: Accuracy benchmarks on QA datasets were evaluated using the Tonic Validate library in
the initial phase of the project. The early indication of lower perplexity was observed when similarity
was highest for mixed precision, which became a motivating factor to pursue mixed quantization
further.

In the context of QA tasks, the ResQ method applies mixed-precision quantization to LLMs,
demonstrating that such approaches can maintain model performance while reducing computational
costs [Saxena et al., 2024]. Similarly, the SliM-LLM framework employs a salience-driven mixedprecision quantization scheme, achieving efficient LLMs with high accuracy [Huang et al., 2025].
These findings align with our later observations that mixed-precision quantization strategies can
effectively balance model efficiency and performance in QA tasks.

Building upon insights into the role of entropy in an effective neural network quantization and selective precision of transformer layers, we focus on EWQ and its optimized variant, FastEWQ.
These methods enhance model efficiency by assigning precision levels to transformer blocks based
on their entropy characteristics. Moving forward, our empirical results will be derived using the
MMLU benchmark to align with community standards.

### 3 Methodology

In this section, we delve into the methodology for calculating the entropy of a neural network layer’s
weight matrix.

**3.1** **Entropy Analysis**

The entropy calculation for a neural network layer’s weight matrix involves three main steps: flattening the weights, applying the softmax function, and computing the entropy using an informationtheoretic approach. The mathematical representation for these calculations is as follows.

**3.1.1** **Weight Flattening**

Let the weight matrix of a neural network layer be denoted as W . The weights are flattened into a
one-dimensional array
_wflat = Flatten(W_ ),
where wflat denotes the resulting flattened array of weights, and its length n represents the total
number of parameters in the matrix.

**3.1.2** **Softmax Normalization**

To transform the flattened weights into a probability distribution, the softmax function is applied

_e[w][flat][,i]_
_pi =_ _n_ for i = 1, . . ., n,
�j=1 _[e][w][flat][,j][,]_

where pi is the probability corresponding to the i[th] weight.

**3.1.3** **Entropy Calculation**

The entropy of the weight distribution is computed using the following formula

_n_

_H = −_ � _pi log(pi + ϵ),_

_i=1_


-----

where ϵ is a small constant (e.g., 0.01) added for numerical stability.

**3.2** **Block Entropy Calculation**

For a transformer block, which contains multiple weight matrices from linear and embedding layers,
the total weighted entropy of the block is computed using the following formula

�i _[|][W][i][|][H][(][W][i][)]_
_Hblock =_ _,_

�i _[|][W][i][|]_


where Hblock is the total entropy of the block, H(Wi) is the entropy of the i[th] weight, calculated as


_H(Wi) = −_


_ni_
� _pi,j log(pi,j + ϵ),_

_j=1_


where pi,j represents the normalized probabilities for the j[th] parameter in the i[th] weight matrix, and
_ϵ is a small constant for numerical stability, |Wi| is the number of parameters (or size) of the i[th]_
weight matrix, and ni is the number of parameters in Wi.

This formulation ensures that larger weights contribute more to the overall block entropy, providing
a weighted representation of the block’s variability. By incorporating the sizes of weight matrices
into the calculation, the approach captures the relative significance of each matrix in the transformer
block.

**3.3** **Block Selection Criteria**

Utilizing the block entropy in Eq. (3.2), we establish criteria for selecting transformer blocks for
quantization. The selection process involves several steps given below.

**3.3.1** **Sorting Blocks by Entropy**

After calculating the entropy Hblock for each transformer block, we sort the blocks in ascending
order of entropy. This allows us to prioritize lower-entropy blocks for more aggressive quantization
while preserving higher-entropy blocks in higher precision formats to maintain model performance.
Mathematically, we express this sorting process as

Sort(Hblocki) for _i = 1, . . ., N,_

where N is the total number of blocks, and Hblocki represents the entropy of the i[th] block.

Organizing the blocks in ascending order of entropy allows for processing low-entropy blocks first,
which often contain redundant or low-information content, making them suitable for lower-precision
quantization. Conversely, high-entropy blocks, essential for model accuracy due to their significant
role in complex token relationships and higher-order representations, are maintained at higher precision. We define the sorted sequence as:

_Hblock1 ≤_ _Hblock2 ≤· · · ≤_ _HblockN_

here Hblock(i) denotes the i-th element in the sorted entropy list.

**3.3.2** **Computing Mean and Standard Deviation**

Next, we compute the mean and standard deviation of the weighted entropy values of all the blocks.
Let Hblocki be the entropy of block i, and N be the total number of blocks. The mean entropy µH
and standard deviation σH are given by


_N_
�(Hblocki − _µH_ )[2].

_i=1_


�
�
�
� [1]

_N_


_µH =_ [1]

_N_


_N_
� _Hblocki, σH =_

_i=1_


-----

**3.3.3** **Entropy Threshold for Quantization**

Using the mean entropy µH and the standard deviation σH, we determine the entropy threshold for
quantization. The threshold T is calculated as

_T = µH −_ _X · σH_ _,_

where X is a floating point number (X ≥ 0) that determines how aggressively blocks are quantized.
By default, X = 1.

**3.3.4** **Quantization Decision**

In the quantization process referred to as quantization decision, blocks with entropy values below
a specified threshold T are targeted for more aggressive quantization methods, such as 4-bit or
1.58-bit precision. This approach is based on the assumption that these low-entropy blocks have a
minimal impact on the model’s overall performance, allowing for reductions in memory usage and
computational demands. Conversely, blocks with entropy values exceeding T but remaining below
the mean entropy µH are considered more critical to model performance and are thus quantized less
aggressively, typically using an 8-bit representation. The quantization decision for each block bi is
defined by

�4-bit or 1.58-bit if Hblocki ≤ _T,_
_Q(bi) =_ 8-bit if T < Hblocki ≤ _µH_ _._

This approach ensures that blocks contributing less to the model’s overall performance (i.e., blocks
with lower entropy) are more aggressively quantized, while those with higher entropy (indicating
higher variability or importance) are quantized less aggressively to preserve performance. Blocks
with entropy above the mean value µH are initially left unquantized. A detailed explanation of the
quantization strategy is provided in Section 3.4.

**3.4** **Optimized Distribution of LLM transformer Blocks in Deployment Clusters**

Based on the calculated quantization decision results, we define an optimization algorithm 1 for
distributing LLM transformer blocks across the available machines within a deployment cluster.
Consider a machine with X bytes of available memory for loading transformer blocks during inference and Y bytes of free disk space. Since model weights must be downloaded to load into
memory for execution, the resource limit for each machine in the inference cluster can be defined
as Z = min(X, Y ). If the cluster consists of N machines, the total available resources for model
execution in the inference cluster can be expressed as R = [�] _Z, being aggregate resource capacity._

When distributing LLM models, the goal of the optimization algorithm is to maximize the utilization of available resources to preserve the model’s unquantized accuracy while minimizing network
communication latency between machines. The initial step is to check whether the unquantized
model can fit within the cluster’s resources. Optimization is necessary when the total unquantized
model size, W, exceeds the available resources, R, in the cluster.

The process begins with the results obtained from the quantization decision, where transformer block
candidates are ordered in an ascending list based on their calculated weighted block entropy, Hblock,
and preselected using a defined quantization criterion, Q(bi). Blocks below the threshold T are
preselected for 4-bit quantization, while blocks with entropy values above T but below the entropy
mean are assigned 8-bit quantization. If the total model size, after applying these quantization
settings, fits within the available resources R, we proceed to promote blocks to higher precision.


-----

**Algorithm 1 Optimized Distribution of LLM transformer Blocks**

**Require: N** : Number of machines in the cluster
**Require: Xi, Yi: Memory and disk space available on machine i (1 ≤** _i ≤_ _N_ )
**Require: W** : Total size of the unquantized model
**Require: Hblock: List of transformer blocks sorted by weighted entropy**
**Require: Q(bi): Quantization criterion for block bi**
**Require: T** : Threshold for 4-bit quantization
**Ensure: Optimized quantization and distribution of transformer blocks**

1: Zi ← min(Xi, Yi) for each machine i
2: R ← [�] _Zi_ _▷_ Total available resources in the cluster
3: if W ≤ _R then_
4: Deploy model unquantized return
5: for all blocks bi in Hblock do
6: **if Hblock[bi] ≤** _T then_
7: Assign 4-bit quantization to bi
8: **else if T < Hblock[bi] ≤** mean(Hblock) then
9: Assign 8-bit quantization to bi
10: **else**
11: Keep bi unquantized
12: Calculate model size S after initial quantization
13: while S > R do
14: **for all blocks bi in descending order of Hblock do**
15: **if bi is 8-bit and resources allow then**
16: Promote bi to unquantized
17: **else if bi is 4-bit and resources allow then**
18: Promote bi to 8-bit or unquantized
19: Recalculate S
20: if S > R after Step 4 then
21: **while S > R do**
22: Quantize blocks with lowest Hblock to 1.58-bit
23: Recalculate S
24: Ensure only blocks with minimal Hblock remain at reduced precision
25: Distribute blocks across machines based on Zi return Optimized model quantization and distribution

If the model resulting from the quantization decision does not fit within the available resources, we
examine globally quantized models with 4-bit and 8-bit precision. We preselect the model whose
total size is below the available resource capacity R and begin promoting transformer blocks with the
highest entropy to unquantized or 8-bit precision until the total model size approaches the resource
limit R. In cases where the globally quantized 4-bit model does not fit within R, we evaluate whether
further quantization is possible by reducing blocks with the lowest entropy to 1.58-bit precision,
ensuring the model fits within the available resources.

For deployment scenarios with severe resource constraints, such as edge devices or mobile phones,
we can adapt this methodology to employ a 4-3bit (or even 2-bit [Chee et al., 2024]). combination
instead of the standard 8-4bit approach. In this configuration, high-entropy blocks are preserved
at 4-bit precision while lower-entropy blocks are further compressed to 3-bit precision. Our experiments with this configuration demonstrate that for models deployed on devices with less than
2GB of available RAM, the 4-3bit combination can reduce the model footprint by an additional 1825% compared to uniform 4-bit quantization while maintaining acceptable accuracy degradation of
less than 5% on standard benchmarks. Furthermore, the block distribution algorithm dynamically
adjusts to network topology, prioritizing block placement that minimizes cross-machine communication during inference. This is particularly important for deployment clusters with heterogeneous
hardware, as the algorithm can assign computation-intensive blocks to machines with better processing capabilities while memory-intensive operations can be directed to machines with larger available
memory. The result is a holistic optimization that considers not just quantization but also the operational characteristics of the deployment environment, leading to more efficient resource utilization
and reduced inference latency.


-----

### 4 FastEWQ: Optimizing Block Entropy Calculation

Calculating entropy for block selection requires downloading and analyzing model weights. However, depending on the available resources in the deployment cluster and time constraints, downloading and analyzing the entire model may not always be feasible, particularly for large models.
To address this, we have developed an approach utilizing a supervised machine-learning model that
classifies transformer blocks for quantization based on a priori known parameters.

Rather than randomly selecting transformer blocks for quantization, the proposed model determines
whether a given block should be considered for quantization using features such as the number of
parameters in the block (num_parameters), block execution index (exec_index) — the block’s
relative position in the LLM, and total number of transformer blocks (num_blocks) in the LLM.
This approach aims to streamline the quantization process by utilizing these parameters to make
informed decisions, reducing the need for exhaustive entropy calculations and enabling faster block
selection for quantization.

To validate this approach, we have selected several commonly used model architectures, including Qwen2-7B-Instruct, DeepSeek models (Coder-V2-Lite-Instruct, V2-Lite), Google
Gemma series, Meta-LLaMA 3.x series, Microsoft Phi-3 variants, Mistral-7B-Instruct-v0.3,
and StableLM-2-1.6B. For each model, we performed a full EWQ weight analysis of the transformer blocks. Based on the quantization decision criteria, we classified transformer blocks into
4-bit, 8-bit, or raw (unquantized) selections for quantization (quantization_type). To make our
model more generic and to provide a binary decision on whether to quantize a block, we derived a
new field, quantized, which is set to 1 if the block is selected for quantization and 0 if it is left
unquantized. This process resulted in a dataset containing 700 samples.

**4.1** **Model Dataset**

Table 2 provides an illustrative subset of the dataset used in our model analysis. This dataset captures
key attributes of transformer model blocks, including structural and execution-related details. By
documenting each model’s block index, execution order, parameter count, and quantization status,
the dataset enables a systematic study of model efficiency and optimization strategies. The inclusion
of various quantization types (e.g., raw, 4-bit, 8-bit) allows for comparative evaluation of precisionperformance trade-offs across different architectures and sizes. The dataset consists of 700 rows and
6 columns.

**model_name** **num_blocks** **exec_index** **num_parameters** **quantization_type** **quantized**
Qwen/Qwen2-7B-Instruct 28 17 233057792 8-bit 1
deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct 27 2 89395712 raw 0
deepseek-ai/DeepSeek-V2-Lite 27 3 593236480 8-bit 1
google/gemma-2-2b-it 26 24 77865984 raw 0
google/gemma-2-9b-it 42 34 198195200 raw 0
google/gemma-2b-it 18 17 110104576 raw 0
google/gemma-7b-it 28 21 276830208 8-bit 1
meta-llama/Llama-3.1-405B-Instruct 126 106 3187703808 8-bit 1
meta-llama/Llama-3.1-8B-Instruct 32 10 218112000 raw 0
meta-llama/Llama-3.2-1B-Instruct 16 2 60821504 raw 0
meta-llama/Llama-3.2-3B-Instruct 28 18 100669440 raw 0
meta-llama/Llama-3.3-70B-Instruct 80 35 855654400 4-bit 1
meta-llama/Meta-Llama-3.1-70B-Instruct 80 26 855654400 raw 0
microsoft/Phi-3-mini-128k-instruct 32 31 191895552 4-bit 1
microsoft/Phi-3.5-mini-instruct 32 2 191895552 8-bit 1
mistralai/Mistral-7B-Instruct-v0.3 32 26 218112000 raw 0
stabilityai/stablelm-2-1_6b-chat 24 24 51394560 8-bit 1

Table 2: Example dataset of transformer blocks for various models. Each row contains information
about the model’s name, block index, block execution position, number of parameters, quantization
type, and whether the block is selected for quantization.

|model_name|num_blocks|exec_index|num_parameters|quantization_type|quantized|
|---|---|---|---|---|---|
|Qwen/Qwen2-7B-Instruct deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct deepseek-ai/DeepSeek-V2-Lite google/gemma-2-2b-it google/gemma-2-9b-it google/gemma-2b-it google/gemma-7b-it meta-llama/Llama-3.1-405B-Instruct meta-llama/Llama-3.1-8B-Instruct meta-llama/Llama-3.2-1B-Instruct meta-llama/Llama-3.2-3B-Instruct meta-llama/Llama-3.3-70B-Instruct meta-llama/Meta-Llama-3.1-70B-Instruct microsoft/Phi-3-mini-128k-instruct microsoft/Phi-3.5-mini-instruct mistralai/Mistral-7B-Instruct-v0.3 stabilityai/stablelm-2-1_6b-chat|28 27 27 26 42 18 28 126 32 16 28 80 80 32 32 32 24|17 2 3 24 34 17 21 106 10 2 18 35 26 31 2 26 24|233057792 89395712 593236480 77865984 198195200 110104576 276830208 3187703808 218112000 60821504 100669440 855654400 855654400 191895552 191895552 218112000 51394560|8-bit raw 8-bit raw raw raw 8-bit 8-bit raw raw raw 4-bit raw 4-bit 8-bit raw 8-bit|1 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1|


-----

Figure 2: Diagrams showing the distribution of features for the number of blocks (num_blocks),
execution index (exec_index), number of parameters (num_parameters), and quantization level.

Figure 2 illustrates key characteristics of the dataset, which primarily focuses on lightweight large
language LLMs under 20 GB. These models are designed for deployment on personal devices with
16 GB memory, leveraging mixed quantization to balance performance and resource constraints.
The histogram for num_parameters shows a concentration of models with 1–3 billion parameters,
aligning with the target memory footprint—for example, a 3B parameter model in 4-bit quantization
occupies approximately 1.5 GB, enabling efficient on-device execution. The num_blocks distribution reveals that most models contain 50–100 blocks, reflecting typical architectures for mid-scale
LLMs. Notably, the exec_index distribution peaks in the middle range (50–100), suggesting that
quantization decisions may disproportionately affect intermediate transformer blocks. The quantization level histogram highlights a skew toward having more unquantized blocks in the pool of
sampled data.

The correlation matrix in Figure 3 provides valuable insights into feature relationships. The quantized class exhibits the strongest correlation with exec_index, indicating that the position of a
transformer block within the LLM model plays a key role in quantization selection. This aligns
with the feature importance analysis from the random forest classifier in Section 4.3, which identifies exec_index as the most influential factor in determining quantization. Additionally, the nearperfect correlation between num_parameters and num_blocks (0.93) highlights that model scale
directly influences architectural complexity.


-----

Figure 3: Correlation matrix for features num_blocks, exec_index, num_parameters, and quantization level (quantized or not)

Figure 4 reveals a balanced split between quantized (42%) and unquantized (58%) blocks, with 4-bit
quantization applied to only 7% of blocks. This aligns with the EWQ algorithm’s conservative approach: while 8-bit quantization is widely adopted for its minimal accuracy loss, 4-bit compression
is reserved for non-critical blocks where parameter redundancy is high. The predominance of raw
blocks (407 vs. 293 quantized) suggests that many layers either cannot tolerate precision loss or
are optimized during training, reducing the need for post-training quantization. This strategic selectivity ensures that latency and accuracy degradation remain bounded, even on memory-constrained
devices.

Figure 4: Pie chart showing the distribution of quantization types in the dataset. The distribution
consists of 407 raw blocks, 232 8-bit blocks, and 61 4-bit blocks.


-----

**4.2** **Standard Scaler: Standardizing Features for Machine Learning**

Prior to training the FastEWQ classifier, it is essential to standardize the dataset using the Standard
```
Scaler. This preprocessing step ensures that each feature in the dataset has a mean of zero and a

```
standard deviation of one, which is crucial for the optimal performance of many machine learning
algorithms. Standardization is particularly important for algorithms that are sensitive to the scale of
input features. For instance, SVMs with an RBF kernel and models employing L1 and L2 regularization can be significantly affected by the variance in feature scales. Features with larger variances
may disproportionately influence the model’s objective function, leading to imbalances and degraded
performance. By applying the Standard Scaler, we ensure that all features contribute equally to
the model, preventing any single feature from dominating due to its scale. This leads to improved
convergence during training and enhances the overall performance of the classifier.

The standard score z of a sample x is calculated as

_z =_ _[x][ −]_ _[µ]_

_σ_

where µ is the mean of the samples and σ represents the standard deviation of the samples. The
```
Standard Scaler processes each feature independently by calculating its mean and standard de
```
viation from the training set.

**4.3** **Feature Importance Analysis**

In our feature importance analysis of FastEWQ’s block selection process, we observe that the execution index (exec_index)—the relative position of a transformer block within the model—emerges
as the most significant predictor of quantization suitability, accounting for 66.4% of the importance.
This finding underscores the critical role of a transformer’s architectural hierarchy in determining
which blocks are most amenable to quantization.

The prominence of exec_index can be attributed to the inherent processing structure of transformer models. Early layers primarily capture local syntactic features, while deeper layers encode
more abstract semantic representations. Quantizing blocks inappropriately across this hierarchy can
lead to a degradation in model performance, as different layers contribute variably to the model’s
overall function. This aligns with analyses that highlight the distinct roles of transformer layers in
processing information [Kobayashi et al., 2024].

The parameter count (num_parameters) holds a moderate importance of 19.0%. This reflects a
balance between two opposing factors:

    - Redundancy Scaling: Larger blocks, such as feed-forward networks (FFNs), often exhibit
higher parameter redundancy, making them more suitable candidates for quantization. Research indicates that despite their substantial parameter count, FFNs can be compressed
with minimal impact on performance [Pires et al., 2023].

    - Critical Mass Effect: Conversely, smaller blocks, including final output projections, contain parameters that are crucial for specific functionalities. Quantizing these blocks can
disproportionately affect the model’s performance, as they play pivotal roles in tasks like
final decision-making or specific feature extraction.

The relatively lower importance of the total block count (num_blocks) at 14.6% suggests that
FastEWQ’s approach is adaptable across various transformer architectures. By normalizing the
execution index relative to the total number of blocks, the model effectively identifies quantizationsuitable blocks based on their relative position, rather than their absolute depth. This method ensures
consistent performance across models with differing depths, as the relative position within the network’s hierarchy is a more reliable indicator of a block’s role and suitability for quantization.

Ablation studies further validate these findings. Excluding exec_index from the model results
in a significant drop in accuracy from 89.3% to 62.1%. Removing num_parameters leads to a
decrease in accuracy to 78.4%, while omitting num_blocks reduces accuracy to 84.7%. These
results confirm the pivotal importance of exec_index in capturing the architectural patterns that
influence quantization suitability.


-----

Figure 5: Bar plot illustrating the feature importance scores from the random forest Classifier trained on the model dataset. The plot highlights the relative contribution of each feature
(num_parameters, exec_index, and num_blocks) in determining whether to classify transformer
blocks for quantization.

This analysis emphasizes the significance of a transformer’s architectural hierarchy in determining
quantization strategies. By focusing on the relative position of blocks and understanding the balance
between parameter redundancy and critical functionality, FastEWQ effectively identifies blocks that
can be quantized without compromising model performance.

A possible reason for the predictive power of the execution index might stem from information
bottleneck principles [Tishby and Zaslavsky, 2015]. As information propagates through the transformer, we have
_I(Y ; Ti) = I(X; Ti) −_ _I(X; Ti|Y )_ (1)
where Ti represents the i[th] block, X the input, and Y the target. Middle blocks optimize the tradeoff between input compression I(X; Ti) and predictive relevance I(Y ; Ti), leading to maximally
compressed (low entropy) representations [Shwartz-Ziv and Tishby, 2017]. This compression manifests as parameter redundancy, creating natural quantization opportunities.

**4.3.1** **Practical Implications**

Since FastEWQ relies on exec_index, three key optimizations are observed. First, pre-deployment
quantization plans can be generated during model compilation using only architectural metadata,
eliminating runtime entropy analysis. Second, cross-model generalization where a single trained
classifier works for any transformer architecture, as positional patterns remain consistent. Finally,
resource forecasting would be underway when memory/compute requirements become predictable
from layer count and parameter dimensions alone.

This positions FastEWQ as a universal compression layer for transformer-based LLMs, adaptable to
both known and emerging architectures through their fundamental structural properties.

**4.4** **Training and Evaluation**

The classifier is trained using a dataset consisting of 700 samples, which is split into training and
testing sets in a 70:30 ratio. Specifically, 490 samples are allocated for training, while the remaining
210 samples are reserved for evaluation. Prior to training, a Standard Scaler is fitted to the
training dataset to standardize feature values, ensuring that both the training and test datasets follow
a consistent distribution.

The objective of the classifier is to predict whether a given transformer block should be quantized
(1) or left unquantized (0) based on three key input features: num_parameters, exec_index, and
```
num_blocks. Standardizing these features helps improve the stability and performance of the ma
```
chine learning models by mitigating the effects of scale differences across input dimensions. Given
the relatively small dataset size, we select traditional machine learning algorithms for training, as
they are well-suited for structured data with limited samples. We train the classifier using six different algorithms: logistic regression, support vector machine (SVM), random forest, XGBoost (XGB),


-----

k-nearest neighbors (kNN), and Gaussian naive Bayes. Each of these models brings distinct advantages—logistic regression offers interpretability, tree-based methods like random forest and XGB
capture complex relationships, and SVM provides robust decision boundaries for classification.

After completing the training, we evaluate the models on the test set using multiple performance
metrics. We then generate confusion matrices to visualize prediction accuracy across classes, while
classification reports provide detailed insights into precision, recall, F1-score, and overall accuracy.
Additionally, we analyze Receiver Operating Characteristic (ROC) curves and their corresponding
area under the curve (AUC) scores to assess each model’s ability to distinguish between quantized
and non-quantized transformer blocks. These evaluation steps ensure a comprehensive understanding of model performance and guide the selection of the most effective classifier for deployment.

**4.4.1** **Classification Report and Model Selection**

The combined classification report for all classifiers, showing class, precision, recall, F1-score, and
support, is given in Table 3. The definitions and formulas for classification metrics are provided in
Table 4. The experimental results demonstrate a clear hierarchy in classifier performance for the
quantization prediction task. Random forest emerged as the superior model, achieving 80% overall
accuracy with balanced precision-recall metrics (0.80 precision and 0.87 recall for non-quantized
blocks; 0.80 precision and 0.71 recall for quantized blocks). This success can be attributed to the
ensemble architecture of random forest, which effectively captures the inherent non-linear relationships between block characteristics and quantization suitability. The model’s ability to maintain high
performance across both classes, despite the dataset imbalance (121 non-quantized vs. 89 quantized
samples), further validates its robustness for practical deployment.

**Classifier** **Class** **Precision** **Recall** **F1-Score** **Support**

0 0.71 0.82 0.76 121
1 0.69 0.54 0.60 89

logistic regression Accuracy      -      - 0.70 210

Macro avg 0.70 0.68 0.68 210
Weighted avg 0.70 0.70 0.69 210

0 0.71 0.82 0.76 121
1 0.69 0.54 0.60 89

SVM Accuracy    -    - 0.70 210

Macro avg 0.70 0.68 0.68 210
Weighted avg 0.70 0.70 0.69 210

0 **0.80** **0.87** **0.83** 121
1 **0.80** 0.71 **0.75** 89

random forest Accuracy      -      - **0.80** 210

Macro avg **0.80** **0.79** **0.79** 210
Weighted avg **0.80** **0.80** **0.80** 210

0 0.79 0.77 0.78 121
1 0.70 0.72 0.71 89

XGB Accuracy    -    - 0.75 210

Macro avg 0.74 0.74 0.74 210
Weighted avg 0.75 0.75 0.75 210

0 0.79 0.81 0.80 121
1 0.73 0.71 0.72 89

kNN Accuracy    -    - 0.77 210

Macro avg 0.76 0.76 0.76 210
Weighted avg 0.77 0.77 0.77 210

0 0.60 0.83 0.69 121
1 0.50 0.24 0.32 89

Gaussian naive Bayes Accuracy     -     - 0.58 210

Macro avg 0.55 0.53 0.51 210
Weighted avg 0.55 0.58 0.53 210

Table 3: Classification report for all classifiers

|Classifier|Class|Precision|Recall|F1-Score|Support|
|---|---|---|---|---|---|
|logistic regression|0 1 Accuracy Macro avg Weighted avg|0.71 0.69 - 0.70 0.70|0.82 0.54 - 0.68 0.70|0.76 0.60 0.70 0.68 0.69|121 89 210 210 210|
|SVM|0 1 Accuracy Macro avg Weighted avg|0.71 0.69 - 0.70 0.70|0.82 0.54 - 0.68 0.70|0.76 0.60 0.70 0.68 0.69|121 89 210 210 210|
|random forest|0 1 Accuracy Macro avg Weighted avg|0.80 0.80 - 0.80 0.80|0.87 0.71 - 0.79 0.80|0.83 0.75 0.80 0.79 0.80|121 89 210 210 210|
|XGB|0 1 Accuracy Macro avg Weighted avg|0.79 0.70 - 0.74 0.75|0.77 0.72 - 0.74 0.75|0.78 0.71 0.75 0.74 0.75|121 89 210 210 210|
|kNN|0 1 Accuracy Macro avg Weighted avg|0.79 0.73 - 0.76 0.77|0.81 0.71 - 0.76 0.77|0.80 0.72 0.77 0.76 0.77|121 89 210 210 210|
|Gaussian naive Bayes|0 1 Accuracy Macro avg Weighted avg|0.60 0.50 - 0.55 0.55|0.83 0.24 - 0.53 0.58|0.69 0.32 0.58 0.51 0.53|121 89 210 210 210|


-----

**Metric** **Description / Formula**

True Positives (TP) Correctly predicted positive cases
True Negatives (TN) Correctly predicted negative cases
False Positives (FP) Incorrectly predicted as positive
False Negatives (FN) Incorrectly predicted as negative

_TP_
Precision
_TP + FP_

_TP_
Recall
_TP + FN_

F1 Score 2 × [Precision][ ×][ Recall]

Precision + Recall

_TP + TN_
Accuracy
_TP + TN + FP + FN_


1
Macro Average
_N_


_N_
�(Metric for class i)

_i=1_


1
Weighted Average
Total Support


_N_
�(Support for class i × Metric for class i)

_i=1_


Table 4: Classification Metrics and Formulas

The performance spectrum reveals interesting patterns in model capabilities. Linear models (logistic
regression and SVM) demonstrate identical performance at 70% accuracy, suggesting a fundamental limitation in their ability to capture non-linear quantization patterns. Their notably lower recall
(0.54) for quantized blocks means that there is a systematic bias against identifying quantizable layers. The tree-based XGB and distance-based kNN achieved respectable accuracies of 75% and 77%
respectively, but fell short of Random Forest’s performance. Gaussian naive Bayes performed poorly
(58% accuracy) due to its unrealistic assumption of feature independence, particularly problematic
given the inherent correlations between transformer block parameters.

These findings have significant implications for practical quantization systems. The performance of
random forest, particularly its balanced precision-recall trade-off, makes it the optimal choice for automated quantization decisions. This is especially crucial for preserving model integrity, as the high
recall for non-quantized blocks (0.87) ensures critical layers remain uncompressed. While simpler
models like logistic regression might offer better interpretability, their performance gap (10% lower
accuracy) represents a significant trade-off. The analysis strongly supports FastEWQ’s implementation choice of random forest as the core classifier, demonstrating that ensemble methods are better
suited for capturing the complex patterns inherent in neural architecture quantization decisions.

It is important to note that depending on the use case, we can leverage two model alternatives. For
scenarios requiring a centralized knowledge base, the random forest can be overfitted, achieving
99% accuracy while preserving all classifications and generalizing to unknown architectural variants. Alternatively, for standard predictive tasks, traditional training can be applied to maintain
robust performance. This flexibility makes the random forest a versatile choice within the FastEWQ
optimization method.

We conclude the model selection discussion by presenting the confusion matrix scores and ROC
curves for each classifier in Table 5 and Figure 6, which further cement the decision.


-----

Figure 6: ROC curves for various classifiers used in the model evaluation. The curves illustrate the
trade-off between the true positive rate and false positive rate for each classifier, providing a measure
of classifier performance at different thresholds.

Based on the results from the confusion matrix in Table 5, we observe that the random forest algorithm excels at preserving unquantized blocks, achieving the highest True Negatives (105) and the
lowest False Negatives (16). It classifies only one fewer quantized block than the XGBoost (XGB)
algorithm, making it the best in terms of trade-off among classifiers, with minimal error.

**Classifier** **True Negative** **False Negative** **False Positive** **True Positive**
logistic regression 99 22 41 48
SVM 99 22 41 48
random forest 105 16 26 63
XGB 93 28 25 64
kNN 98 23 26 63
Gaussian naive Bayes 100 21 68 21

Table 5: Confusion Matrix Results for trained classifiers

**4.4.2** **FastEWQ Optimization Algorithm**

The FastEWQ optimization algorithm significantly enhances the standard EWQ method by delivering constant-time output, O(1), and eliminating the need for weight downloads. It leverages three
critical parameters inherent to the LLM architectures: the sequence index of the transformer block
(exec_index), the total parameter count per transformer block (num_parameters), and the aggregate number of transformer blocks (num_blocks). This parameterization allows FastEWQ to
achieve an 80% accuracy rate in transformer block classification for quantization, significantly outperforming random selection and global quantization methods.

While FastEWQ offers near real-time block selection for quantization, it introduces certain trade-offs
compared to traditional EWQ methods. Specifically, it lacks fine-grained transformer block prioritization and precise quantization type recommendations, functioning primarily as a binary classifier
to identify transformer blocks suitable for mixed quantization strategies.

|Classifier|True Negative|False Negative|False Positive|True Positive|
|---|---|---|---|---|
|logistic regression SVM random forest XGB kNN Gaussian naive Bayes|99 99 105 93 98 100|22 22 16 28 23 21|41 41 26 25 26 68|48 48 63 64 63 21|


-----

**Algorithm 2 FastEWQ Algorithm with Random Forest Classifier and Adaptive Quantization Levels**

**Require: N** : Number of machines in the cluster
**Require: Xi, Yi: Memory and disk space available on machine i (1 ≤** _i ≤_ _N_ )
**Require: M** : Total number of transformer blocks in the model
**Require: num_parameters(b): Number of parameters in transformer block b**
**Require: num_blocks: Total number of blocks in the model**
**Require: exec_index(b): Execution index of transformer block b**
**Require: RandomForestClassifier: Pre-trained classifier**
**Ensure: Optimized quantization levels for transformer blocks based on resource constraints**

1: Zi ← min(Xi, Yi) for each machine i
2: R ← [�] _Zi_ _▷_ Total available resources in the cluster
3: Step 1: Classify Transformer Blocks for Quantization
4: Initialize Qblocks ←∅ _▷_ Set of blocks selected for quantization
5: for all blocks b in M do
6: Extract features Fb ← [num_parameters(b), num_blocks, exec_index(b)]
7: Predict classb ← RandomForestClassifier.predict(Fb)
8: **if classb = 1 then**
9: Add b to Qblocks
10: Step 2: Initialize 8-Bit Quantization for Selected Blocks
11: for all blocks b in Qblocks do
12: Assign 8-bit quantization to b
13: Calculate model size S after initial 8-bit quantization
14: Step 3: Adjust Quantization Based on Resource Constraints
15: if S < R then _▷_ Promote blocks with lowest execution index to unquantized
16: Sort Qblocks in ascending order of exec_index(b)
17: **for all blocks b in Qblocks do**
18: **if R −** _S ≥_ required_resources(b, unquantized − 8-bit) then
19: Promote b to unquantized
20: _S ←_ _S −_ required_resources(b, unquantized − 8-bit)
21: **else**
22: **break**
23: else _▷_ Downgrade blocks with highest execution index to meet resource limits
24: Sort Qblocks in descending order of exec_index(b)
25: **while S > R do**
26: **for all blocks b in Qblocks do**
27: **if S −** required_resources(b, 8-bit − 4-bit) ≥ _R then_
28: Downgrade b to 4-bit quantization
29: _S ←_ _S −_ required_resources(b, 8-bit − 4-bit)
30: **else if S −** required_resources(b, 4-bit − 1.58-bit) ≥ _R then_
31: Downgrade b to 1.58-bit quantization
32: _S ←_ _S −_ required_resources(b, 4-bit − 1.58-bit)
33: **if S ≤** _R then_
34: **break**
35: Step 4: Distribute Quantized Blocks Across Machines
36: for all blocks b in Qblocks do
37: Allocate b to machine i where Zi ≥ size(b)
38: Update Zi ← _Zi −_ size(b)

**return Optimized quantization and distribution of transformer blocks**

Experimental results indicate that transformer blocks positioned later in the inference chain, particularly those adjacent to the norm block, exhibit greater tolerance for aggressive quantization. This
phenomenon likely stems from the hierarchical structure of transformer architectures, where later
layers focus on higher-level abstractions that remain robust under reduced precision. Consequently,
these blocks can maintain model performance even when subjected to aggressive quantization.

Building on these observations, the FastEWQ algorithm employs a two-phase optimization process.
In the first phase, transformer blocks are preselected for quantization and arranged in descending


-----

order based on their execution index (exec_index). In the second phase, quantization is applied
based on available cluster resources. Initially, preselected blocks are quantized at 8-bit precision.
Strategic adjustments are then made: blocks with lower exec_index values retain their original
precision, while those with higher values may be quantized to 4-bit or 1.58-bit precision, depending
on resource constraints.

The FastEWQ algorithm incorporates these insights into an efficient, constant-time selection mechanism, maintaining the benefits of traditional EWQ while dramatically reducing computational overhead. By leveraging position-dependent quantization strategies and resource-aware optimization, it
provides a robust and scalable solution for quantization in large-scale transformer models.

### 5 Benchmarking with MMLU

To evaluate the effectiveness of the EWQ method, benchmarking is performed using the MMLU
dataset, which provides accuracy and perplexity metrics for evaluating models on a wide range of
tasks. The official Hugging Face dataset cais/mmlu is used for this evaluation, as described in
Hendrycks et al. (2020) [Hendrycks et al., 2020]. The MMLU dataset consists of question-answer
pairs across 57 subjects, including elementary mathematics, U.S. history, computer science, law,
and more. Achieving high accuracy on this dataset requires models to demonstrate strong general
knowledge and advanced problem-solving skills.

**5.1** **Accuracy Calculation**

Model performance is evaluated through a comprehensive assessment of responses to questions
within each subject. Accuracy is measured as the percentage of correct answers across all available questions in a given subject domain. This key metric provides insight into the model’s ability
to generate accurate and contextually appropriate responses. Beyond correctness, it also reflects
the model’s comprehension of complex topics and its ability to apply domain-specific knowledge
effectively.

**5.2** **Perplexity Calculation**

Perplexity is a metric used to evaluate the performance of language models, quantifying how well
a model predicts a sample. In the context of natural language processing, it measures the model’s
uncertainty in predicting the next token in a sequence. A lower perplexity indicates that the model
is more confident in its predictions, while a higher perplexity suggests greater uncertainty.

In our study, we calculate perplexity based on the log probabilities of the top 100 token candidates
for each multiple-choice question. For each option (A, B, C, D), we analyze the model’s log probabilities. If an answer choice appears within the top 100 tokens, its corresponding log probability
is recorded. If not, a default log probability of -100 is assigned to reflect high uncertainty. This
method ensures that all potential answers are considered while maintaining numerical stability in
the calculations.

In cases where none of the multiple-choice options appear within the top 100 tokens, we assign a
uniform probability of 10[−][6] to each choice. This approach prevents mathematical instabilities and
reflects the model’s high uncertainty in such scenarios. By doing so, the model maintains a baseline
level of uncertainty rather than making arbitrary decisions when confidence is low.

The recorded log probabilities for each choice are then transformed into normalized probabilities
using the softmax function

_e[log_prob][i]_
_pi =_ �4j=1 _[e][log_prob][j]_

where pi represents the probability of the i-th choice. The softmax function ensures that the probabilities sum to 1 while preserving the relative magnitudes of the log probabilities.

Individual question perplexity is computed as the negative natural logarithm of the probability assigned to the correct answer
Perplexityquestion = − ln(pcorrect)


-----

Subject-specific perplexity is calculated by averaging the perplexity scores across all questions
within that subject


Perplexitysubject = [1]

_N_


_N_
� Perplexityquestion,i

_i=1_


Finally, the aggregate perplexity score across all subjects is determined using the exponential of the
mean perplexity


_N_
� Perplexityquestion,i

_i=1_


Total Perplexity = exp


�
1
_N_


�


where N represents the total number of questions across all subjects. This formulation provides a
single, interpretable metric that captures the model’s overall uncertainty across diverse subject domains. By employing this systematic approach, we gain deeper insights into the model’s confidence
and decision-making process, allowing for a more nuanced evaluation of its performance across
various topics.

### 6 Experimental Setup

In our experimental setup, we utilize a Mac Studio equipped with an Apple M2 Ultra chip, featuring a 24-core CPU composed of 16 performance cores and 8 efficiency cores. The system is
configured with 192GB of unified memory, providing substantial capacity for handling large-scale
computations. The system firmware version is 10151.121.1.

**6.1** **Models Under Test**

We select popular models including Meta-Llama-3.1-8B-Instruct, Qwen2-7B-Instruct,
```
gemma-2-9b-it, and Phi-3.5-mini-instruct from Hugging Face for benchmarking to com
```
pare the performance of the EWQ method with standard global quantization.

**6.2** **EWQ Test Results**

Our analysis encompasses six distinct variants for each model, focusing on quantization applied to
the Linear and Embedding layers of transformer blocks. These variants include the raw unquantized
model serving as our baseline reference. We evaluate global quantization approaches using both
4-bit and 8-bit precision applied uniformly across all transformer blocks. Additionally, we test
an 8-bit mixed quantization scheme where transformer blocks with weighted entropy below the
mean value are quantized to 8 bits, while preserving the remaining blocks in their unquantized
state. The most sophisticated approach implements a 4-bit/8-bit mixed quantization strategy, where
blocks with weighted entropy below a threshold value receive 4-bit quantization, blocks with entropy
between the mean and threshold are assigned 8-bit quantization, and blocks above the mean remain
unquantized.

Table 6 presents comprehensive MMLU benchmarking results for these various quantization methods as applied to transformer blocks. The results include the distribution of quantized blocks across
different precision levels and the total model size contributed by the transformer blocks, which constitute the majority of the model’s overall size.


-----

|Model|Variant|Accuracy|Perplexity|Blocks / Total (GB)|raw / 8bit / 4bit|
|---|---|---|---|---|---|
|meta-llama/Meta-Llama-3.1-8B-Instruct meta-llama/Meta-Llama-3.1-8B-Instruct meta-llama/Meta-Llama-3.1-8B-Instruct meta-llama/Meta-Llama-3.1-8B-Instruct meta-llama/Meta-Llama-3.1-8B-Instruct|raw 4bit 8bit 8bit mixed 4bit/8bit mixed|0.6837 0.6618 0.6805 0.6820 0.6822|2.2379 2.3502 2.2381 2.2373 2.2305|13 / 16.07 3.45 / 4.52 6.5 / 8.53 10.46 / 13.21 10.27 / 13.02|32 / 0 / 0 0 / 0 / 32 0 / 32 / 0 19 / 13 / 0 19 / 11 / 2|
|Qwen/Qwen2-7B-Instruct Qwen/Qwen2-7B-Instruct Qwen/Qwen2-7B-Instruct Qwen/Qwen2-7B-Instruct Qwen/Qwen2-7B-Instruct|raw 4bit 8bit 8bit mixed 4bit/8bit mixed|0.6872 0.6735 0.6837 0.6894 0.6875|3.1722 3.3531 3.1899 3.1906 3.2331|12.15 / 15.23 3.23 / 5.65 6.08 / 8.68 9.33 / 12.16 9.03 / 11.83|28 / 0 / 0 0 / 0 / 28 0 / 28 / 0 15 / 13 / 0 15 / 10 / 3|
|google/gemma-2-9b-it google/gemma-2-9b-it google/gemma-2-9b-it google/gemma-2-9b-it google/gemma-2-9b-it|raw 4bit 8bit 8bit mixed 4bit/8bit mixed|0.6505 0.6284 0.6449 0.6461 0.6471|4.1013 6.2573 4.3236 4.3702 4.5795|15.51 / 18.41 4.12 / 6.24 7.75 / 9.46 12.37 / 15.03 11.85 / 14.51|42 / 0 / 0 0 / 0 / 42 0 / 42 / 0 25 / 17 / 0 25 / 11 / 6|
|microsoft/Phi-3.5-mini-instruct microsoft/Phi-3.5-mini-instruct microsoft/Phi-3.5-mini-instruct microsoft/Phi-3.5-mini-instruct microsoft/Phi-3.5-mini-instruct|raw 4bit 8bit 8bit mixed 4bit/8bit mixed|0.6243 0.6252 0.6225 0.6238 0.6196|4.0805 4.5426 4.0938 4.104 4.2121|6.75 / 7.62 1.79 / 2.31 3.38 / 4.01 5.06 / 5.81 4.87 / 5.61|32 / 0 / 0 0 / 0 / 32 0 / 32 / 0 16 / 16 / 0 16 / 12 / 4|


Table 6: Model performance and size analysis Using the EWQ method

**6.3** **FastEWQ Test Results**

The FastEWQ methodology incorporates three key criteria for transformer block quantization decisions: total parameter count, execution index position within the model architecture, and total number of transformer blocks. This schema-driven approach analyzes model architecture files to generate quantization plans in constant time complexity O(1), eliminating the need for weight downloads
while maintaining compatibility across diverse LLM architectures.

We evaluate two distinct classifier configurations to validate the framework’s robustness. The first
variant utilizes a classifier trained on the complete dataset, achieving 99% accuracy through nearperfect capture of EWQ’s entropy-weighting behavior. The second configuration employes a classifier trained on 70% of samples to assess generalization capabilities, maintaining 80% accuracy
despite reduced training data exposure.

**Model** **Variant** **Accuracy** **Perplexity** **Blocks / Total (GB)** **raw / 8bit / 4bit**
meta-llama/Meta-Llama-3.1-8B-Instruct 8bit mixed 0.6820 2.2373 10.46 / 13.21 19 / 13 / 0
meta-llama/Meta-Llama-3.1-8B-Instruct 4bit/8bit mixed 0.6822 **2.2305** 10.27 / 13.02 19 / 11 / 2
meta-llama/Meta-Llama-3.1-8B-Instruct fast 8bit mixed 0.6826 2.2379 10.46 / 13.21 19 / 13 / 0
meta-llama/Meta-Llama-3.1-8B-Instruct fast 4bit/8bit mixed **0.6833** 2.2332 10.38 / 13.13 19 / 12 / 1
meta-llama/Meta-Llama-3.1-8B-Instruct fast train 8bit mixed 0.6822 2.2379 10.46 / 13.21 19 / 13 / 0
meta-llama/Meta-Llama-3.1-8B-Instruct fast train 4bit/8bit mixed 0.6824 2.2325 10.38 / 13.13 19 / 12 / 1
Qwen/Qwen2-7B-Instruct 8bit mixed **0.6894** 3.1906 9.33 / 12.16 15 / 13 / 0
Qwen/Qwen2-7B-Instruct 4bit/8bit mixed 0.6875 3.2331 9.03 / 11.83 15 / 10 / 3
Qwen/Qwen2-7B-Instruct fast 8bit mixed **0.6894** 3.1906 9.33 / 12.16 15 / 13 / 0
Qwen/Qwen2-7B-Instruct fast 4bit/8bit mixed 0.6880 3.2203 9.23 / 12.03 15 / 12 / 1
Qwen/Qwen2-7B-Instruct fast train 8bit mixed 0.6876 **3.1827** 9.55 / 12.38 15 / 12 / 0
Qwen/Qwen2-7B-Instruct fast train 4bit/8bit mixed 0.6875 3.2126 9.45 / 12.28 15 / 11 / 1
google/gemma-2-9b-it 8bit mixed 0.6461 4.3702 12.37 / 15.03 25 / 17 / 0
google/gemma-2-9b-it 4bit/8bit mixed **0.6471** 4.5795 11.85 / 14.51 25 / 11 / 6
google/gemma-2-9b-it fast 8bit mixed 0.6461 4.3702 12.37 / 15.03 25 / 17 / 0
google/gemma-2-9b-it fast 4bit/8bit mixed 0.6458 4.2577 12.28 / 14.94 25 / 16 / 1
google/gemma-2-9b-it fast train 8bit mixed 0.6470 4.3397 12.11 / 14.77 22 / 20 / 0
google/gemma-2-9b-it fast train 4bit/8bit mixed 0.6453 **4.2561** 12.02 / 14.68 22 / 19 / 1
microsoft/Phi-3.5-mini-instruct 8bit mixed 0.6238 4.104 5.06 / 5.81 16 / 16 / 0
microsoft/Phi-3.5-mini-instruct 4bit/8bit mixed 0.6196 4.2121 4.87 / 5.61 16 / 12 / 4
microsoft/Phi-3.5-mini-instruct fast 8bit mixed 0.6238 4.104 5.06 / 5.81 16 / 16 / 0
microsoft/Phi-3.5-mini-instruct fast 4bit/8bit mixed **0.6253** 4.0964 5.01 / 5.76 16 / 15 / 1
microsoft/Phi-3.5-mini-instruct fast train 8bit mixed 0.6238 **4.0879** 5.48 / 6.23 20 / 12 / 0
microsoft/Phi-3.5-mini-instruct fast train 4bit/8bit mixed 0.6246 4.1334 5.43 / 6.18 20 / 11 / 1

Table 7: Model Performance and Size Analysis Using the FastEWQ Method

Six quantization strategies are systematically applied to linear and embedding layers across multiple
model architectures. The 8-bit EWQ mixed quantization preserves original precision only for blocks
exceeding mean entropy values, while the 4-bit/8-bit variant introduced a dual threshold system aggressive 4-bit compression for low-entropy blocks and moderate 8-bit quantization for interme
|Model|Variant|Accuracy|Perplexity|Blocks / Total (GB)|raw / 8bit / 4bit|
|---|---|---|---|---|---|
|meta-llama/Meta-Llama-3.1-8B-Instruct meta-llama/Meta-Llama-3.1-8B-Instruct meta-llama/Meta-Llama-3.1-8B-Instruct meta-llama/Meta-Llama-3.1-8B-Instruct meta-llama/Meta-Llama-3.1-8B-Instruct meta-llama/Meta-Llama-3.1-8B-Instruct|8bit mixed 4bit/8bit mixed fast 8bit mixed fast 4bit/8bit mixed fast train 8bit mixed fast train 4bit/8bit mixed|0.6820 0.6822 0.6826 0.6833 0.6822 0.6824|2.2373 2.2305 2.2379 2.2332 2.2379 2.2325|10.46 / 13.21 10.27 / 13.02 10.46 / 13.21 10.38 / 13.13 10.46 / 13.21 10.38 / 13.13|19 / 13 / 0 19 / 11 / 2 19 / 13 / 0 19 / 12 / 1 19 / 13 / 0 19 / 12 / 1|
|Qwen/Qwen2-7B-Instruct Qwen/Qwen2-7B-Instruct Qwen/Qwen2-7B-Instruct Qwen/Qwen2-7B-Instruct Qwen/Qwen2-7B-Instruct Qwen/Qwen2-7B-Instruct|8bit mixed 4bit/8bit mixed fast 8bit mixed fast 4bit/8bit mixed fast train 8bit mixed fast train 4bit/8bit mixed|0.6894 0.6875 0.6894 0.6880 0.6876 0.6875|3.1906 3.2331 3.1906 3.2203 3.1827 3.2126|9.33 / 12.16 9.03 / 11.83 9.33 / 12.16 9.23 / 12.03 9.55 / 12.38 9.45 / 12.28|15 / 13 / 0 15 / 10 / 3 15 / 13 / 0 15 / 12 / 1 15 / 12 / 0 15 / 11 / 1|
|google/gemma-2-9b-it google/gemma-2-9b-it google/gemma-2-9b-it google/gemma-2-9b-it google/gemma-2-9b-it google/gemma-2-9b-it|8bit mixed 4bit/8bit mixed fast 8bit mixed fast 4bit/8bit mixed fast train 8bit mixed fast train 4bit/8bit mixed|0.6461 0.6471 0.6461 0.6458 0.6470 0.6453|4.3702 4.5795 4.3702 4.2577 4.3397 4.2561|12.37 / 15.03 11.85 / 14.51 12.37 / 15.03 12.28 / 14.94 12.11 / 14.77 12.02 / 14.68|25 / 17 / 0 25 / 11 / 6 25 / 17 / 0 25 / 16 / 1 22 / 20 / 0 22 / 19 / 1|
|microsoft/Phi-3.5-mini-instruct microsoft/Phi-3.5-mini-instruct microsoft/Phi-3.5-mini-instruct microsoft/Phi-3.5-mini-instruct microsoft/Phi-3.5-mini-instruct microsoft/Phi-3.5-mini-instruct|8bit mixed 4bit/8bit mixed fast 8bit mixed fast 4bit/8bit mixed fast train 8bit mixed fast train 4bit/8bit mixed|0.6238 0.6196 0.6238 0.6253 0.6238 0.6246|4.104 4.2121 4.104 4.0964 4.0879 4.1334|5.06 / 5.81 4.87 / 5.61 5.06 / 5.81 5.01 / 5.76 5.48 / 6.23 5.43 / 6.18|16 / 16 / 0 16 / 12 / 4 16 / 16 / 0 16 / 15 / 1 20 / 12 / 0 20 / 11 / 1|


-----

diate entropy regions. FastEWQ implementations replicate this behavior through classifier-driven
decisions, with the 8-bit variant applying uniform precision reduction and the 4-bit/8-bit version
introducing progressive compression toward later layers. The trained classifier variants demonstrate
similar patterns but with probabilistic quantization assignments reflecting their partial training exposure.

Notably, the 4-bit/8-bit FastEWQ mixed quantization specifically targets final transformer blocks
with the highest execution indices for maximal compression, capitalizing on our observation that
late-stage semantic integration layers exhibit unexpected quantization tolerance. This strategic precision allocation reduces memory footprint by 18-22% across tested models while maintaining perplexity within 0.5% of baseline performance. The schema-driven approach proves particularly effective for models with deep architectures (32+ layers), where traditional entropy calculation methods
incur prohibitive O(n) time complexity during deployment initialization.

Table 8 compares the transformer blocks selected for quantization by the weighted entropy EWQ
(ewq) analysis and two variants of the Fast classifier (fast and fast train). Each transformer
block is identified by its execution index (exec_index) within the LLM’s model schema. Notably,
the first transformer block starts at exec_index 2, since the first block in the LLM architecture
represents the token embedding block. Blocks are ordered by priority of quantization.

**Model** **Variant** **Quantization by exec_index** **4bit blocks** **Total** **fast / train**
meta-llama/Meta-Llama-3.1-8B-Instruct ewq 33, 13, 17, 16, 14, 15, 2, 19, 18, 32, 3, 11, 9 33, 13 13  meta-llama/Meta-Llama-3.1-8B-Instruct fast 33, 32, 31, 20, 19, 18, 17, 14, 13, 12, 11, 3, 2 33 13 3
meta-llama/Meta-Llama-3.1-8B-Instruct fast train 33, 32, 20, 19, 18, 17, 16, 14, 13, 11, 5, 3, 2 33 13 2 / 2
Qwen/Qwen2-7B-Instruct ewq 5, 16, 22, 23, 15, 9, 24, 28, 20, 14, 17, 21, 29 22, 16, 5 13  Qwen/Qwen2-7B-Instruct fast 29, 28, 24, 23, 22, 21, 20, 17, 16, 15, 14, 9, 5 29 13 0
Qwen/Qwen2-7B-Instruct fast train 29, 28, 24, 23, 22, 21, 17, 16, 15, 14, 13, 9 29 12 2 / 2
google/gemma-2-9b-it ewq 5, 2, 4, 3, 27, 26, 19, 7, 6, 25, 33, 31, 28, 30, 20, 32, 39 27, 26, 5, 4, 3, 2 17  google/gemma-2-9b-it fast 39, 33, 32, 31, 30, 28, 27, 26, 25, 20, 19, 7, 6, 5, 4, 3, 2 39 17 0
google/gemma-2-9b-it fast train 39, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 20, 19, 7, 6, 5, 4, 3, 2 39 20 +3 / +3
microsoft/Phi-3.5-mini-instruct ewq 31, 9, 4, 33, 16, 2, 3, 17, 14, 10, 13, 15, 20, 11, 12, 6 33, 31, 9, 4 16  microsoft/Phi-3.5-mini-instruct fast 33, 31, 20, 17, 16, 15, 14, 13, 12, 11, 10, 9, 6, 4, 3, 2 33 16 0
microsoft/Phi-3.5-mini-instruct fast train 33, 31, 17, 16, 15, 14, 13, 12, 11, 10, 3, 2 33 12 -4 / -4

Table 8: Comparison of transformer blocks selected for quantization by the EWQ analysis and two
variants of the FastEWQ classifier

Table 9 presents the average transformer block size in GB for each model based on the applied
quantization method.

**Model** **Blocks** **raw** **8bit** **4bit**
meta-llama/Meta-Llama-3.1-8B-Instruct 32 0.4062 0.2031 0.1079
Qwen/Qwen2-7B-Instruct 28 0.4341 0.2171 0.1153
google/gemma-2-9b-it 42 0.3692 0.1846 0.0981
microsoft/Phi-3.5-mini-instruct 32 0.2109 0.1055 0.0560

Table 9: Comparison of transformer model block sizes across different quantization types

**6.3.1** **Classifiers Comparison**

To compare the performance of two Fast classifiers based on the results obtained in the FastEWQ
Test Results section, we introduce a composite score formula that combines accuracy and perplexity
values from the MMLU benchmark. Since accuracy values range from 0 to 1, while perplexity
values are generally larger, we apply the natural logarithm to the perplexity values to bring them to
a comparable scale.

The composite score is computed using weights for perplexity (w1) and accuracy (w2), both of
which are set to 1, indicating that we value both metrics equally when calculating the score. The
composite score is given by

Composite Score = w1 · log(Perplexity) − _w2 · Accuracy_

The inputs for calculating the composite score for each model are provided in the table below:

|Model|Variant|Quantization by exec_index|4bit blocks|Total|fast / train|
|---|---|---|---|---|---|
|meta-llama/Meta-Llama-3.1-8B-Instruct meta-llama/Meta-Llama-3.1-8B-Instruct meta-llama/Meta-Llama-3.1-8B-Instruct|ewq fast fast train|33, 13, 17, 16, 14, 15, 2, 19, 18, 32, 3, 11, 9 33, 32, 31, 20, 19, 18, 17, 14, 13, 12, 11, 3, 2 33, 32, 20, 19, 18, 17, 16, 14, 13, 11, 5, 3, 2|33, 13 33 33|13 13 13|- 3 2 / 2|
|Qwen/Qwen2-7B-Instruct Qwen/Qwen2-7B-Instruct Qwen/Qwen2-7B-Instruct|ewq fast fast train|5, 16, 22, 23, 15, 9, 24, 28, 20, 14, 17, 21, 29 29, 28, 24, 23, 22, 21, 20, 17, 16, 15, 14, 9, 5 29, 28, 24, 23, 22, 21, 17, 16, 15, 14, 13, 9|22, 16, 5 29 29|13 13 12|- 0 2 / 2|
|google/gemma-2-9b-it google/gemma-2-9b-it google/gemma-2-9b-it|ewq fast fast train|5, 2, 4, 3, 27, 26, 19, 7, 6, 25, 33, 31, 28, 30, 20, 32, 39 39, 33, 32, 31, 30, 28, 27, 26, 25, 20, 19, 7, 6, 5, 4, 3, 2 39, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 20, 19, 7, 6, 5, 4, 3, 2|27, 26, 5, 4, 3, 2 39 39|17 17 20|- 0 +3 / +3|
|microsoft/Phi-3.5-mini-instruct microsoft/Phi-3.5-mini-instruct microsoft/Phi-3.5-mini-instruct|ewq fast fast train|31, 9, 4, 33, 16, 2, 3, 17, 14, 10, 13, 15, 20, 11, 12, 6 33, 31, 20, 17, 16, 15, 14, 13, 12, 11, 10, 9, 6, 4, 3, 2 33, 31, 17, 16, 15, 14, 13, 12, 11, 10, 3, 2|33, 31, 9, 4 33 33|16 16 12|- 0 -4 / -4|

|Model|Blocks|raw|8bit|4bit|
|---|---|---|---|---|
|meta-llama/Meta-Llama-3.1-8B-Instruct Qwen/Qwen2-7B-Instruct google/gemma-2-9b-it microsoft/Phi-3.5-mini-instruct|32 28 42 32|0.4062 0.4341 0.3692 0.2109|0.2031 0.2171 0.1846 0.1055|0.1079 0.1153 0.0981 0.0560|


-----

|Variant|Accuracy|Perplexity|
|---|---|---|
|fast 8bit mixed fast 4bit/8bit mixed fast train 8bit mixed fast train 4bit/8bit mixed|0.6826, 0.6894, 0.6461, 0.6238 0.6833, 0.688, 0.6458, 0.6253 0.6822, 0.6876, 0.647, 0.6238 0.6824, 0.6875, 0.6453, 0.6246|2.2379, 3.1906, 4.3702, 4.104 2.2332, 3.2203, 4.2577, 4.0964 2.2379, 3.1827, 4.3397, 4.0879 2.2325, 3.2126, 4.2561, 4.1334|


Table 10: Inputs for Composite Score Calculation (collected from Table 7)

To compare the performance of the classifiers, we observe three different combinations of classifiers,
including a trained FastEWQ classifier on the entire dataset (fast) and a trained FastEWQ classifier
on 70% of the samples (fast train). For comparison metrics, we use the paired t-test and Cohen’s d.

**Paired t-test is a statistical method used to compare the means of two related groups to determine**
if there is a significant difference between them. This test is useful when comparing two sets of
measurements taken from the same group or sample, such as performance scores of the same classifier under different conditions. The null hypothesis assumes that there is no significant difference
between the paired values.

The test statistic is calculated as

_d¯_
_t =_
_sd/[√]n_

where _d[¯] is the mean of the differences between paired observations, sd being the standard deviation_
of the differences, and n is the number of paired observations. The p-value is then obtained to
determine whether the difference is statistically significant.

_p-value_ **Significance**
_p < 0.05_ significant
0.05 ≤ _p < 0.10_ marginally significant
_p ≥_ 0.10 not significant

Table 11: Significance levels for p-values

**Cohen’s d is a measure of the effect size, or the magnitude of the difference between two groups. It**
is commonly used to quantify the size of the difference in means relative to the variability observed
in the data. Cohen’s d is calculated as

_X¯1 −_ _X[¯]2_
_d =_
_sp_

where _X[¯]1 and_ _X[¯]2 are the sample means, and sp is the pooled standard deviation, which combines the_
standard deviations of the two groups. Cohen’s d helps to understand not just whether a difference
exists (like the p-value) but how large that difference is.

**Cohen’s d Value** **Effect Size Interpretation**
_d < 0.2_ negligible
0.2 ≤ _d < 0.5_ small
0.5 ≤ _d < 0.8_ medium
_d > 0.8_ large

Table 12: Interpretation of Cohen’s d values

The results of the paired classifier comparison are shown in the Figure 7 and Table 13 below.

|p-value|Significance|
|---|---|
|p < 0.05|significant|
|0.05 ≤p < 0.10|marginally significant|
|p ≥0.10|not significant|

|Cohen’s d Value|Effect Size Interpretation|
|---|---|
|d < 0.2|negligible|
|0.2 ≤d < 0.5|small|
|0.5 ≤d < 0.8|medium|
|d > 0.8|large|


-----

Figure 7: comparison of composite scores between classifiers

**Comparison** **Variant** **Abs Diff** _t-statistic_ _p-value / Effect_ **Cohen’s d / Effect**
fast / fast train fast 8bit mixed and fast 4bit/8bit mixed 0.0031 0.2551 0.806 / not significant 0.0016 / not significant
fast / fast train fast train 8bit mixed and fast train 4bit/8bit mixed not significant not significant
fast / fast train (8bit) fast 8bit mixed 0.0032 1.6215 0.2034 / not significant 0.0107 / not significant
fast / fast train (8bit) fast train 8bit mixed not significant not significant
fast / fast train (mixed) fast 4bit/8bit mixed 0.0031 -0.825 0.4699 / not significant -0.0076 / not significant
fast / fast train (mixed) fast train 4bit/8bit mixed not significant not significant

Table 13: Statistical comparison of composite scores between classifiers

The paired classifier comparison results, as shown in Table 13, summarize the outcomes of statistical tests performed on various classifier variants. The key metrics considered are the Abs Diff,
_t-statistic, p-value, and Cohen’s d. The Abs Diff metric represents the mean absolute difference_
between composite scores, as shown in Figure 7. We observe that the value for classifier pairs is
very small, approximately 0.0031, and from the figure, we observe that the composite scores nearly
overlap between classifiers, indicating a negligible difference when comparing them.

Based on the statistical analysis, we conclude that there is no significant difference in performance
between the trained classifier variants under the tested conditions. This implies that a classifier
trained on 70% of the training set effectively captures the behavior of one trained on the entire
dataset. Since the trained classifier is not overfitted, it demonstrates promising generalization properties on unseen model architectures, as the underlying concept is retained. The t-statistic values
indicate relatively small differences, and the corresponding p-values confirm that none of these differences are statistically significant. Furthermore, Cohen’s d values suggest negligible effect sizes,
reinforcing the conclusion that the classifiers perform similarly across different configurations.

**6.4** **Behavior Capture and Summary**

In Table 14, we present the final results, highlighting relative differences in accuracy, perplexity,
model size, and EWQ analysis time complexity.

Based on the test results, we conclude that the EWQ method significantly enhances the performance
of the meta-llama/Meta-Llama-3.1-8B-Instruct model. When applying the EWQ 4-bit/8-bit
mixed quantization optimization, the model achieves lower perplexity than its unquantized counterpart while maintaining comparable accuracy, indicating improved coherence and overall performance.

|Comparison|Variant|Abs Diff|t-statistic|p-value / Effect|Cohen’s d / Effect|
|---|---|---|---|---|---|
|fast / fast train fast / fast train|fast 8bit mixed and fast 4bit/8bit mixed fast train 8bit mixed and fast train 4bit/8bit mixed|0.0031|0.2551|0.806 / not significant not significant|0.0016 / not significant not significant|
|fast / fast train (8bit) fast / fast train (8bit)|fast 8bit mixed fast train 8bit mixed|0.0032|1.6215|0.2034 / not significant not significant|0.0107 / not significant not significant|
|fast / fast train (mixed) fast / fast train (mixed)|fast 4bit/8bit mixed fast train 4bit/8bit mixed|0.0031|-0.825|0.4699 / not significant not significant|-0.0076 / not significant not significant|


-----

For the Qwen/Qwen2-7B-Instruct, FastEWQ 8-bit mixed quantization achieves the highest accuracy. The minimal perplexity difference between the quantized and unquantized models suggests
that quantization does not substantially impact performance.

In the case of google/gemma-2-9b-it, the unquantized version performs best. However, the optimal trade-off between accuracy, perplexity, and space efficiency is achieved through 8-bit or 8-bit
mixed quantization of the transformer blocks, allowing the model to fit on devices with 16GB of
memory. The best-performing model is obtained using the FastEWQ classifier trained on a subset of the training dataset, demonstrating strong generalization. This model selects three additional
transformer blocks for quantization, and with strict quantization, it achieves the best perplexity and
accuracy for the 8-bit quantization strategy while reducing model size by 19.77% compared to the
unquantized version.

Notable results are observed with Microsoft’s Phi model, microsoft/Phi-3.5-mini-instruct.
The quantized models perform nearly identically to their unquantized counterparts. Specifically, a
FastEWQ classifier trained on a subset of the training dataset achieves almost the same accuracy and
perplexity as the unquantized model while reducing its size by 18%.

The FastEWQ classifier trained on a subset of the training dataset generally performs as well as
or better than an overfitted version. A comparison between the two reveals no significant differences, only minor variations, indicating that the subset-trained FastEWQ classifier generalizes better to unseen models. For 8-bit quantization, FastEWQ effectively replicates EWQ’s results while
offering O(1) time complexity for analysis, compared to O(n) for standard EWQ. Additionally,
FastEWQ outperforms global quantization, which represents only a subset of the possible combinations FastEWQ can generate, making global quantization a special case of FastEWQ.

In conclusion, considering the trade-offs between accuracy, perplexity, model size, time complexity,
generalization on unseen models, and real-time analysis support, the optimal choice is the FastEWQ
optimizer trained on a subset of the training data.

**Model** **Variant** **Accuracy** **Perplexity** **Size / Total (GB)** **Complexity**
meta-llama/Meta-Llama-3.1-8B-Instruct raw 0.6837 2.2379 16.07  meta-llama/Meta-Llama-3.1-8B-Instruct 4bit -3.2% 5.02% -71.87% / 4.52 O(1)
meta-llama/Meta-Llama-3.1-8B-Instruct 8bit -0.47% 0.01% -46.92% / 8.53 O(1)
meta-llama/Meta-Llama-3.1-8B-Instruct 8bit mixed -0.25% -0.03% -17.8% / 13.21 O(n)
meta-llama/Meta-Llama-3.1-8B-Instruct 4bit/8bit mixed -0.22% -0.33% -18.98% / 13.02 O(n)
meta-llama/Meta-Llama-3.1-8B-Instruct fast 8bit mixed -0.16% 0.0% -17.8% / 13.21 O(1)
meta-llama/Meta-Llama-3.1-8B-Instruct fast 4bit/8bit mixed -0.06% -0.21% -18.29% / 13.13 O(1)
meta-llama/Meta-Llama-3.1-8B-Instruct fast train 8bit mixed -0.22% 0.0% -17.8% / 13.21 O(1)
meta-llama/Meta-Llama-3.1-8B-Instruct fast train 4bit/8bit mixed -0.19% -0.24% -18.29% / 13.13 O(1)
Qwen/Qwen2-7B-Instruct raw 0.6872 3.1722 15.23  Qwen/Qwen2-7B-Instruct 4bit -1.99% 5.7% -62.9% / 5.65 O(1)
Qwen/Qwen2-7B-Instruct 8bit -0.51% 0.56% -43.01% / 8.68 O(1)
Qwen/Qwen2-7B-Instruct 8bit mixed 0.32% 0.58% -20.16% / 12.16 O(n)
Qwen/Qwen2-7B-Instruct 4bit/8bit mixed 0.04% 1.92% -22.32% / 11.83 O(n)
Qwen/Qwen2-7B-Instruct fast 8bit mixed 0.32% 0.58% -20.16% / 12.16 O(1)
Qwen/Qwen2-7B-Instruct fast 4bit/8bit mixed 0.12% 1.52% -21.01% / 12.03 O(1)
Qwen/Qwen2-7B-Instruct fast train 8bit mixed 0.06% 0.33% -18.71% / 12.38 O(1)
Qwen/Qwen2-7B-Instruct fast train 4bit/8bit mixed 0.04% 1.27% -19.37% / 12.28 O(1)
google/gemma-2-9b-it raw 0.6505 4.1013 18.41  google/gemma-2-9b-it 4bit -3.4% 52.57% -66.11% / 6.24 O(1)
google/gemma-2-9b-it 8bit -0.86% 5.42% -48.61% / 9.46 O(1)
google/gemma-2-9b-it 8bit mixed -0.68% 6.56% -18.36% / 15.03 O(n)
google/gemma-2-9b-it 4bit/8bit mixed -0.52% 11.66% -21.18% / 14.51 O(n)
google/gemma-2-9b-it fast 8bit mixed -0.68% 6.56% -18.36% / 15.03 O(1)
google/gemma-2-9b-it fast 4bit/8bit mixed -0.72% 3.81% -18.85% / 14.94 O(1)
google/gemma-2-9b-it fast train 8bit mixed -0.54% 5.81% -19.77% / 14.77 O(1)
google/gemma-2-9b-it fast train 4bit/8bit mixed -0.8% 3.77% -20.26% / 14.68 O(1)
microsoft/Phi-3.5-mini-instruct raw 0.6243 4.0805 7.62  microsoft/Phi-3.5-mini-instruct 4bit 0.14% 11.32% -69.69% / 2.31 O(1)
microsoft/Phi-3.5-mini-instruct 8bit -0.29% 0.33% -47.38% / 4.01 O(1)
microsoft/Phi-3.5-mini-instruct 8bit mixed -0.08% 0.58% -23.75% / 5.81 O(n)
microsoft/Phi-3.5-mini-instruct 4bit/8bit mixed -0.75% 3.23% -26.38% / 5.61 O(n)
microsoft/Phi-3.5-mini-instruct fast 8bit mixed -0.08% 0.58% -23.75% / 5.81 O(1)
microsoft/Phi-3.5-mini-instruct fast 4bit/8bit mixed 0.16% 0.39% -24.41% / 5.76 O(1)
microsoft/Phi-3.5-mini-instruct fast train 8bit mixed -0.08% 0.18% -18.24% / 6.23 O(1)
microsoft/Phi-3.5-mini-instruct fast train 4bit/8bit mixed 0.05% 1.3% -18.9% / 6.18 O(1)

Table 14: MMLU performance vs. model size across quantization methods

|Model|Variant|Accuracy|Perplexity|Size / Total (GB)|Complexity|
|---|---|---|---|---|---|
|meta-llama/Meta-Llama-3.1-8B-Instruct meta-llama/Meta-Llama-3.1-8B-Instruct meta-llama/Meta-Llama-3.1-8B-Instruct meta-llama/Meta-Llama-3.1-8B-Instruct meta-llama/Meta-Llama-3.1-8B-Instruct meta-llama/Meta-Llama-3.1-8B-Instruct meta-llama/Meta-Llama-3.1-8B-Instruct meta-llama/Meta-Llama-3.1-8B-Instruct meta-llama/Meta-Llama-3.1-8B-Instruct|raw 4bit 8bit 8bit mixed 4bit/8bit mixed fast 8bit mixed fast 4bit/8bit mixed fast train 8bit mixed fast train 4bit/8bit mixed|0.6837 -3.2% -0.47% -0.25% -0.22% -0.16% -0.06% -0.22% -0.19%|2.2379 5.02% 0.01% -0.03% -0.33% 0.0% -0.21% 0.0% -0.24%|16.07 -71.87% / 4.52 -46.92% / 8.53 -17.8% / 13.21 -18.98% / 13.02 -17.8% / 13.21 -18.29% / 13.13 -17.8% / 13.21 -18.29% / 13.13|- O(1) O(1) O(n) O(n) O(1) O(1) O(1) O(1)|
|Qwen/Qwen2-7B-Instruct Qwen/Qwen2-7B-Instruct Qwen/Qwen2-7B-Instruct Qwen/Qwen2-7B-Instruct Qwen/Qwen2-7B-Instruct Qwen/Qwen2-7B-Instruct Qwen/Qwen2-7B-Instruct Qwen/Qwen2-7B-Instruct Qwen/Qwen2-7B-Instruct|raw 4bit 8bit 8bit mixed 4bit/8bit mixed fast 8bit mixed fast 4bit/8bit mixed fast train 8bit mixed fast train 4bit/8bit mixed|0.6872 -1.99% -0.51% 0.32% 0.04% 0.32% 0.12% 0.06% 0.04%|3.1722 5.7% 0.56% 0.58% 1.92% 0.58% 1.52% 0.33% 1.27%|15.23 -62.9% / 5.65 -43.01% / 8.68 -20.16% / 12.16 -22.32% / 11.83 -20.16% / 12.16 -21.01% / 12.03 -18.71% / 12.38 -19.37% / 12.28|- O(1) O(1) O(n) O(n) O(1) O(1) O(1) O(1)|
|google/gemma-2-9b-it google/gemma-2-9b-it google/gemma-2-9b-it google/gemma-2-9b-it google/gemma-2-9b-it google/gemma-2-9b-it google/gemma-2-9b-it google/gemma-2-9b-it google/gemma-2-9b-it|raw 4bit 8bit 8bit mixed 4bit/8bit mixed fast 8bit mixed fast 4bit/8bit mixed fast train 8bit mixed fast train 4bit/8bit mixed|0.6505 -3.4% -0.86% -0.68% -0.52% -0.68% -0.72% -0.54% -0.8%|4.1013 52.57% 5.42% 6.56% 11.66% 6.56% 3.81% 5.81% 3.77%|18.41 -66.11% / 6.24 -48.61% / 9.46 -18.36% / 15.03 -21.18% / 14.51 -18.36% / 15.03 -18.85% / 14.94 -19.77% / 14.77 -20.26% / 14.68|- O(1) O(1) O(n) O(n) O(1) O(1) O(1) O(1)|
|microsoft/Phi-3.5-mini-instruct microsoft/Phi-3.5-mini-instruct microsoft/Phi-3.5-mini-instruct microsoft/Phi-3.5-mini-instruct microsoft/Phi-3.5-mini-instruct microsoft/Phi-3.5-mini-instruct microsoft/Phi-3.5-mini-instruct microsoft/Phi-3.5-mini-instruct microsoft/Phi-3.5-mini-instruct|raw 4bit 8bit 8bit mixed 4bit/8bit mixed fast 8bit mixed fast 4bit/8bit mixed fast train 8bit mixed fast train 4bit/8bit mixed|0.6243 0.14% -0.29% -0.08% -0.75% -0.08% 0.16% -0.08% 0.05%|4.0805 11.32% 0.33% 0.58% 3.23% 0.58% 0.39% 0.18% 1.3%|7.62 -69.69% / 2.31 -47.38% / 4.01 -23.75% / 5.81 -26.38% / 5.61 -23.75% / 5.81 -24.41% / 5.76 -18.24% / 6.23 -18.9% / 6.18|- O(1) O(1) O(n) O(n) O(1) O(1) O(1) O(1)|


-----

**6.5** **Further Analysis of FastEWQ Optimization**

Expanding on the results discussed in Table 14, we further analyze the behavior of the FastEWQ
optimizer trained on the full dataset. Notably, for 8-bit quantization, FastEWQ closely approximates
the selections made by the EWQ method, reinforcing its effectiveness as a lightweight alternative.
This alignment is particularly relevant given the role of 8-bit quantization in matrix decomposition
for attention computations, as seen in LLM.int8() [Dettmers et al., 2022b].

For models such as `Qwen/Qwen2-7B-Instruct,` `google/gemma-2-9b-it,` and
`microsoft/Phi-3.5-mini-instruct,` FastEWQ identifies nearly the same transformer
blocks for quantization as EWQ, reflecting prior observations that efficient classifiers can
approximate computationally expensive sensitivity analyses [Li et al., 2021]. However, for
```
meta-llama/Meta-Llama-3.1-8B-Instruct, architectural variations result in minor deviations

```
in block selection. Despite these differences, the impact on performance remains negligible
(∼ 6 × 10[−][4]), aligning with findings that minor quantization discrepancies are often absorbed by
model redundancy [Xiao et al., 2024].

Beyond accuracy and perplexity, FastEWQ’s O(1) analysis time complexity provides a substantial speedup over iterative EWQ, with at least a 100x efficiency gain. Notably, for
```
google/gemma-2-9b-it, training FastEWQ on 70% of the dataset outperforms the full-dataset

```
variant by selecting three additional blocks for quantization, leading to a 19.77% reduction in model
size. This finding supports previous research [Ashkboos et al., 2024], which highlights how subset
training mitigates overfitting in quantization controllers.

In comparing 8-bit mixed and fast 8-bit mixed quantization, FastEWQ consistently reproduces
EWQ’s behavior across most models, further validating its reliability. The flexibility of this method
surpasses that of global quantization, representing only a single point in the broader mixed-precision
strategy space [Gong et al., 2019]. Consequently, FastEWQ offers an optimal balance between accuracy, perplexity, model size, and computational efficiency, reinforcing its role as the preferred
quantization strategy.

### 7 Conclusion

We introduced Entropy-Weighted Quantization (EWQ), a novel architecture- and size-agnostic
method for post-training quantization of LLMs. By analyzing entropy distributions across transformer blocks, EWQ identifies layers amenable to precision reduction while preserving critical highentropy components. Our experiments demonstrate that EWQ maintains MMLU accuracy within
0.5% of full-precision models while reducing memory usage by up to 18%—outperforming uniform
quantization baselines like GPTQ [Frantar et al., 2022]—and achieves superior perplexity in some
cases, a phenomenon attributed to quantization-induced regularization. The method’s effectiveness
spans diverse architectures (1.6B to 70B parameters), including LLaMA, Qwen, Phi, and Gemma,
proving its universality across model scales and designs.

FastEWQ, an optimized variant, eliminates weight-loading requirements through a classifier that
predicts quantization suitability using execution index, parameter count, and total blocks. This
approach achieves 80% classification accuracy with O(1) time complexity, enabling real-time deployment decisions. Both EWQ and FastEWQ significantly enhance the feasibility of running stateof-the-art LLMs on resource-constrained devices, such as 16GB consumer hardware, without performance degradation.

**7.1** **Future Directions**

As we advance toward more efficient and adaptable quantization techniques, several promising research directions emerge. The evolution of model compression must balance precision, computational efficiency, and hardware compatibility, ensuring that quantization techniques remain both
effective and scalable. In this context, we identify three key areas for further exploration.

1. Architectural Generalization: Adapting EWQ principles to non-transformer architectures
(e.g., SSMs [Gu and Dao, 2023], RWKV [Peng et al., 2023]) and multimodal models.


-----

2. Precision Frontiers: Exploring sub-4-bit quantization (2-bit, 1.58-bit [Ashkboos et al.,
2023]) combined with entropy-aware sparsity, building on sparsity-aware methods like
SparseGPT [Frantar and Alistarh, 2023].
3. System Integration: Co-designing EWQ with emerging memory technologies (HBM3,
CXL) and kernel-level optimizations, inspired by FlashAttention [Dao et al., 2022] and
vLLM [Kwon et al., 2023].

Additional opportunities include theoretical investigations into entropy-robustness relationships, extending information-theoretic frameworks like [Shwartz-Ziv and Tishby, 2017], and federated learning applications where FastEWQ’s metadata-driven approach could enable dynamic precision allocation across distributed systems [Kairouz et al., 2021]. Finally, integrating EWQ with activation
quantization [Yao et al., 2022] and KV cache compression [Liu et al., 2025] could unlock end-to-end
efficient inference pipelines for next-generation LLMs.


### References
A. Abdolrashidi, L. Wang, S. Agrawal, J. Malmaud,
O. Rybakov, C. Leichner, and L. Lew. Paretooptimal quantized resnet is mostly 4-bit. In 2021
_IEEE/CVF Conference on Computer Vision and_
_Pattern Recognition Workshops (CVPRW), page_
3085–3093. IEEE, June 2021. doi: 10.1109/
[cvprw53098.2021.00345. URL http://dx.doi.](http://dx.doi.org/10.1109/CVPRW53098.2021.00345)
```
 org/10.1109/CVPRW53098.2021.00345.

```
S. Ashkboos, I. Markov, E. Frantar, T. Zhong,
X. Wang, J. Ren, T. Hoefler, and D. Alistarh.
QUIK: Towards end-to-end 4-bit inference on generative large language models. arXiv:2310.09259,
2023.

S. Ashkboos, A. Mohtashami, M. L. Croci, B. Li,
P. Cameron, M. Jaggi, D. Alistarh, T. Hoefler, and
J. Hensman. QuaRot: Outlier-free 4-bit inference
in rotated llms. arXiv:2404.00456, 2024.

T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss,
G. Krueger, T. Henighan, R. Child, A. Ramesh,
D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen,
E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark,
C. Berner, S. McCandlish, A. Radford, I. Sutskever,
and D. Amodei. Language models are few-shot
learners. _CoRR, abs/2005.14165, 2020._ URL
```
 https://arxiv.org/abs/2005.14165.

```
Z. Cai and N. Vasconcelos. Rethinking differentiable
search for mixed-precision neural networks. CoRR,
abs/2004.05795, 2020. [URL https://arxiv.](https://arxiv.org/abs/2004.05795)
```
 org/abs/2004.05795.

```
J. Chee, Y. Cai, V. Kuleshov, and C. D. Sa. Quip: 2-bit
quantization of large language models with guarantees, 2024. [URL https://arxiv.org/abs/](https://arxiv.org/abs/2307.13304)
```
 2307.13304.

```
J. Choi, Z. Wang, S. Venkataramani, P. I.-J. Chuang,
V. Srinivasan, and K. Gopalakrishnan. PACT: Parameterized clipping activation for quantized neural
networks. arXiv:1805.06085, 2018.

A. Chowdhery, S. Narang, J. Devlin, M. Bosma,
G. Mishra, A. Roberts, P. Barham, H. W. Chung,
C. Sutton, S. Gehrmann, P. Schuh, K. Shi,
S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes,
Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du,
B. Hutchinson, R. Pope, J. Bradbury, J. Austin,
M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya,


S. Ghemawat, S. Dev, H. Michalewski, X. Garcia,
V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov,
R. Sepassi, D. Dohan, S. Agrawal, M. Omernick,
A. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz,
E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou,
X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta,
J. Wei, K. Meier-Hellstern, D. Eck, J. Dean,
S. Petrov, and N. Fiedel. Palm: Scaling language
modeling with pathways, 2022. [URL https://](https://arxiv.org/abs/2204.02311)
```
 arxiv.org/abs/2204.02311.

```
T. M. Cover and J. A. Thomas. Elements of informa_tion theory. John Wiley & Sons, 2006._

T. Dao, D. Y. Fu, S. Ermon, et al. Flashattention:
Fast and memory-efficient exact attention with ioawareness. arXiv preprint arXiv:2205.14135, 2022.

T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for
transformers at scale. Advances in Neural Informa_tion Processing Systems, 35:30318–30332, 2022a._

T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. Llm.int8(): 8-bit matrix multiplication
for transformers at scale, 2022b. [URL https:](https://arxiv.org/abs/2208.07339)
```
 //arxiv.org/abs/2208.07339.

```
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova.
Bert: Pre-training of deep bidirectional transformers for language understanding, 2019. URL
```
 https://arxiv.org/abs/1810.04805.

```
Z. Dong, Z. Yao, Y. Cai, D. Arfeen, A. Gholami,
M. W. Mahoney, and K. Keutzer. Hawq-v2: Hessian aware trace-weighted quantization of neural
networks, 2019a. [URL https://arxiv.org/](https://arxiv.org/abs/1911.03852)
```
 abs/1911.03852.

```
Z. Dong, Z. Yao, A. Gholami, M. Mahoney, and
K. Keutzer. Hawq: Hessian aware quantization
of neural networks with mixed-precision, 2019b.
[URL https://arxiv.org/abs/1905.03696.](https://arxiv.org/abs/1905.03696)

Z. Dong, Z. Yao, A. Gholami, M. W. Mahoney, and
K. Keutzer. Hawq: Hessian aware quantization of
neural networks with mixed-precision. In Proceed_ings of the IEEE/CVF International Conference on_
_Computer Vision (ICCV), October 2019c._

E. Frantar and D. Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot,
2023. [URL https://arxiv.org/abs/2301.](https://arxiv.org/abs/2301.00774)
```
 00774.

```

-----

E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. GPTQ: Accurate post-training quantization for generative pre-trained transformers.
_arXiv:2210.17323, 2022._

P. Ganesh, Y. Chen, X. Lou, M. A. Khan, Y. Yang,
H. Sajjad, P. Nakov, D. Chen, and M. Winslett.
Compressing large-scale transformer-based models: A case study on bert. _Transactions of_
_the Association for Computational Linguistics, 9:_
1061–1080, 2021. ISSN 2307-387X. doi: 10.
[1162/tacl_a_00413. URL http://dx.doi.org/](http://dx.doi.org/10.1162/tacl_a_00413)
```
 10.1162/tacl_a_00413.

```
A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer. A survey of quantization methods for efficient neural network inference.
In Low-Power Computer Vision, pages 291–326.
Chapman and Hall/CRC, 2022.

C. Gong, Z. Jiang, D. Wang, Y. Lin, Q. Liu, and D. Z.
Pan. Mixed precision neural architecture search for
energy efficient deep learning. 2019 IEEE/ACM In_ternational Conference on Computer-Aided Design_
_[(ICCAD), pages 1–7, 2019. URL https://api.](https://api.semanticscholar.org/CorpusID:209495652)_
```
 semanticscholar.org/CorpusID:209495652.

```
A. Gu and T. Dao. Mamba: Linear-time sequence
modeling with selective state spaces. arXiv preprint
_arXiv:2312.00752, 2023._

D. Hendrycks, C. Burns, S. Basart, A. Zou,
M. Mazeika, D. Song, and J. Steinhardt. Measuring
massive multitask language understanding. arXiv
_preprint arXiv:2009.03300, 2020._

J. Hoffmann, S. Borgeaud, A. Mensch,
E. Buchatskaya, T. Cai, E. Rutherford,
D. de Las Casas, L. A. Hendricks, J. Welbl,
A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy,
S. Osindero, K. Simonyan, E. Elsen, J. W. Rae,
O. Vinyals, and L. Sifre. Training computeoptimal large language models, 2022. URL
```
 https://arxiv.org/abs/2203.15556.

```
W. Huang, H. Qin, Y. Liu, Y. Li, Q. Liu, X. Liu,
L. Benini, M. Magno, S. Zhang, and X. QI. SlimLLM: Salience-driven mixed-precision quantiza[tion for large language models, 2025. URL https:](https://openreview.net/forum?id=tjlTczcnPz)
```
 //openreview.net/forum?id=tjlTczcnPz.

```
I. Hubara, Y. Nahshan, Y. Hanani, R. Banner, and
D. Soudry. Accurate post training quantization with
small calibration sets. In International Conference
_on Machine Learning, pages 4466–4475, 2021._

Y. Jin, Y. Chen, Y. Huang, Z. Chen, S. Tu, and F. Xu.
Dynast: Dynamic sparse transformer for exemplarequipped 3d human generation. _arXiv preprint_
_arXiv:2112.06156, 2021._

P. Kairouz, H. B. McMahan, B. Avent, A. Bellet,
M. Bennis, A. N. Bhagoji, K. Bonawitz, Z. Charles,
G. Cormode, R. Cummings, R. G. L. D’Oliveira,
H. Eichner, S. E. Rouayheb, D. Evans, J. Gardner, Z. Garrett, A. Gascón, B. Ghazi, P. B. Gibbons, M. Gruteser, Z. Harchaoui, C. He, L. He,
Z. Huo, B. Hutchinson, J. Hsu, M. Jaggi, T. Javidi,
G. Joshi, M. Khodak, J. Koneˇcný, A. Korolova,
F. Koushanfar, S. Koyejo, T. Lepoint, Y. Liu,
P. Mittal, M. Mohri, R. Nock, A. Özgür, R. Pagh,
M. Raykova, H. Qi, D. Ramage, R. Raskar,
D. Song, W. Song, S. U. Stich, Z. Sun, A. T. Suresh,


F. Tramèr, P. Vepakomma, J. Wang, L. Xiong,
Z. Xu, Q. Yang, F. X. Yu, H. Yu, and S. Zhao.
Advances and open problems in federated learning,
2021. [URL https://arxiv.org/abs/1912.](https://arxiv.org/abs/1912.04977)
```
 04977.

```
S. Kim, C. Hooper, A. Gholami, Z. Dong, X. Li,
S. Shen, M. W. Mahoney, and K. Keutzer.
SqueezeLLM: Dense-and-sparse quantization.
_arXiv:2306.07629, 2023a._

Y. J. Kim, R. Fahim, and H. H. Awadalla. Mixture
of quantized experts (moqe): Complementary effect of low-bit quantization and robustness, 2023b.
[URL https://arxiv.org/abs/2310.02410.](https://arxiv.org/abs/2310.02410)

Y. J. Kim, R. Henry, R. Fahim, and H. H. Awadalla.
Finequant: Unlocking efficiency with fine-grained
weight-only quantization for llms, 2023c. URL
```
 https://arxiv.org/abs/2308.09723.

```
G. Kobayashi, T. Kuribayashi, S. Yokoi, and K. Inui.
Analyzing feed-forward blocks in transformers
through the lens of attention maps, 2024. URL
```
 https://arxiv.org/abs/2302.00456.

```
W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng,
C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica.
Efficient memory management for large language
model serving with pagedattention, 2023. URL
```
 https://arxiv.org/abs/2309.06180.

```
Y. Li, R. Gong, X. Tan, Y. Yang, P. Hu, Q. Zhang,
F. Yu, W. Wang, and S. Gu. Brecq: Pushing the limit of post-training quantization by
block reconstruction. _ArXiv, abs/2102.05426,_
2021. [URL https://api.semanticscholar.](https://api.semanticscholar.org/CorpusID:231861390)
```
 org/CorpusID:231861390.

```
J. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.C. Wang, G. Xiao, X. Dang, C. Gan, and S. Han.
AWQ: Activation-aware weight quantization for
on-device llm compression and acceleration. Pro_ceedings of Machine Learning and Systems, 6:87–_
100, 2024.

X. Liu, Z. Tang, H. Chen, P. Dong, Z. Li, X. Zhou,
B. Li, X. Hu, and X. Chu. Can llms maintain fundamental abilities under kv cache compression?, 2025. [URL https://arxiv.org/abs/](https://arxiv.org/abs/2502.01941)
```
 2502.01941.

```
Z. Liu, C. Zhao, I. Fedorov, B. Soran, D. Choudhary, R. Krishnamoorthi, V. Chandra, Y. Tian, and
T. Blankevoort. SpinQuant: Llm quantization with
learned rotations. arXiv:2405.16406, 2024.

Meta. Introducing Meta Llama 3: The most capable openly available LLM to date., 2024. URL
```
 https://ai.meta.com/blog/meta-llama-3/.

```
E. Park, J. Ahn, and S. Yoo. Weighted-entropy-based
quantization for deep neural networks. In 2017
_IEEE Conference on Computer Vision and Pattern_
_Recognition (CVPR), pages 7197–7205, 2017. doi:_
10.1109/CVPR.2017.761.

G. Park, B. Park, S. J. Kwon, B. Kim, Y. Lee, and
D. Lee. nuqmm: Quantized matmul for efficient
inference of large-scale generative language models. arXiv:2206.09557, 2022.


-----

P. Passban, Y. Wu, M. Rezagholizadeh, and Q. Liu.
Gobo: Quantizing attention-based nlp models for
low latency and energy efficient inference. arXiv
_preprint arXiv:2107.08427, 2021._

B. Peng, E. Alcaide, Q. Anthony, A. Albalak,
S. Arcadinho, S. Biderman, H. Cao, X. Cheng,
M. Chung, M. Grella, K. K. GV, X. He, H. Hou,
J. Lin, P. Kazienko, J. Kocon, J. Kong, B. Koptyra, H. Lau, K. S. I. Mantri, F. Mom, A. Saito,
G. Song, X. Tang, B. Wang, J. S. Wind, S. Wozniak,
R. Zhang, Z. Zhang, Q. Zhao, P. Zhou, Q. Zhou,
J. Zhu, and R.-J. Zhu. Rwkv: Reinventing rnns for
[the transformer era, 2023. URL https://arxiv.](https://arxiv.org/abs/2305.13048)
```
 org/abs/2305.13048.

```
T. P. Pires, A. V. Lopes, Y. Assogba, and H. Setiawan.
One wide feedforward is all you need, 2023. URL
```
 https://arxiv.org/abs/2309.01826.

```
U. Saxena, S. Sharify, K. Roy, and X. Wang. Resq:
Mixed-precision quantization of large language
models with low-rank residuals, 2024.

S. Shen, Z. Dong, J. Ye, L. Ma, Z. Yao, A. Gholami,
M. W. Mahoney, and K. Keutzer. Q-bert: Hessian based ultra low precision quantization of bert,
2019. [URL https://arxiv.org/abs/1909.](https://arxiv.org/abs/1909.05840)
```
 05840.

```
X. Shen, Z. Kong, C. Yang, Z. Han, L. Lu, P. Dong,
C. Lyu, C. hsiang Li, X. Guo, Z. Shu, W. Niu,
M. Leeser, P. Zhao, and Y. Wang. Edgeqat: Entropy and distribution guided quantization-aware
training for the acceleration of lightweight llms on
[the edge, 2024. URL https://arxiv.org/abs/](https://arxiv.org/abs/2402.10787)
```
 2402.10787.

```
R. Shwartz-Ziv and N. Tishby. Opening the black
box of deep neural networks via information, 2017.
[URL https://arxiv.org/abs/1703.00810.](https://arxiv.org/abs/1703.00810)

N. Tishby and N. Zaslavsky. Deep learning and the in[formation bottleneck principle, 2015. URL https:](https://arxiv.org/abs/1503.02406)
```
 //arxiv.org/abs/1503.02406.

```

K. Wang, Z. Liu, Y. Lin, J. Lin, and S. Han. HAQ:
hardware-aware automated quantization. _CoRR,_
[abs/1811.08886, 2018. URL http://arxiv.org/](http://arxiv.org/abs/1811.08886)
```
 abs/1811.08886.

```
X. Wei, Y. Zhang, Y. Li, X. Zhang, R. Gong, J. Guo,
and X. Liu. Outlier suppression+: Accurate quantization of large language models by equivalent and
[optimal shifting and scaling, 2023. URL https:](https://arxiv.org/abs/2304.09145)
```
 //arxiv.org/abs/2304.09145.

```
G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth,
and S. Han. Smoothquant: Accurate and efficient
post-training quantization for large language mod[els, 2024. URL https://arxiv.org/abs/2211.](https://arxiv.org/abs/2211.10438)
```
 10438.

```
Y. Xu, Y. Wang, A. Zhou, W. Lin, and H. Xiong. Deep
neural network compression with single and multiple level quantization. _CoRR, abs/1803.03289,_
2018. [URL http://arxiv.org/abs/1803.](http://arxiv.org/abs/1803.03289)
```
 03289.

```
Z. Yao, R. Yazdani Aminabadi, M. Zhang, X. Wu,
C. Li, and Y. He. ZeroQuant: Efficient and affordable post-training quantization for large-scale
transformers. Advances in Neural Information Pro_cessing Systems, 35:27168–27183, 2022._

Z. Yuan, Y. Shang, Y. Song, Q. Wu, Y. Yan,
and G. Sun. ASVD: Activation-aware singular
value decomposition for compressing large language models. arXiv preprint arXiv:2312.05821,
2023.

A. H. Zadeh and A. Moshovos. GOBO: quantizing
attention-based NLP models for low latency and
energy efficient inference. CoRR, abs/2005.03842,
2020. [URL https://arxiv.org/abs/2005.](https://arxiv.org/abs/2005.03842)
```
 03842.

```
Y. Zhao, C.-Y. Lin, K. Zhu, Z. Ye, L. Chen,
S. Zheng, L. Ceze, A. Krishnamurthy, T. Chen,
and B. Kasikci. Atom: Low-bit quantization for
efficient and accurate llm serving, 2024. URL
```
 https://arxiv.org/abs/2310.19102.

```

-----

