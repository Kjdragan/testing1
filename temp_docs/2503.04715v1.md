## Predictable Scale: Part I — Optimal Hyperparameter Scaling Law in Large Language Model Pretraining

### Houyi Li[1][,][2], Wenzhen Zheng[1], Jingcheng Hu[1][,][3], Qiufeng Wang[1], Hanshan Zhang[1], Zili Wang[1], Shijie Xuyang[1][,][2], Yuantao Fan[1], Shuigeng Zhou[2], Xiangyu Zhang[1][,][4], Daxin Jiang[1]


1 StepFun 2 Fudan University
3 Tsinghua University 4 Megvii Technology

### Abstract


2.18

2.16


The impressive capabilities of Large Language
Models (LLMs) across diverse tasks are now
well-established, yet their effective deployment
necessitates careful hyperparameter optimization. Through extensive empirical studies involving grid search across diverse configurations, we discover universal scaling laws governing these hyperparameters: optimal learning rate follows a power-law relationship with
both model parameters and data sizes, while
optimal batch size scales primarily with data
sizes. Our analysis reveals a convex optimization landscape for hyperparameters under
fixed models and data size conditions. This
convexity implies an optimal hyperparameter
plateau. We contribute a universal, plug-andplay optimal hyperparameter tool for the community. Its estimated values on the test set
are merely 0.07% away from the globally optimal LLM performance found via exhaustive
search. These laws demonstrate remarkable
robustness across variations in model sparsity,
training data distribution, and model shape. To
our best known, this is the first work that unifies different model shapes and structures, such
as Mixture-of-Experts models and dense transformers, as well as to establish optimal hyperparameter scaling laws across diverse data distributions. This exhaustive optimization process
demands substantial computational resources,
utilizing nearly one million NVIDIA H800
GPU hours to train 3,700 LLMs of varying
sizes and hyperparameters from scratch and
consuming approximately 100 trillion tokens
in total. To facilitate reproducibility and further research, we will progressively release
all loss measurements and model checkpoints
[through our designated repository https://step-](https://step-law.github.io/)
[law.github.io/ .](https://step-law.github.io/)

### 1 Introduction


106


2.14

2.12


2.10

2.08


10 3

Learning Rate

|Col1|Col2|0%|
|---|---|---|
|+2.000%||+0.25 +0.125% +1.000%|


State-of-the-art Large Language Models (LLMs)
(Brown et al., 2020; Jin et al., 2023; Biderman et al.,
2023; Scao et al., 2022; Touvron et al., 2023a,b;


Figure 1: This plot shows the hyperparameter space for
a model with 1 billion (B) parameters trained on 100B
tokens. We trained 120 LLMs from scratch with different learning rate (LR) and batch size (BS) combinations,
obtaining contour lines and global optimal points based
on real data. Optimal points represent the lowest training loss for each LR and BS pair, while contour lines
depict the relative loss differences from these optima.
Our Step Law predicts the optimum with the highest
accuracy compared to other methods, nearly matching
the global optimal points.

Grattafiori et al., 2024a; DeepSeek-AI et al., 2024a;
Yang et al., 2024; DeepSeek-AI et al., 2024b, 2025),
have reached unprecedented scales, with models
being trained on billions of parameters and trillions of tokens. Recent developments like Llama
3 (Grattafiori et al., 2024a) demonstrate this trend,
utilizing 15 trillion tokens for training (Grattafiori
et al., 2024a). At such massive scales, identifying
optimal hyperparameter configurations becomes
both critical for model performance and challenging due to computational constraints.
The success of LLM pretraining heavily depends
on hyperparameter settings, particularly the learning rate and batch size. Suboptimal configurations
can lead to various issues: excessive learning rates
may cause training divergence, while insufficient
rates slow down progress (Shen et al., 2024; Wen


-----

**Name** **Data Recipe** **Model Sparsity** **LR** **BS** **Relative Error**
OpenAI Law
3.239 ∗ 10[−][3] + −1.395 ∗ 10[−][4]log(N ) 2e18L[−][4][.][76190] 9.51‰
(Kaplan et al., 2020)

Microsoft Law 1.3192e[−][5]N _[−][0][.][23]D[−][0][.][32]_  - 9.25‰
(Bjorck et al., 2024)

DeepSeek Law
0.3188C _[−][0][.][1250]_ 0.2920C [0][.][3271] 9.26‰
(DeepSeek-AI et al., 2024a)

Porian Law 3.7N _[−][0][.][36]_ 0.7576N [0][.][703] 3.71‰
(Porian et al., 2024)

MiniCPM Law(Hu et al., 2024)  - _L2[6]e[.]18[24]_  
MeiTuan Law(Wang et al., 2024) _λL[−][α]_ _λBL[−][α][−]B[1]_  
1.79N _[−][0][.][713]D[0][.][307]_ 0.58D[0][.][571] **0.94‰**
Ours (Step Law)

Table 1: Comparison of optimal hyperparameter scaling laws across different approaches. Data Recipe and Model
**Sparsity denotes whether the approach is suitable for different data recipe and model sparsity. Relative Error**
denotes the relative loss, as same as Fig 1. The variables in scaling laws are described in Section 1.1.


et al., 2024); similarly, batch size must balance
computational efficiency and model quality (Perko,
2023; Filatov et al., 2024; McCandlish et al., 2018).
Traditional grid search becomes prohibitively expensive at scale, leading researchers to rely on hyperparameter transfer methods extrapolating optimal configurations from smaller-scale experiments
to larger ones (Yang and Hu, 2020; Yang et al.,
2023).

_η(N, D) = 1.79N_ _[−][0][.][713]D[0][.][307]_

(1)
_B(D) = 0.58D[0][.][571]_

Prior work in hyperparameter transfer broadly
falls into two categories: theory-driven and datadriven approaches. In theory-driven methods, µP
(Yang et al., 2022) pioneered learning rate transfer rules across model widths, with subsequent
work (Everett et al., 2024; Lingle, 2024; Blake
et al., 2024; Yang et al., 2023; Bordelon et al.,
2023) extending these findings to various model
depths while also revealing their limitations. In
data-driven approaches, Kaplan et al. (2020) established foundational learning rate scaling laws based
on model size N, inspiring further investigations
(Bjorck et al., 2024; DeepSeek-AI et al., 2024a; Porian et al., 2024; Hu et al., 2024) into learning rate
(LR) and batch size (BS) scaling for dense models.
Recently, Wang et al. (2024); Ludziejewski et al.
(2025) had begun exploring these relationships in
Mixture-of-Experts (MoE) models(Du et al., 2021;
Fedus et al., 2021).
However, a significant gap remains in understanding hyperparameter transfer across different
dimensions: data recipe, model shape, model sparsity, and data sizes D. While existing research
has made progress in understanding scaling behav

ior across model sizes(Kaplan et al., 2020; Halfon
et al., 2024) the interaction of these other critical factors remains under-explored. Our work addresses this gap by empirically discovering universal hyperparameter (HP) scaling laws that hold
across these varied dimensions, providing a more
comprehensive understanding of optimal hyperparameter selection in LLM pretraining.
Our main contributions are as follows:
(i) We are the first to discover and demonstrate
the convexity property of the loss landscape under fixed parameter count and data size conditions.
This provides fundamental insights into hyperparameter optimization, as shown in Fig. 2.
(ii) We establish the first universal and robust
hyperparameter scaling laws for LLM pretraining,
which is called Step Law. Our Empirically discovered the power-law relationship between optimal learning rate η(N, D) and optimal batch size
_B(D). Step Law demonstrates that the optimal_
batch size exhibits a primary dependence on dataset
size D, while the optimal learning rate manifests a
joint dependence on both model parameters N and
dataset size D. Step Law is defined as Eq.(1).
The Step Law achieves substantially superior
convergence results compared to baseline methods
when generalized to 1 billion parameter models, as
illustrated in Fig. 1. Step Law provides a plug-andplay formula that eliminates extensive hyperparameter tuning efforts for industry applications.
(iii) We are the first to study the transferability
and invariance of optimal hyperparameter scaling
laws across different pretraining data distributions.
We systematically analyze how optimal hyperparameter scaling laws transfer across different pretraining data distributions and model architectures.
Our work pioneers the investigation into whether


-----

Figure 2: Learning Rate vs. Batch Size Loss Landscape Analysis for 1B Model (Trained on 100B Tokens): Scatter
Plots and 3D Surface Visualizations of Hyperparameter Sensitivity.


dense LLMs and sparse (MoE) LLMs with varying
sparsity levels share common optimal hyperparameter patterns, revealing significant invariance properties between them. Through extensive grid search
experiments, we validate that Step Law maintains
high generalizability and robustness across different pretraining corpora distributions, model architectures, and both dense and sparse (MoE) LLMs
with varying sparsity ratios.
(iv) We conduct an unprecedented large-scale
empirical study, involving:

  - Extensive experimentation across 3700 model
configurations, training LLMs from scratch
with Dense and MoE (varying sparsity ratios),
model architectures, data distributions, and
hyperparameter settings.

  - Total compute consumption approaching 1
million H800 GPU Hours (equivalent to over
$1 million), processing approximately 100 trillion tokens during training.

This represents the largest dataset of hyperparameter optimization results in the field, derived purely
from empirical observations without prior assumptions. Training checkpoints and hyperparameter
configurations will be made publicly available.

**1.1** **Notation**

We use the following notation:



  - L: Cross-entropy loss.

  - D: Dataset size in tokens.

 - N : Number of non-embedding parameters in
the model.

  - _N[ˆ]_ [1]: Total number of parameters in the model
.

  - C: Compute budget in FLOPs.

  - Nlayer: Number of layers in the Transformer
model.

  - dff : Dimension of the feed-forward network
hidden layer in the Transformer.

  - dmodel: Hidden dimension of the Transformer
model.

  - Nhead: Number of attention heads in the
Transformer model.

  - η(N, D): Optimal peak learning rate for a
given parameter count N and dataset size D.

  - B(N, D): Optimal batch size (in tokens) for
a given parameter count N and dataset size
_D._

1 ˆN excludes embedding layer but includes the model’s
head layer


-----

### 2 Related Works

Hyperparameter transfer, which involves extrapolating optimal settings from smaller to larger models, has become essential for efficient large-scale
training. Among these, learning rate (LR) and
batch size (BS) are particularly crucial hyperparameters that substantially influence LLM pretraining performance(Halfon et al., 2024). Research
on optimal learning rate and batch size selection
broadly falls into two categories: theory-driven and
data-driven approaches.
In theory-driven approaches, µP (Yang et al.,
2022) established foundational learning rate transfer rules for varying model widths, though this
required specific modifications to initialization and
attention mechanisms. However, the µP framework and its extensions (Yang et al., 2022; Everett
et al., 2024; Lingle, 2024; Blake et al., 2024; Yang
et al., 2023; Bordelon et al., 2023) are limited in
scope, lacking guidance for learning rate adaptation
across different data distributions, model architectures, sparsity levels, and data sizes. Additionally,
these works do not address batch size optimization.
In data-driven approaches, a fundamental principle in deep learning is that larger models require
smaller learning rates to ensure training stability
and convergence. Kaplan et al. (2020) formalized this relationship, expressing learning rate as
a function of model size. Bjorck et al. (2024)
incorporated data size dependency by proposing
_LR(N, D) = CN_ _[−][α]D[−][β]. Batch size optimiza-_
tion is equally important for balancing convergence
and computational efficiency. While several approaches followed Kaplan et al. (2020)’s framework, they face limitations: Wang et al. (2024);
Hu et al. (2024) derived batch size based on expected loss but require prior knowledge of model
behavior; Porian et al. (2024) refined scaling laws
across two datasets but only considered model size,
setting final learning rates at 0.1% of peak values.
Notably, both early work (McCandlish et al.,
2018) and recent Critical Batch Size (CBS) analysis (Zhang et al., 2024) support our empirical finding that optimal batch size primarily depends on
dataset size rather than model size, with CBS establishing a theoretical upper bound for this relationship. DeepSeek-LLM (DeepSeek-AI et al.,
2024a), while comprehensive in its approach using
IsoFLOP (Hoffmann et al., 2022), is constrained
by its fixed compute budget assumption.
Our research advances these findings by demon

strating that model size and dataset size are sufficient predictors of optimal hyperparameters. We
validate these scaling rules across diverse architectures, including variations in model sparsity, data
distributions and model shape.

### 3 Problem Setup

**3.1** **Preliminaries**

For training LLMs, the comprehensive performance metric is defined as

_L(A, D, N, D, LR, BS),_ (2)

where A, D, N, D, LR, and BS represent the model
architecture space, training data distribution, number of non-vocabulary parameters, number of training tokens, learning rate and batch size.
Based on this definition, when considering specific conditions, first, given that both A and D are
discrete variables, the performance metric can alternatively be expressed as

_LA,D(N, D, LR, BS)._ (3)

Furthermore, for given N and D, Eq.(3) can be
transformed into

_LA,D,N,D(LR, BS)_ (4)

In light of the above transformations, we can generate the following definition.
**Definition 1: (Hyperparameter Optimality) For**
fixed architecture A, data distribution D, and training budget (N, D), the optimal learning rate η and
batch size B satisfy:

_η, B = arg min_ _LA,D,N,D(LR, BS)._ (5)
LR,BS

**3.2** **Experimental Settings**

We train our models using language modeling loss
on a dataset that includes web text, mathematical
content, and code. The dataset is tokenized using a
BPE (Gage, 1994) tokenizer with a vocabulary size
of 65,536.
Our model architecture uses RMSNorm (Zhang
and Sennrich, 2019) for pre-normalization and the
SwiGLU (Shazeer, 2020) activation function in the
feed-forward network, without applying dropout
(Srivastava et al., 2014). We mainly use ALiBi
(Press et al., 2021) positional encoding. The models are initialized from scratch, with weights drawn
from a truncated normal distribution (mean of 0,


-----

|Col1|Training Loss Hyperparameter Configuration Space|Col3|Col4|
|---|---|---|---|
||Global Minimum|||
||+0.500% 16 +0.250% +0.125% +2.000% +1.000%|||
||0.|||
|BS = 3932|16 25|||
|||||
||LR = 0.||001950|
|||LR = 0.|001950|

|Col1|Validation Loss Hyperparameter Configuration Space|Col3|Col4|
|---|---|---|---|
|+2.000%|Global Minimum|||
||+0.250% +0.125% 16 +0.500% +1.000% +2.000%|||
|||||
|BS = 3932|16|||
|||||
||LR = 0.||001950|
|||LR = 0.|001950|


Figure 3: Contour plots of training loss (left) and validation loss (right) across hyperparameter configurations.
Both plots share the global minimum (✗) at batch size 393,216 and the learning rate of 0.001950.


standard deviation of 0.02). For the output projection of attention layers and the W2 component of the GLU, weights are further divided by
2·layer depths based on existing methods (Touvron
et al., 2023a,b; Grattafiori et al., 2024a; DeepSeekAI et al., 2024a; Yang et al., 2024; DeepSeek-AI
et al., 2024b, 2025).

We use the AdamW (Loshchilov and Hutter,
2017) optimizer with β values of [0.9, 0.95], an
epsilon of 10[−][8], a weight decay of 0.1, and a gradient clipping norm of 1.0. Our learning rate schedule includes a linear warmup phase over the initial
2,000 steps, followed by a cosine decay reaching
a final learning rate of 10[−][5] for the remainder of
the training. A detailed analysis and rationale for
this strategy are provided in Section 4.1.3. The
sequence length is set to 2,048 tokens. We conduct experiments using training data proportions
aligned with Llama-2 (Touvron et al., 2023b) (
More details are described in Tab. 6 ).

The learning rate is selected from a logarithmic
sequence of powers of 2, spanning exponents from
-10.5 to -7.0 in regular 0.5-interval increments. The
batch size is selected from a predefined geometric progression ranging from 32,768 to 4,194,304,
where each subsequent batch size is obtained by
multiplying the previous value by a constant factor of 2, maintaining an exponential growth trend.
Both parameter configurations correspond to the
18 LLMs detailed in Tab. 4 in Appendix A.2.


### 4 Experiments

**4.1** **Ablations**

**4.1.1** **Evaluation metric**

As described in Chinchilla (Hoffmann et al., 2022),
smoothed training loss is considered an unbiased
estimate of validation loss for simplicity. We operate under this same setting and supplement our
investigation with experimental analysis.
As shown in Fig. 3, for the case where the
smoothed training loss converges to the optimal
value of 2.279 ( as indicated by the solid redframed line in Fig. 3 (b)), the corresponding LR
and BS are 1.95 × 10[−][3] and 393, 216 respectively.
This is the same as the position of the LR and BS
corresponding to the validation loss converging to
the optimal value of 2.038 (as indicated by the
solid red-framed line in Fig. 3 left). Moreover, the
overall trend of how the smoothed training loss deviates from the optimal value with varying learning
rates and batch sizes (as shown by the heatmap
patterns in Fig. 3 right) closely mirrors the corresponding variations observed in validation loss
measurements. This alignment demonstrates that
the smoothed training loss provides consistent optimization guidance for learning rate and batch size
selection, matching the parameter configurations
that would be obtained through direct validation
loss evaluation.

**4.1.2** **Loss Landscape Convexity with LR and**
**BS**

To investigate the property of the loss landscape
with respect to learning rate and batch size, we
conducted systematic experiments across a wide


-----

|min_lr = peak_lr / 10 keep min_lr = 1e-5 conventional decay schedule loss Minimum fixed final lr loss Minimum +2.00% +0.25% +0.50% +2.00% +0.50+0.12% % Size 106 Batch +0.12% +0.25% +1.00% 103 Learning Rate|+2.00% +2.00% +0.50% +0.50+0.12% % +0.25%|min_lr = peak_lr / 10 keep min_lr = 1e-5 conventional decay schedule loss Minimum fixed final lr loss Minimum 2%|Col4|
|---|---|---|---|
|||+0.1 +1.00% +0.25%||
|||||


Figure 4: Comparison of learning rate schedules.
These contour plots illustrate two distinct learning
rate schedules. Blue contours represent the conven_tional decay schedule, where the minimum learning rate_
(min _lr) is set to one-tenth of the maximum learning
rate ( [max]10[ _][lr] ). Red contours depict our proposed fixed

_final learning rate schedule, with a constant minimum_
learning rate of min _lr = 10[−][5]. The visualization
reveals that the conventional decay method leads to a
discernible leftward bias in the optimal learning rate
range, indicated by the shift of the lowest loss region
towards lower learning rates in the blue contours compared to the red.

range of configurations under fixed model parameters N and data size D. As shown in Fig. 2, one
of our experiment settings described in Section 3.2
demonstrates this property.
Through extensive empirical analysis, we discovered a fundamental property of the loss landscape
with respect to hyperparameters: both the learning rate and batch size exhibit convex relationships
with the training loss under fixed model parameters
and data size conditions. ( As shown in Fig. 2 one
of our experiment setting described in section.)
Furthermore, we observe that the loss surface
demonstrates a stable region around the optimal
configuration, evidenced by the plateau-like behavior shown in Fig. 3. This stability provides practical
tolerance for small deviations in hyperparameter
selection while maintaining near-optimal performance.
These properties form the theoretical foundation
for our subsequent development of scaling laws
and validate their applicability across different architectural configurations.

**4.1.3** **Fixed Final Learning Rate Strategy**

We investigated two approaches for the final minimum learning rate (min _lr): the conventional
decay schedule (min _lr = [max]10[ _][lr] )(Brown et al.,


2020; Jin et al., 2023; Touvron et al., 2023a,b;
Biderman et al., 2023; Scao et al., 2022; Shen
et al., 2024), and our proposed fixed schedule
(min _lr = 10[−][5]). Using 1B model training for
80B tokens, we compared these schedules across
various LR and BS.
Fig. 4 presents comparative heatmaps of the final
training loss. We observe that compared to using
a fixed final learning rate, setting it as max_lr/10
shows distinct optimal hyperparameter points and
an overall left-skewed distribution of suboptimal
learning rate and batch size combinations. We analyze that this is because, for the relatively high
peak learning rates, conventional schedules result
in disproportionately large minimum learning rates,
which adversely affects the final stages of training and prevents the loss from converging to better
local optima.
As can also be seen in Fig. 1, aside from Porian
Law, which converges the min_lr to a sufficiently
small value, the optimal learning rates calculated
by other traditional learning rate decay schedules
all exhibit varying degrees of a left-skew issue.
This aligns with advanced training practices
which suggest that the minimum learning rate significantly impacts the loss. This phenomenon is
unfavorable for fitting our scaling laws, and in practice, it is generally preferred to keep the min_lr
fixed at a relatively low value. So we adopt the
fixed final learning rate strategy in our subsequent
experiments.

**Key Takeaways**

 - Convex Loss Landscape: The loss landscape exhibits convexity with respect to both
learning rate and batch size. This convexity, coupled with a stable plateau around the
optimum, underpins the robustness of hyperparameter selection.

 - Fixed Final Learning Rate Benefit: Compared to setting a small, fixed final learning rate, the traditional decay to max_lr/10
causes the discovered optimal learning rate to
be biased towards lower values (left-skewed).

**4.2** **Fitting HP Scaling Laws**

**4.2.1** **LR and BS with Respect to N and D**

In accordance with Definition 3.1, we experimentally derive the LR and BS by keeping other vari

-----

2.44

2.42


following power-law relationships:

_η(N, D) = cN_ _[α]D[β],_

(6)
_B(D) = dD[γ]_


106

105


Global Minimum
Ours(Step Law)
Microsoft Law

+2.000% DeepSeek Law

OpenAI Law
Porian Law

10 3 10 2

Learning Rate


2.40

2.38


2.36

|Col1|+1.000%|+0.125%|
|---|---|---|
|+2.000%||+0.250% +0.500% Global Mini Ours(Step L Microsoft La DeepSeek L +2.000%|
|||OpenAI Law Porian Law|


Figure 5: The Illustration of Hyperparameter Configuration Space for 210M model parameters and 100B
tokens.

ables fixed. This section focuses on elucidating the
relationships between these empirically determined
hyperparameters and N and D. For the parameter
count N, we set up seven experiments spanning
60M, 120M, 210M, 270M, 430M, 540M, and 1B
parameters. As demonstrated in Fig. 6a, our experiments reveal a positive correlation between optimal
LR and BS and the data scale D for each value of
_N_ . Furthermore, we conducted experiments across
five different data scales D: 2B, 4B, 8B, 20B, and
100B tokens. Notably, we specifically reserved
the 1B parameter and 100B token settings as test
points to validate our findings, as discussed in Section 4.2.4. As visualized in Fig. 6b, we find that for
each data scale D, the optimal LR increases with
model size N . Notably, our findings indicate that
optimal BS is largely independent of N . Based on
these experimental observations, we will present
and fit the Hyperparameter (HP) scaling law formulations in Section 4.2.2.


where the parameters c, α, β, d, and γ are five
constants, the values of which will be determined
through fitting in Section 4.2.3. It is particularly
noteworthy that our proposed scaling law demonstrates significant generality, meaning it is applicable across diverse architectures A and data distributions D. This aspect of generality will be further
elaborated upon in Section 5.

Table 2: Fitted power-law coefficients for hyperparameter scaling laws


**Parameter** _α_ _β_ _γ_ _c_ _d_

**Fitted value** -0.713 0.307 0.571 1.79 0.58


**4.2.3** **Fitting Methods**
Building upon the HP scaling law from Section 4.2.1, we transform the power laws in Eq. (6)
into the linear form:

log η = log c + α log N + β log D (7)


**4.2.2** **Scaling Laws**

Building upon the insights gained from Section 4.2.1, we delve into the scaling behavior of
optimal hyperparameters. Specifically, we investigate how the optimal LR scales with N and D, and
how the optimal BS scales with D. Our empirical
observations, particularly when visualized on a loglog scale, reveal a strong linear trend, suggesting a
power-law relationship. Based on this, the scaling
law for hyperparameters can be described by the


log B = log d + γ log D (8)

In this way, we can employ Ordinary Least Squares
to fit the unknown parameters log c, α, β, log d and
_γ. Specifically, we set up 7 groups of experiments_
with different N and D as shown in Appendix A.2.
Following (Hoffmann et al., 2022), we fit the optimal LR and BS with the experimentally predicted
LR and BS. We averaged the results of these 1000
bootstrap samples to obtain the intermediate final
parameters. This averaged result is what we present
in Tab. 2. Furthermore, the variability across these
1000 bootstrap samples is depicted as the shaded
regions in Fig. 6, providing an indication of the
uncertainty associated with the fitted results. These
shaded regions allow us to visually assess the robustness and confidence of the optimal LR and BS
values derived from our procedure.

**4.2.4** **Experimental Comparison with Existing**
**Approaches**
Since we have obtained the fitted scaling laws, we
directly extrapolate them to the test point (N = 1B
and D = 100B) for comparison with other methods, noting that these are out-of-sample extrapolations beyond the fitting range. As shown in Fig. 1


-----

(a) Scaling laws with D for different N

(b) Scaling laws with N for different D

Figure 6: (a) Scatter points indicate empirical optimal learning rate vs. batch size for model scale N ; (b) Analogous
results for dataset scale D. Curves show our hp-scaling law predictions, with shaded regions representing parameter
uncertainty bounds from the sampling-based fitting strategy. Both plots use double logarithmic scaling (1912
training samples).


and Fig. 11, our method predicts solutions closest
to the global minimum, while other approaches (indicated by dashed lines) remain distant from the
global minimum. This discrepancy arises because
previous methods only considered LR fitting without jointly modeling both LR and BS. The approach
in (DeepSeek-AI et al., 2024a) enforces a fixed
compute budget assumption, which constrains the
feasible (N, D) sampling range and consequently
reduces fitting accuracy. Additionally, as discussed
in Section 4.1.3, existing methods typically relate final LR to initial LR through a fixed multiplier. This
assumption leads to excessively large final LRs
when initial LRs are large, ultimately impairing
convergence. While Porian et al. (2024) achieves
comparable results to ours through similar min_lr
constraints, their method exhibits instability due to
incomplete consideration of hyperparameter interactions with model dimension D, particularly at the
subsequent MoE experiments 5.2 and data recipe
experiments 5.3, where the D/N ratios are rela

tively small, their methods predict learning rates
and batch sizes that fall completely outside reasonable ranges, leading to training instability.


-----

### 5 Universal HP Scaling laws: Empirical Validation Across Architectural Heterogeneity and Data Recipes

**5.1** **Topological Invariance Across Varied**
**Model Shape**

As illustrated in Fig. 7, we conduct a series of controlled experiments to systematically investigate
the relationship between HP scaling and model architecture topology. Specifically, we set a model
with 430 million parameters and varied its structural configuration by defining six distinct model
shape combinations. These model shape variations
involved changes in key architectural factors (e.g.,
number of layers, attention heads, feed-forward
network dimensions).
For each of these 6 model shapes, we perform
extensive hyperparameter tuning to identify the
optimal LR and BS. The results reveal a striking pattern: the optimal LR and BS values for
all configurations (highlighted within the solidline box) consistently fall within a well-defined
and narrow range (enclosed by the dashed-line
box). This consistency holds across all model
shape combinations, despite significant variations
in architectural topology. These empirical findings provide strong evidence supporting our hypothesis that the HP scaling law exhibits statistical invariance with respect to changes in model
topology. In other words, while the architectural
components—including depth (number of layers),
attention mechanism complexity (number of attention heads), and feedforward network width—may
vary, the fundamental scaling relationships governing LR, BS, model size N, and dataset size D
remain unchanged.

**5.2** **Sparsity-Independent in MoE**

The HP scaling law has been extensively studied
for dense Transformers, but its applicability to
sparse architectures remains uncertain. Mixture-ofExperts (MoE) (Shazeer et al., 2017; Fedus et al.,
2022) is a widely used sparse model that activates
only a subset of parameters per token, introducing fundamental structural differences from dense
models. This raises the question of whether the
HP scaling law can be generalized to MoE settings.
To investigate this, we conducted experiments on
MoE models across 16 different sparsity levels and
model shapes (refer to Tab. 5 in the appendix A.2).
These settings allow us to examine how the scaling
law behaves under different levels of sparsity. We


evaluate multiple existing scaling methods under
this framework.
As shown in Fig. 8, our approach consistently
achieves a relative prediction error within 0.5%
across all sparsity levels, significantly outperforming competing methods. In contrast, the DeepSeek
Formula yields a relative error over four times
larger, indicating its reduced accuracy in MoE settings. While Eq.(1) achieves comparable accuracy
in LR prediction, it fails to predict BS. In contrast, our method provides a more comprehensive
framework, successfully predicting multiple hyperparameters. These results demonstrate that the
HP scaling law extends beyond dense architectures
and remains effective for sparse models like MoE,
regardless of sparsity level. This suggests that
the underlying principles of scaling laws emerge
from broader optimization and capacity constraints
rather than being specific to dense parameterization.
Our findings reinforce the general applicability of
HP scaling laws and their potential to guide efficient scaling in diverse neural architectures A.

**5.3** **Data-Distribution Robustness**

To rigorously assess the robustness of our HP scaling law across varied data distributions D, we design three distinct data distributions, progressively
diverging from the original composition, as detailed in Appendix Tab. 6:

1. Bilingual Corpus: We augmented the original English-only dataset with Chinese data,
creating a bilingual distribution to test the
law’s validity in multilingual settings.

2. Code Integration: We reduced English content and incorporated 32.36% of the code-thestack dataset, examining the law’s adaptability
to code-heavy distributions.

3. Code-Dominant: We further decreased English content and increased code-the-stack to
57.05%, representing an extreme shift towards
code-based data.

As shown in Fig. 9, our formula maintains remarkable predictive accuracy across all three distributions, with relative prediction errors within
0.25% of the global minimum. This performance consistently surpasses alternative approaches, which exhibit larger deviations. These
results highlight two crucial insights:


-----

Figure 7: Topological Invariance Across Varied Model Shape. dmodel, dff, Nlayer, and Nhead denote the hidden
dimension, feed-forward network hidden size, number of attention heads, and number of Transformer layers,
respectively.


N=2.156b, D=20.0b | Na = 1.241b, Na/N = 0.58

|+2.000%|+0.250% +0.125% +0.500% +|+2.000% Global Minimum Ours(Step Law) Microsoft Law DeepSeek Law OpenAI Law 2.000%|Col4|
|---|---|---|---|
||+1.000%|||

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|+0.500%|+1.000% +0.125% +0|+2.000% .250%||
|||Global Minimum Ours(Step Law)||
||+2.000|Microsoft Law % DeepSeek Law OpenAI Law||

|+1.000% +2.000% +0.500% +0.125% +0.250%|+2.000% Global Minimum Ours(Step Law) Microsoft Law DeepSeek Law OpenAI Law|
|---|---|
|||


10 3


106

105


N=2.155b, D=20.0b | Na = 590.436m, Na/N = 0.27

10 3


2.29

2.28

2.27

2.26

2.25

2.24

2.23

2.22


106

105


N=2.156b, D=8.0b | Na = 1.241b, Na/N = 0.58

+0.250%

10 3


106

105


2.25

2.24

2.23

2.22

2.21

2.20

Global Minimum
Ours(Step Law) 2.19
Microsoft Law
DeepSeek Law
OpenAI Law 2.18


2.38

2.36


2.34

2.32


Learning Rate


Learning Rate


Learning Rate


Figure 8: Validation loss landscapes of MoE models under varying sparsity ratios (Na/N **). Left: Low sparsity**
(Na/N = 0.27). Middle: Medium sparsity (Na/N = 0.58). Right: Reduced model depth (D = 8.0B) at medium
sparsity. Our method consistently approximates global minima across sparsity regimes.


2.71

2.70


2.07

2.06


2.69

2.68


2.05

2.04


1.66

1.65


2.67

2.66


105


105


2.03

2.02


105


1.64

1.63


2.65


10 3

Learning Rate


10 3

Learning Rate


10 3

Learning Rate

|Col1|Global Minimum Ours(Step Law) Microsoft Law DeepSeek Law OpenAI Law +0.500% +0.125% +0.250% +1.000%|Col3|
|---|---|---|
||+2.000%||

|+1.000% +0.500%|Global Minimum Ours(Step Law) Microsoft Law DeepSeek Law OpenAI Law +0.250% +0.125% +1.000%|Global Minimum Ours(Step Law) Microsoft Law DeepSeek Law|Col4|
|---|---|---|---|
||+2.000%|||

|+1.000% +0.500% +0.250% +0.125% +0.500%|Global Minimum Ours(Step Law) Microsoft Law DeepSeek Law OpenAI Law +2.000%|
|---|---|
|+1.000%||


(a) Bilingual Corpus


(b) Code Integration


(c) Code-Dominante


Figure 9: Configuration Space Analysis under Different Data Recipes. Our method demonstrates stable
convergence patterns across varying data compositions.


-----

  - The HP scaling law demonstrates statistical invariance across linguistic and structural
changes in the dataset, supporting its generalizability beyond standard natural language
distributions.

  - The predicted optimal hyperparameters remain stable even with highly heterogeneous
training data, reinforcing our approach’s robustness.

These findings are particularly significant for designing scalable and adaptable training paradigms
applicable across diverse deployment scenarios
with varying dataset characteristics.

**Key Takeaways**

 - Topological Invariance: Our HP scaling
laws exhibit statistical invariance in the scaling constants for LR and BS with respect to
model scale N and data size D, even when
varying topological features of model architectures.

 - Sparsity Independence: The HP scaling law
extends beyond dense Transformers and remains effective for sparse MoE models, with
our approach achieving superior prediction
accuracy across different sparsity levels, reinforcing the broader applicability of scaling
laws in diverse neural architectures A.

 - Data-Distribution Robustness: The HP
scaling law shows robustness across diverse
data distributions D.

### 6 Conclusions

In this paper, we provide a crucial advancement
in efficient hyperparameter optimization for LLMs.
By empirically unveiling and rigorously validating
universal scaling laws for learning rate and batch
size—underpinned by the discovery of loss landscape convexity—we move beyond computationally expensive grid searches and limited transfer
methods. Our robust HP scaling laws, supported by
an unprecedentedly large empirical study and opensourced resources, empower the community with a
practical and generalizable approach for navigating
the hyperparameter configuration space in LLM
pretraining, thereby facilitating more efficient and
scalable LLM development.


### Limitations

While our empirical study provides valuable universal HP scaling laws and demonstrates their practical
efficacy, it is essential to acknowledge the limitations inherent in an empirical approach. Our findings are primarily data-driven. Future work should
focus on developing a more theoretical understanding of the observed power-law relationships, potentially deriving them from first principles to enhance
their predictive power and generalizability beyond
the empirically validated domain.

### Acknowledgments

The work was supported by National Science and Technology Major Project of China
(2023ZD0121300).

### References

Stella Biderman, Hailey Schoelkopf, Quentin Anthony,
Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai
Prashanth, Edward Raff, Aviya Skowron, Lintang
Sutawika, and Oskar van der Wal. 2023. [Pythia:](http://arxiv.org/abs/2304.01373)
[A suite for analyzing large language models across](http://arxiv.org/abs/2304.01373)
[training and scaling.](http://arxiv.org/abs/2304.01373)

Johan Bjorck, Alon Benhaim, Vishrav Chaudhary, Furu
[Wei, and Xia Song. 2024. Scaling optimal lr across](http://arxiv.org/abs/2409.19913)
[token horizons.](http://arxiv.org/abs/2409.19913)

Charlie Blake, Constantin Eichenberg, Josef Dean,
Lukas Balles, Luke Y. Prince, Björn Deiseroth, Andres Felipe Cruz-Salinas, Carlo Luschi, Samuel Wein[bach, and Douglas Orr. 2024. u-µp: The unit-scaled](http://arxiv.org/abs/2407.17465)
[maximal update parametrization.](http://arxiv.org/abs/2407.17465)

Blake Bordelon, Lorenzo Noci, Mufan Bill Li, Boris
[Hanin, and Cengiz Pehlevan. 2023. Depthwise hyper-](http://arxiv.org/abs/2309.16620)
[parameter transfer in residual networks: Dynamics](http://arxiv.org/abs/2309.16620)
[and scaling limit.](http://arxiv.org/abs/2309.16620)

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
[2020. Language models are few-shot learners.](http://arxiv.org/abs/2005.14165)

DeepSeek-AI, Xiao Bi, Deli Chen, Guanting Chen,
Shanhuang Chen, Damai Dai, Chengqi Deng, and
[et al. 2024a. Deepseek llm: Scaling open-source](http://arxiv.org/abs/2401.02954)
[language models with longtermism.](http://arxiv.org/abs/2401.02954)


-----

DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang,
Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,
[Shirong Ma, Peiyi Wang, et al. 2025. Deepseek-](http://arxiv.org/abs/2501.12948)
[r1: Incentivizing reasoning capability in llms via](http://arxiv.org/abs/2501.12948)
[reinforcement learning.](http://arxiv.org/abs/2501.12948)

DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang
Zhao, Chengqi Deng, Chenyu Zhang, et al. 2024b.
[Deepseek-v3 technical report.](http://arxiv.org/abs/2412.19437)

Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong,
Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,
Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret
Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou,
Tao Wang, Yu Emma Wang, Kellie Webster, Marie
Pellat, Kevin Robinson, Kathleen Meier-Hellstern,
Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le,
Yonghui Wu, Zhifeng Chen, and Claire Cui. 2021.

[Glam: Efficient scaling of language models with](http://arxiv.org/abs/2112.06905)
[mixture-of-experts.](http://arxiv.org/abs/2112.06905)

Katie Everett, Lechao Xiao, Mitchell Wortsman,
Alexander A. Alemi, Roman Novak, Peter J. Liu,
Izzeddin Gur, Jascha Sohl-Dickstein, Leslie Pack
Kaelbling, Jaehoon Lee, and Jeffrey Pennington.
[2024. Scaling exponents across parameterizations](http://arxiv.org/abs/2407.05872)
[and optimizers.](http://arxiv.org/abs/2407.05872)

William Fedus, Barret Zoph, and Noam Shazeer. 2021.

[Switch transformers: Scaling to trillion parameter](http://arxiv.org/abs/2101.03961)
[models with simple and efficient sparsity.](http://arxiv.org/abs/2101.03961)

William Fedus, Barret Zoph, and Noam Shazeer. 2022.

[Switch transformers: Scaling to trillion parameter](http://www.jmlr.org/papers/v23/21-0998.html)
[models with simple and efficient sparsity. Journal of](http://www.jmlr.org/papers/v23/21-0998.html)
_Machine Learning Research, 23(120):1–39._

Oleg Filatov, Jan Ebert, Jiangtao Wang, and Stefan
[Kesselheim. 2024. Time transfer: On optimal learn-](http://arxiv.org/abs/2410.05838)
[ing rate and batch size in the infinite data limit.](http://arxiv.org/abs/2410.05838)

[Philip Gage. 1994. A new algorithm for data compres-](https://api.semanticscholar.org/CorpusID:59804030)
[sion. The C Users Journal archive, 12:23–38.](https://api.semanticscholar.org/CorpusID:59804030)

Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,
Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten,
[Alex Vaughan, and Amy Yang et al. 2024a. The](http://arxiv.org/abs/2407.21783)
[llama 3 herd of models.](http://arxiv.org/abs/2407.21783)

Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,
Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten,
[Alex Vaughan, Amy Yang,, et al. 2024b. The llama](http://arxiv.org/abs/2407.21783)
[3 herd of models.](http://arxiv.org/abs/2407.21783)

Alon Halfon, Shai Gretz, Ofir Arviv, Artem Spector, Orith Toledo-Ronen, Yoav Katz, Liat Ein-Dor,
Michal Shmueli-Scheuer, and Noam Slonim. 2024.
[Stay tuned: An empirical study of the impact of hy-](http://arxiv.org/abs/2407.18990)
[perparameters on llm tuning in real-world applica-](http://arxiv.org/abs/2407.18990)
[tions.](http://arxiv.org/abs/2407.18990)


Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,
Elena Buchatskaya, Trevor Cai, Eliza Rutherford,
Diego de Las Casas, Lisa Anne Hendricks, Johannes
Welbl, Aidan Clark, Tom Hennigan, Eric Noland,
Katie Millican, George van den Driessche, Bogdan
Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,
[and Laurent Sifre. 2022. Training Compute-Optimal](https://doi.org/10.48550/arXiv.2203.15556)
[Large Language Models. ArXiv:2203.15556 [cs].](https://doi.org/10.48550/arXiv.2203.15556)

Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu
Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang
Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng
Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao,
Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai,
Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li,
[Zhiyuan Liu, and Maosong Sun. 2024. Minicpm:](http://arxiv.org/abs/2404.06395)
[Unveiling the potential of small language models](http://arxiv.org/abs/2404.06395)
[with scalable training strategies.](http://arxiv.org/abs/2404.06395)

Hongpeng Jin, Wenqi Wei, Xuyu Wang, Wenbin Zhang,
[and Yanzhao Wu. 2023. Rethinking learning rate](http://arxiv.org/abs/2309.08859)
[tuning in the era of large language models.](http://arxiv.org/abs/2309.08859)

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.
Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeffrey Wu, and Dario Amodei.
[2020. Scaling Laws for Neural Language Models.](https://doi.org/10.48550/arXiv.2001.08361)
ArXiv:2001.08361 [cs, stat].

[Lucas Lingle. 2024. A large-scale exploration of µ-](http://arxiv.org/abs/2404.05728)
[transfer.](http://arxiv.org/abs/2404.05728)

[Ilya Loshchilov and Frank Hutter. 2017. Decoupled](http://arxiv.org/abs/1711.05101)
[weight decay regularization.](http://arxiv.org/abs/1711.05101)

Jan Ludziejewski, Maciej Pióro, Jakub Krajewski, Maciej Stefaniak, Michał Krutul, Jan Mała´snicki, Marek
Cygan, Piotr Sankowski, Kamil Adamczewski, Piotr
[Miło´s, and Sebastian Jaszczur. 2025. Joint moe scal-](http://arxiv.org/abs/2502.05172)
[ing laws: Mixture of experts can be memory efficient.](http://arxiv.org/abs/2502.05172)

Sam McCandlish, Jared Kaplan, Dario Amodei, and
[OpenAI Dota Team. 2018. An empirical model of](http://arxiv.org/abs/1812.06162)
[large-batch training.](http://arxiv.org/abs/1812.06162)

[Stefan Perko. 2023. Unlocking optimal batch size sched-](http://arxiv.org/abs/2312.01898)
[ules using continuous-time control and perturbation](http://arxiv.org/abs/2312.01898)
[theory.](http://arxiv.org/abs/2312.01898)

Tomer Porian, Mitchell Wortsman, Jenia Jitsev, Ludwig
[Schmidt, and Yair Carmon. 2024. Resolving dis-](http://arxiv.org/abs/2406.19146)
[crepancies in compute-optimal scaling of language](http://arxiv.org/abs/2406.19146)
[models.](http://arxiv.org/abs/2406.19146)

[Ofir Press, Noah A. Smith, and Mike Lewis. 2021. Train](http://arxiv.org/abs/2108.12409)
[short, test long: Attention with linear biases enables](http://arxiv.org/abs/2108.12409)
[input length extrapolation.](http://arxiv.org/abs/2108.12409)

Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon,
[Matthias Gallé, Thomas Wolf, et al. 2022. Bloom: A](http://arxiv.org/abs/2211.05100)
[176b-parameter open-access multilingual language](http://arxiv.org/abs/2211.05100)
[model.](http://arxiv.org/abs/2211.05100)


-----

[Noam Shazeer. 2020. Glu variants improve transformer.](http://arxiv.org/abs/2002.05202)

Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,
Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff
[Dean. 2017. Outrageously large neural networks:](https://arxiv.org/abs/1701.06538)
[The sparsely-gated mixture-of-experts layer. arXiv](https://arxiv.org/abs/1701.06538)
_preprint arXiv:1701.06538._

Yikang Shen, Matthew Stallone, Mayank Mishra,
Gaoyuan Zhang, Shawn Tan, Aditya Prasad, Adriana Meza Soria, David D. Cox, and Rameswar Panda.
[2024. Power scheduler: A batch size and token num-](http://arxiv.org/abs/2408.13359)
[ber agnostic learning rate scheduler.](http://arxiv.org/abs/2408.13359)

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
[Dropout: A simple way to prevent neural networks](http://jmlr.org/papers/v15/srivastava14a.html)
[from overfitting. Journal of Machine Learning Re-](http://jmlr.org/papers/v15/srivastava14a.html)
_search, 15(56):1929–1958._

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
[Grave, and Guillaume Lample. 2023a. Llama: Open](http://arxiv.org/abs/2302.13971)
[and efficient foundation language models.](http://arxiv.org/abs/2302.13971)

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, and
[Shruti Bhosale et al. 2023b. Llama 2: Open founda-](http://arxiv.org/abs/2307.09288)
[tion and fine-tuned chat models.](http://arxiv.org/abs/2307.09288)

Siqi Wang, Zhengyu Chen, Bei Li, Keqing He, Min
[Zhang, and Jingang Wang. 2024. Scaling laws across](http://arxiv.org/abs/2410.05661)
[model architectures: A comparative analysis of dense](http://arxiv.org/abs/2410.05661)
[and moe models in large language models. pages](http://arxiv.org/abs/2410.05661)
5583–5595.

Kaiyue Wen, Zhiyuan Li, Jason Wang, David Hall,
[Percy Liang, and Tengyu Ma. 2024. Understanding](http://arxiv.org/abs/2410.05192)
[warmup-stable-decay learning rates: A river valley](http://arxiv.org/abs/2410.05192)
[loss landscape perspective.](http://arxiv.org/abs/2410.05192)

An Yang, Baosong Yang, Binyuan Hui, Bo Zheng,
Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan
Li, Dayiheng Liu, Fei Huang, Guanting Dong, Qwen
[Team, and Alibaba Group et al. 2024. Qwen2 techni-](https://arxiv.org/abs/2407.10671)
[cal report.](https://arxiv.org/abs/2407.10671)

[Greg Yang and Edward J. Hu. 2020. Feature learning in](http://arxiv.org/abs/2011.14522)
[infinite-width neural networks.](http://arxiv.org/abs/2011.14522)

Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon
Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub
Pachocki, Weizhu Chen, and Jianfeng Gao. 2022.
[Tensor programs v: Tuning large neural networks via](http://arxiv.org/abs/2203.03466)
[zero-shot hyperparameter transfer.](http://arxiv.org/abs/2203.03466)

Greg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou.
2023. [Tensor programs vi: Feature learning in](http://arxiv.org/abs/2310.02244)
[infinite-depth neural networks.](http://arxiv.org/abs/2310.02244)

[Biao Zhang and Rico Sennrich. 2019. Root mean square](http://arxiv.org/abs/1910.07467)
[layer normalization.](http://arxiv.org/abs/1910.07467)


Hanlin Zhang, Depen Morwani, Nikhil Vyas, Jingfeng
Wu, Difan Zou, Udaya Ghai, Dean Foster, and Sham
[Kakade. 2024. How does critical batch size scale in](http://arxiv.org/abs/2410.21676)
[pre-training?](http://arxiv.org/abs/2410.21676)

### A Appendix

**A.1** **Model Scale Dominates Optimal**
**Hyperparameter Selection Over**
**Computational Complexity**

To investigate how model architecture variations
affect optimal hyperparameter settings, we conducted two sets of control experiments. In the first
set, we maintained a constant parameter count (N ),
while in the second set, we kept the computational
complexity (M ) constant. Both sets used identical training configurations with 8B training tokens,
varying only in their architectural proportions.
Tab. 3 presents the detailed configurations and
results for both experimental groups. For each
model, we systematically varied the hidden dimension (dmodel), feed-forward dimension (dff ), number of attention heads (Nhead), and number of layers (Nlayer) while maintaining either constant N
or M . The embedding dimension (D) was fixed at
8.00E+09 across all experiments.
To visualize the impact of hyperparameters
across different architectural configurations, we
generated heatmaps of the loss landscape with respect to LR and BS in Fig. 7 and 10. The heatmaps
reveal consistent patterns in the optimal hyperparameter regions across different architectural configurations within each experimental group.
The experimental results reveal several key findings: (i) Models with constant N demonstrate remarkably consistent optimal hyperparameter regions, with minimal variation in minimum loss
values (ranging from 2.4294 to 2.4776) despite significant architectural differences. (ii) The constant
M experiments show slightly more variation in optimal hyperparameter regions and minimum loss
values (ranging from 2.4346 to 2.5089), suggesting that parameter count N may be a more robust
indicator for hyperparameter selection than computational complexity M . (iii) Across both experimental groups, the optimal learning rates typically
fall within a narrow range (6.91E-04 to 1.95E-03),
and batch sizes cluster around either 131,072 or
262,144, regardless of the specific architectural
configuration.
These findings strongly suggest that the fundamental scale metrics, particularly the parameter


-----

_dmodel_ _dff_ _Nhead_ _Nlayer_ _lr_ _bs_ _D_ _N_ _M_
Constant N Experiments
1280 12264 10 8 1.95E-03 262,144 8.00E+09 4.29E+08 2.83E+09
1280 6280 10 14 1.38E-03 262,144 8.00E+09 4.29E+08 3.02E+09
1536 9600 12 8 9.77E-04 131,072 8.00E+09 4.29E+08 2.88E+09
1536 7264 12 10 1.38E-03 262,144 8.00E+09 4.29E+08 2.95E+09
1536 4608 12 14 9.77E-04 131,072 8.00E+09 4.29E+08 3.10E+09
2048 6000 16 8 9.77E-04 262,144 8.00E+09 4.29E+08 2.98E+09
2048 4256 16 10 9.77E-04 262,144 8.00E+09 4.29E+08 3.08E+09
2048 2256 16 14 9.77E-04 262,144 8.00E+09 4.29E+08 3.28E+09
Constant M Experiments
1280 12608 10 8 1.38E-03 262,144 8.00E+09 4.40E+08 2.89E+09
1280 5888 10 14 1.38E-03 262,144 8.00E+09 4.08E+08 2.89E+09
1536 9656 12 8 1.38E-03 262,144 8.00E+09 4.31E+08 2.89E+09
1536 7040 12 10 1.38E-03 262,144 8.00E+09 4.19E+08 2.89E+09
1536 4056 12 14 9.77E-04 262,144 8.00E+09 3.94E+08 2.89E+09
2048 5704 16 8 9.77E-04 262,144 8.00E+09 4.15E+08 2.89E+09
2048 3744 16 10 6.91E-04 131,072 8.00E+09 3.98E+08 2.89E+09
2048 1504 16 14 6.91E-04 131,072 8.00E+09 3.64E+08 2.89E+09

Table 3: Model configurations for constant N and constant M experiments. The first group (top) maintains
constant parameter count N ≈ 4.29E+08, while the second group (bottom) maintains constant computational
complexity M ≈ 2.89E+09. M : non-embedding FLOPs/token.

M=2.89e+09

dmodel=1280, dff=5888, Nlayer=14, Nhead=10 2.54 dmodel=1536, dff=7040, Nlayer=10, Nhead=12 2.56 dmodel=1536, dff=9656, Nlayer=8, Nhead=12 2.56

2.4570 2.4515 2.4504 2.4525 2.4602 2.4732 2.4997 2.5462 2.52 2.4681 2.4653 2.4673 2.4674 2.4814 2.5011 2.5250 2.5611 2.54 2.4713 2.4692 2.4689 2.4692 2.4832 2.5085 2.5228 2.5701 2.54

2.50 2.52 2.52

2.4518 2.4449 2.4393 2.4350 2.4370 2.4465 2.4642 2.4964 2.48 2.4620 2.4552 2.4522 2.4492 2.4531 2.4636 2.4825 2.5085 2.50 2.4698 2.4651 2.4592 2.4546 2.4572 2.4660 2.4822 2.5146 2.50

2.4563 2.4470 2.4399 2.4371 2.4346 2.4403 2.4582 2.4732 2.46 2.4665 2.4576 2.4525 2.4485 2.4476 2.4566 2.4678 2.4848 2.48 2.4792 2.4682 2.4615 2.4566 2.4507 2.4604 2.4727 2.4904 2.48

2.4657 2.4541 2.4468 2.4413 2.4384 2.4523 2.4691 2.4970 2.4761 2.4626 2.4546 2.4523 2.4524 2.4672 2.4742 2.4939 2.46 2.4889 2.4760 2.4686 2.4611 2.4600 2.4683 2.4817 2.4992 2.46

2.44

2.4741 2.4604 2.4497 2.4436 2.4453 2.4525 2.4633 2.4852 2.4852 2.4696 2.4613 2.4541 2.4583 2.4687 2.4791 2.4849 2.4946 2.4838 2.4756 2.4651 2.4667 2.4742 2.4856 2.5034

Learning Rate Learning Rate Learning Rate

dmodel=2048, dff=1504, Nlayer=14, Nhead=16 dmodel=2048, dff=3744, Nlayer=10, Nhead=16 dmodel=2048, dff=5704, Nlayer=8, Nhead=16

2.62 2.60 2.60

2.5203 2.5197 2.5251 2.5321 2.5555 2.5710 2.5956 2.6321 2.60 2.4913 2.4886 2.4938 2.5002 2.5193 2.5437 2.5794 2.6135 2.58 2.4939 2.4906 2.4925 2.4997 2.5148 2.5360 2.5793 2.6123 2.58

2.58 2.56 2.56

2.5125 2.5097 2.5089 2.5168 2.5302 2.5376 2.5515 2.5672 2.56 2.4858 2.4803 2.4790 2.4824 2.4950 2.5055 2.5261 2.5551 2.54 2.4884 2.4838 2.4769 2.4785 2.4888 2.5036 2.5253 2.5362 2.54

2.52 2.52

2.5162 2.5138 2.5145 2.5151 2.5204 2.5381 2.5428 2.5555 2.54 2.4895 2.4813 2.4796 2.4828 2.4950 2.4988 2.5101 2.5133 2.4882 2.4814 2.4772 2.4756 2.4854 2.4927 2.5017 2.5180

2.50 2.50

2.5238 2.5204 2.5187 2.5203 2.5279 2.5533 2.5607 2.5970 2.52 2.4976 2.4868 2.4846 2.4873 2.4912 2.5023 2.5160 2.5347 2.4985 2.4861 2.4812 2.4830 2.4916 2.4979 2.5151 2.5270

2.48

2.5318 2.5251 2.5242 2.5215 2.5333 2.5474 2.5683 2.6079 2.5047 2.4919 2.4894 2.4862 2.4962 2.4973 2.5212 2.5239 2.48 2.5044 2.4924 2.4853 2.4851 2.4937 2.5039 2.5236 2.5279

Learning Rate Learning Rate Learning Rate

Figure 10: Loss landscapes visualized as heatmaps across learning rate (x-axis) and batch size (y-axis) configurations.
Darker colors indicate lower loss values. Shows results for models with constant computational complexity M,
exhibiting slightly more variance in optimal hyperparameter regions.

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|2.4570|2.4515|2.4504|2.4525|2.4602|2.4732|2.4997|2.5462|
|2.4518|2.4449|2.4393|2.4350|2.4370|2.4465|2.4642|2.4964|
|||||||||
|2.4563|2.4470|2.4399|2.4371|2.4346|2.4403|2.4582|2.4732|
|||||||||
|2.4657|2.4541|2.4468|2.4413|2.4384|2.4523|2.4691|2.4970|
|2.4741|2.4604|2.4497|2.4436|2.4453|2.4525|2.4633|2.4852|

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|2.4681|2.4653|2.4673|2.4674|2.4814|2.5011|2.5250|2.5611|
|2.4620|2.4552|2.4522|2.4492|2.4531|2.4636|2.4825|2.5085|
|||||||||
|2.4665|2.4576|2.4525|2.4485|2.4476|2.4566|2.4678|2.4848|
|||||||||
|2.4761|2.4626|2.4546|2.4523|2.4524|2.4672|2.4742|2.4939|
|2.4852|2.4696|2.4613|2.4541|2.4583|2.4687|2.4791|2.4849|

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|2.4713|2.4692|2.4689|2.4692|2.4832|2.5085|2.5228|2.5701|
|2.4698|2.4651|2.4592|2.4546|2.4572|2.4660|2.4822|2.5146|
|||||||||
|2.4792|2.4682|2.4615|2.4566|2.4507|2.4604|2.4727|2.4904|
|||||||||
|2.4889|2.4760|2.4686|2.4611|2.4600|2.4683|2.4817|2.4992|
|2.4946|2.4838|2.4756|2.4651|2.4667|2.4742|2.4856|2.5034|

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|2.5203|2.5197|2.5251|2.5321|2.5555|2.5710|2.5956|2.6321|
|||||||||
|2.5125|2.5097|2.5089|2.5168|2.5302|2.5376|2.5515|2.5672|
|||||||||
|2.5162|2.5138|2.5145|2.5151|2.5204|2.5381|2.5428|2.5555|
|2.5238|2.5204|2.5187|2.5203|2.5279|2.5533|2.5607|2.5970|
|2.5318|2.5251|2.5242|2.5215|2.5333|2.5474|2.5683|2.6079|

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|2.4913|2.4886|2.4938|2.5002|2.5193|2.5437|2.5794|2.6135|
|||||||||
|2.4858|2.4803|2.4790|2.4824|2.4950|2.5055|2.5261|2.5551|
|||||||||
|2.4895|2.4813|2.4796|2.4828|2.4950|2.4988|2.5101|2.5133|
|2.4976|2.4868|2.4846|2.4873|2.4912|2.5023|2.5160|2.5347|
|2.5047|2.4919|2.4894|2.4862|2.4962|2.4973|2.5212|2.5239|

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|2.4939|2.4906|2.4925|2.4997|2.5148|2.5360|2.5793|2.6123|
|2.4884|2.4838|2.4769|2.4785|2.4888|2.5036|2.5253|2.5362|
|||||||||
|2.4882|2.4814|2.4772|2.4756|2.4854|2.4927|2.5017|2.5180|
|||||||||
|2.4985|2.4861|2.4812|2.4830|2.4916|2.4979|2.5151|2.5270|
|2.5044|2.4924|2.4853|2.4851|2.4937|2.5039|2.5236|2.5279|


count N, are more influential in determining optimal hyperparameter settings than specific architectural choices. This observation motivates our
discussion of hyperparameter scaling laws in relation to N in Section 4.2.


**A.2** **Model Structural Parameters**

**A.3** **Dense Models Results**

**A.4** **MOE Models Results**


-----

Model _N_ _D_ _dmodel_ _dff_ _Nhead_ _Nlayer_

0 2.15E+08 1.14E+10 960 9368 15 7
1 4.29E+08 5.00E+10 1280 9472 10 10
2 2.68E+08 8.00E+10 1024 9552 16 8
3 4.29E+08 8.00E+09 1280 9472 10 10
4 1.07E+09 2.00E+10 2048 8192 16 16
5 5.37E+08 1.00E+10 1280 9048 10 13
6 2.15E+08 4.00E+09 960 9368 15 7
7 2.68E+08 5.00E+09 1024 9552 16 8
8 2.68E+08 1.42E+10 1024 9552 16 8
9 1.07E+09 5.69E+10 2048 8192 16 16
10 2.15E+08 1.00E+11 960 9368 15 7
11 4.29E+08 2.27E+10 1280 9472 10 10
12 5.37E+08 2.84E+10 1280 9048 10 13
13 2.15E+08 2.00E+10 960 9368 15 7
14 4.29E+08 4.00E+10 1280 9472 10 10
15 2.68E+08 2.50E+10 1024 9552 16 8
16 5.37E+08 5.00E+10 1280 9048 10 13
17 1.07E+09 1.00E+11 2048 8192 16 16

Table 4: Dense Model Configuration.

Model _N_ _D_ _dmodel_ _Nhead_ _Nlayer_ _Nexpert_ _dmoe_ Top-k _Na_

0 2150612992 2000000000 1408 11 16 89 352 1 187973632
1 2155174912 2000000000 1408 11 16 8 3528 1 590436352
2 2156188672 2000000000 1408 11 16 8 2888 3 1241270272
3 2150612992 4000000000 1408 11 16 89 352 1 187973632
4 2155174912 4000000000 1408 11 16 8 3528 1 590436352
5 2156188672 4000000000 1408 11 16 8 2888 3 1241270272
6 2150612992 8000000000 1408 11 16 89 352 1 187973632
7 2155174912 8000000000 1408 11 16 8 3528 1 590436352
8 2156188672 8000000000 1408 11 16 8 2888 3 1241270272
9 2150612992 20000000000 1408 11 16 89 352 1 187973632
10 2155174912 20000000000 1408 11 16 8 3528 1 590436352
11 2156188672 20000000000 1408 11 16 8 2888 3 1241270272

Table 5: MoE Model Configuration.

**Dataset** **Baseline** **Code+Math** **More Code+Math** **En-CN**

web-data-en 79.53 44.75 20.00 44.99
web-data-cn – – – 34.52
code-the-stack 4.62 32.36 57.05 4.63
web-data-math – 7.07 7.07 –
book-non-novel-en 4.35 4.34 4.34 4.35
paper 3.38 3.37 3.37 3.38
wikipedia-mtlg 3.24 3.24 3.24 3.25
stackexchange 2.21 2.21 2.21 2.22
wikipedia-en 1.69 1.69 1.69 1.69
book-novel-en 0.83 0.83 0.83 0.83
wikipedia-cn 0.13 0.13 0.13 0.13

Table 6: Comparison of dataset weights (%) across different training recipes. Each recipe represents a different
focus: baseline (llama1), enhanced code and mathematics capability, and English-Chinese bilingual ability.


-----

106

105


N=429.261m, D=8.0b | dmodel = 1280, dff = 9472, Nlayer = 10, Nhead = 10

Global Minimum
Ours(Step Law)
Microsoft Law
DeepSeek Law
OpenAI Law
Porian Law

10 3 10 2

Learning Rate


2.54

2.52


2.46

2.44


2.50

2.48

|Global Ours(St Microso DeepSe OpenAI Porian L|Minimum ep Law) ft Law ek Law Law aw|
|---|---|
|||
|+0.500% +2.000%||
|||


Figure 11: Illustration of Hyperparameter Configuration
Space for Model 0.

N=429.261m, D=50.0b | dmodel = 1280, dff = 9472, Nlayer = 10, Nhead = 10

Global Minimum
Ours(Step Law) 2.36
Microsoft Law
DeepSeek Law
OpenAI Law
Porian Law

2.34

106

2.32

2.30

2.28

105

2.26

10 3 10 2

Learning Rate

Figure 12: Illustration of Hyperparameter Configuration
Space for Model 1.


Figure 14: Illustration of Hyperparameter Configuration
Space for Model 3.

N=1.074b, D=20.0b | dmodel = 2048, dff = 8192, Nlayer = 16, Nhead = 16

Global Minimum
Ours(Step Law)
Microsoft Law
DeepSeek Law 2.32
OpenAI Law
Porian Law

106 2.30

2.28

2.26

105 2.24

10 3 10 2

Learning Rate

Figure 15: Illustration of Hyperparameter Configuration
Space for Model 4.


2.50

2.48


106

105


N=536.873m, D=10.0b | dmodel = 1280, dff = 9048, Nlayer = 13, Nhead = 10

Global Minimum
Ours(Step Law)
Microsoft Law
DeepSeek Law
OpenAI Law
Porian Law

10 3 10 2

Learning Rate


2.46

2.44


2.42

2.40

|Global Ours(St Microso DeepSe OpenAI Porian L|Minimum ep Law) ft Law ek Law Law aw|
|---|---|
|||
|+1.000%||
|||


Figure 13: Illustration of Hyperparameter Configuration
Space for Model 2.


Figure 16: Illustration of Hyperparameter Configuration
Space for Model 5.


-----

2.74

2.72


2.20

2.19


106

105


N=214.664m, D=4.0b | dmodel = 960, dff = 9368, Nlayer = 7, Nhead = 15

Global Minimum
Ours(Step Law)
Microsoft Law
DeepSeek Law
OpenAI Law
Porian Law

+1.000%

10 3 10 2

Learning Rate


N=1.074b, D=56.9b | dmodel = 2048, dff = 8192, Nlayer = 16, Nhead = 16

+0.125%

+0.500%

Global Minimum

+1.000% Ours(Step Law)

+2.000% Microsoft Law

DeepSeek Law
OpenAI Law
Porian Law

4 × 10 4 6 × 10 4 10 3 2 × 10 3

Learning Rate


2.18

2.17


2.70

2.68


2.16

2.15


2.66

2.64


106

105


2.14

2.13

|Col1|Col2|Global Minimum Ours(Step Law) Microsoft Law DeepSeek Law OpenAI Law Porian Law|
|---|---|---|
||||
|+2.000% +0.250% +1.000|+0.500% %||
||||

|Col1|+2.000% +|1.000% +0.500%|
|---|---|---|
||+1.000%|+0.125% +0.250% +0.500% Global Minimum Ours(Step Law) Microsoft Law +2.000% DeepSeek Law|
|||OpenAI Law Porian Law|


Figure 17: Illustration of Hyperparameter Configuration
Space for Model 6.


Figure 20: Illustration of Hyperparameter Configuration
Space for Model 9.


2.68

2.66


2.60

2.58


106

105


N=268.304m, D=5.0b | dmodel = 1024, dff = 9552, Nlayer = 8, Nhead = 16

Global Minimum
Ours(Step Law)
Microsoft Law
DeepSeek Law
OpenAI Law
Porian Law

10 3 10 2

Learning Rate


N=214.664m, D=11.4b | dmodel = 960, dff = 9368, Nlayer = 7, Nhead = 15

Global Minimum
Ours(Step Law)
Microsoft Law
DeepSeek Law
OpenAI Law
Porian Law

10 3 10 2

Learning Rate


2.64

2.62


2.56

2.54


2.60

2.58


106

105


2.52

2.50


2.56

|Col1|Col2|Global Minimum Ours(Step Law) Microsoft Law DeepSeek Law OpenAI Law Porian Law|
|---|---|---|
||||
|+2.000% +0.500%|+1.000%||
||||

|Col1|Col2|Global Minimum Ours(Step Law) Microsoft Law DeepSeek Law OpenAI Law Porian Law|
|---|---|---|
||+2.000% +0.250% +0.500%|+1.000%|
||||


Figure 18: Illustration of Hyperparameter Configuration
Space for Model 7.


Figure 21: Illustration of Hyperparameter Configuration
Space for Model 10.


2.54

2.52


106

105


N=268.304m, D=14.2b | dmodel = 1024, dff = 9552, Nlayer = 8, Nhead = 16

Global Minimum
Ours(Step Law)
Microsoft Law
DeepSeek Law
OpenAI Law
Porian Law

10 3 10 2

Learning Rate


2.46

2.44


N=429.261m, D=22.7b | dmodel = 1280, dff = 9472, Nlayer = 10, Nhead = 10

Global Minimum
Ours(Step Law)
Microsoft Law
DeepSeek Law
OpenAI Law
Porian Law

10 3 10 2

Learning Rate


2.42

2.40


2.50

2.48


2.38

2.36


106

105


2.34

|Col1|Col2|Global Minimum Ours(Step Law) Microsoft Law DeepSeek Law OpenAI Law Porian Law|
|---|---|---|
||||
|+0.500% +1.000% +0|.125% +0.250% +2.000%||
||||

|+2.000%|Global Minimum Ours(Step Law) Microsoft Law DeepSeek Law OpenAI Law Porian Law|
|---|---|
|+0 +0.500% +1.000%|.250%|
|||


Figure 19: Illustration of Hyperparameter Configuration
Space for Model 8.


Figure 22: Illustration of Hyperparameter Configuration
Space for Model 11.


-----

106

105


N=536.873m, D=28.4b | dmodel = 1280, dff = 9048, Nlayer = 13, Nhead = 10

Global Minimum
Ours(Step Law)
Microsoft Law
DeepSeek Law
OpenAI Law
Porian Law

10 3 10 2

Learning Rate


2.36

2.34


2.32

2.30


2.28

|Col1|Global Ours(St Microso DeepSe OpenAI Porian|Minimum ep Law) ft Law ek Law Law Law|
|---|---|---|
||||
|+0.250% +0.500% +1.000%|||
|+2.000%|||


Figure 23: Illustration of Hyperparameter Configuration
Space for Model 12.


106

105


N=214.664m, D=20.0b | dmodel = 960, dff = 9368, Nlayer = 7, Nhead = 15

Global Minimum
Ours(Step Law)
Microsoft Law
DeepSeek Law
OpenAI Law
Porian Law

10 3 10 2

Learning Rate


2.56

2.54


2.48

2.46


2.52

2.50


Figure 26: Illustration of Hyperparameter Configuration
Space for Model 15.

N=536.873m, D=50.0b | dmodel = 1280, dff = 9048, Nlayer = 13, Nhead = 10

Global Minimum
Ours(Step Law) 2.32
Microsoft Law
DeepSeek Law
OpenAI Law
Porian Law 2.30

106

2.28

2.26

2.24

105

2.22

10 3 10 2

Learning Rate

Figure 27: Illustration of Hyperparameter Configuration
Space for Model 16.

|0%|Global Minimum Ours(Step Law) Microsoft Law DeepSeek Law|
|---|---|
|+2.00 +1.000% +0.250% +0.500%|OpenAI Law Porian Law|
|+2.000%||


Figure 24: Illustration of Hyperparameter Configuration
Space for Model 13.


2.38

2.36


106

105


N=429.261m, D=40.0b | dmodel = 1280, dff = 9472, Nlayer = 10, Nhead = 10

Global Minimum
Ours(Step Law)
Microsoft Law
DeepSeek Law
OpenAI Law
Porian Law

10 3 10 2

Learning Rate


2.30

2.28


2.34

2.32

|+2.000% +1.000% 250%|Global Ours(St Microso DeepSe OpenAI Porian|Minimum ep Law) ft Law ek Law Law Law|
|---|---|---|
||||
|+0.125% +0. +0.5|00%||
||||


Figure 25: Illustration of Hyperparameter Configuration
Space for Model 14.


Figure 28: Illustration of Hyperparameter Configuration
Space for Model 17.


-----

Figure 29: Illustration of Hyperparameter Configuration
Space for MoE Model 0.

N=2.155b, D=2.0b | Na = 590.436m, Na/N = 0.27

106 Global Minimum

Ours(Step Law)
Microsoft Law 2.70
DeepSeek Law
OpenAI Law

2.68

2.66

2.64

2.62

105

2.60

10 3

Learning Rate

Figure 30: Illustration of Hyperparameter Configuration
Space for MoE Model 1.

N=2.156b, D=2.0b | Na = 1.241b, Na/N = 0.58

106 Global Minimum

Ours(Step Law) 2.68
Microsoft Law
DeepSeek Law
OpenAI Law

2.66

2.64

2.62

2.60

5
10 2.58

10 3

Learning Rate

Figure 31: Illustration of Hyperparameter Configuration
Space for MoE Model 2.


Figure 32: Illustration of Hyperparameter Configuration
Space for MoE Model 3.

N=2.155b, D=4.0b | Na = 590.436m, Na/N = 0.27

106 Global MinimumOurs(Step Law) 2.56

Microsoft Law
DeepSeek Law
OpenAI Law

2.54

2.52

2.50

2.48

105

2.46

10 3

Learning Rate

Figure 33: Illustration of Hyperparameter Configuration
Space for MoE Model 4.

N=2.156b, D=4.0b | Na = 1.241b, Na/N = 0.58

106 Global Minimum 2.54

Ours(Step Law)
Microsoft Law
DeepSeek Law 2.52
OpenAI Law

2.50

2.48

2.46

105 2.44

10 3 2.42

Learning Rate

Figure 34: Illustration of Hyperparameter Configuration
Space for MoE Model 5.


-----

Figure 35: Illustration of Hyperparameter Configuration
Space for MoE Model 6.

N=2.155b, D=8.0b | Na = 590.436m, Na/N = 0.27

106 2.41

2.40

2.39

2.38

Global Minimum
Ours(Step Law)
Microsoft Law 2.37
DeepSeek Law
OpenAI Law

2.36

2.35

105

2.34

10 3

Learning Rate

Figure 36: Illustration of Hyperparameter Configuration
Space for MoE Model 7.

N=2.156b, D=8.0b | Na = 1.241b, Na/N = 0.58

106

2.38

2.36

2.34

105 +0.250% Global MinimumOurs(Step Law) 2.32

Microsoft Law
DeepSeek Law
OpenAI Law

10 3

Learning Rate

Figure 37: Illustration of Hyperparameter Configuration
Space for MoE Model 8.


Figure 38: Illustration of Hyperparameter Configuration
Space for MoE Model 9.

N=2.155b, D=20.0b | Na = 590.436m, Na/N = 0.27

106

2.29

2.28

2.27

Global Minimum 2.26
Ours(Step Law)
Microsoft Law
DeepSeek Law 2.25
OpenAI Law

2.24

2.23

105

2.22

10 3

Learning Rate

Figure 39: Illustration of Hyperparameter Configuration
Space for MoE Model 10.

N=2.156b, D=20.0b | Na = 1.241b, Na/N = 0.58

2.25

106

2.24

2.23

2.22

2.21

+0.125%

2.20

Global Minimum

105 Ours(Step Law)Microsoft Law 2.19

DeepSeek Law
OpenAI Law 2.18

10 3

Learning Rate

Figure 40: Illustration of Hyperparameter Configuration
Space for MoE Model 11.


-----

