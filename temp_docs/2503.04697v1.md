## L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning

**Pranjal Aggarwal** **Sean Welleck**
Carnegie Mellon University Carnegie Mellon University


### Abstract


Reasoning language models have shown an uncanny ability to improve performance at test-time by “thinking longer”—that is, by generating longer
chain-of-thought sequences and hence using more compute. However,
the length of their chain-of-thought reasoning is not controllable, making
it impossible to allocate test-time compute to achieve a desired level of
performance. We introduce Length Controlled Policy Optimization (LCPO),
a simple reinforcement learning method that optimizes for accuracy and
adherence to user-specified length constraints. We use LCPO to train L1, a
reasoning language model that produces outputs satisfying a length constraint given in its prompt. L1’s length control allows for smoothly trading
off computational cost and accuracy on a wide range of tasks, and outperforms the state-of-the-art S1 method for length control. Furthermore, we
uncover an unexpected short chain-of-thought capability in models trained
with LCPO. For instance, our 1.5B L1 model surpasses GPT-4o at equal reasoning lengths. Overall, LCPO enables precise control over reasoning length,
allowing for fine-grained allocation of test-time compute and accuracy.[1]

### 1 Introduction


An emerging class of reasoning language
_models (OpenAI et al., 2024a; DeepSeek-AI_
et al., 2025) improve performance at testtime by thinking longer when solving complex problems—that is, by generating extended chain-of-thought (Wei et al., 2023)
sequences and in turn using more compute.
However, current reasoning models have
a key limitation: the length of their reasoning is uncontrolled, making it impossible
to allocate a test-time compute budget to
achieve a target performance level. In some
cases, sequences span tens of thousands of
tokens, wasting compute, while in others,
models stop too early on complex problems.


60%

50%


40%

30%


20%

10%

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|L|1-Max|||||||
|L1-Exa|ct|||||||
|||||||||
|||||||||
|||||U re|U re|ncontrolla asoning m|ble odels|
|||||||||
|||S1: Co||||e, but||
||||S1: Co|ntrollabl||e, but||
|||worse|worse|perform||ance||
|||||||||


512 1K 2K 4K 8K
Tokens Used


Figure 1: L1 is a length-controllable reasoning

Recent methods like S1 (Muennighoff et al.,

model, that achieves higher performance per

2025) attempt to achieve length control by

token than other methods.

forcing a model to generate special tokens
(e.g., “Wait”, “Final Answer”) when the generation is too short or too long. However, this rigid, hand-engineered strategy severely
degrades performance compared to the underlying model (Figure 1). Other work investigates controlling output lengths in instruction following and general domains (Butcher
et al., 2024; Yuan et al., 2024). However, reasoning models face fundamentally new challenges such as much longer output lengths and the need to trade off computational cost for
improved performance.

[1Code and models released at https://cmu-l3.github.io/l1](https://cmu-l3.github.io/l1)


-----

We propose Length Controlled Policy Optimization (LCPO), a simple reinforcement learning
(RL)-based method that gives reasoning language models precise and adaptive length
control. LCPO trains models to satisfy two objectives: (1) correctness of the final output, and
(2) generating reasoning sequences that meet a length constraint specified in the prompt. In
doing so, LCPO-trained models learn to satisfy length constraints while optimizing reasoning
performance rather than relying on hand-engineered heuristics.

We experiment with two practical constraints: (1) LCPO-Exact, which requires the generated
reasoning to be exactly equal to the target length, and (2) LCPO-Max, which requires the
output to be no longer than the target length. We use LCPO to fine-tune a 1.5B-parameter
reasoning model based on Qwen-Distilled-R1-1.5B (DeepSeek-AI et al., 2025; Qwen et al.,
2025), producing L1-Max and L1-Exact. Our L1 models can precisely trade off token budget
and reasoning performance, smoothly interpolating between short, efficient reasoning and
longer, more accurate reasoning by simply prompting the model with different length
constraints (Figure 1). Crucially, one point on this trade-off curve recovers the original
base model’s performance, while outperforming S1 (Muennighoff et al., 2025) in performance across the entire range of reasoning lengths (Figure 1). On math reasoning tasks, L1
outperforms S1 by up to 100% relative and 20% absolute, under identical conditions.

Beyond improved length control in the standard math reasoning setting, we find that
LCPO-trained models generalize surprisingly well to out-of-distribution tasks, including
logical reasoning, and general-knowledge benchmarks like MMLU (Hendrycks et al., 2021a).
Furthermore, we show that “long-CoT” models trained with LCPO become unexpectedly
strong “short-CoT” models: when prompted to generate short reasoning traces, LCPOtrained models outperform their original counterparts by significant margins (up to 10%
improvement), even at the same generation length. To the best of our knowledge, for the
first time we show that a 1.5B model can match the performance of GPT-4o (OpenAI et al.,
2024b) despite using the same token budget. In summary, our contributions are:

   - We introduce Length Controlled Policy Optimization (LCPO), the first reinforcement learning-based method for training reasoning language models that produce
outputs adhering to user-specified length constraints.

   - We use LCPO to train L1, which demonstrates high degree of length control and
achieves state-of-the-art reasoning accuracy at fixed token budgets on challenging
math reasoning benchmarks.

   - We show length-control of L1 generalizes beyond math reasoning tasks to diverse
out-of-distribution tasks, including logical reasoning, and general-domain benchmarks (MMLU).

   - We demonstrate that LCPO-trained models can act as strong short-CoT models,
significantly outperforming their non-reasoning counterparts and much larger
models such as GPT-4o, despite using the same token budget.

### 2 Related Work

**Test-Time Scaling in Large Language Models.** Increasing test-time computation has consistently been shown to improve performance in complex reasoning tasks, mathematical
problem-solving, and code generation (Wu et al., 2024; Wang et al., 2023; Wei et al., 2023;
DeepSeek-AI et al., 2025; Snell et al., 2024). Test-time scaling laws indicate predictable performance gains from increasing inference computation, either by generating more reasoning
chains or longer ones (Wu et al., 2024; Snell et al., 2024; OpenAI et al., 2024a). Prominent
approaches include parallel sampling of multiple reasoning paths (Wang et al., 2023; Aggarwal et al., 2023), tree-based search (Yao et al., 2023; Wu et al., 2024; Xin et al., 2024),
and iterative refinement techniques (Welleck et al., 2023; Madaan et al., 2023; Snell et al.,
2024; Welleck et al., 2024). Recent reasoning language models such as “O1” and “R1”-style
models (OpenAI et al., 2024a; DeepSeek-AI et al., 2025) simplify test-time scaling by generating extended reasoning traces (longer chains-of-thought). Despite their promising results,
these methods lack precise and dynamic control over the length of the generated reasoning
chains, resulting in often suboptimal performance or unrealized potential efficiency gains.


-----

Our work complements and extends this line of research by enabling reasoning models to
precisely control the length of generated outputs, thereby providing flexibility to calibrate
inference compute based on task-specific requirements.

**Length Control in Large Language Models.** Controlling the length of LLM-generated
outputs is an important practical consideration across various generation tasks. Approaches
proposed thus far include architectural modifications—such as manipulating positional
encodings for exact sequence-length generation (Butcher et al., 2024)—training objective
adjustments to explicitly enforce length constraints (Jie et al., 2023; Singhal et al., 2024), or
directly training models on instruction-style data explicitly labeled with desired output
lengths (Yuan et al., 2024). Previous works on length control largely fall into two use-case
categories. The first aims primarily to reduce unnecessary verbosity (as often desired
in RLHF-tuned instruction-following models) while the second aims either to impose
maximum length budgets or achieve precise token-level length adherence (Jie et al., 2023;
Yuan et al., 2024; Singhal et al., 2024). However, existing methods predominantly focus
on general-purpose text generation or instruction-following contexts, where cost-quality
efficiency trade-offs are less critical or remain unaddressed (Jie et al., 2023; Yuan et al., 2024;
Butcher et al., 2024). Our work addresses the new challenges present in reasoning models.

Length control specifically tailored to reasoning tasks remains relatively unexplored. Recent
works, such as those by Arora & Zanette (2025) and Kang et al. (2024), emphasize generating
shorter reasoning chains for efficiency, but they do not enable explicit length control or
precise alignment with user-specified inference budgets. Another work S1 (Muennighoff
et al., 2025) introduces “budget-forcing” by imposing a strict token limit: either truncating
output at budget exhaustion or inserting a special token (“Wait”) to request continued
generation until reaching the full length budget. Unfortunately, this strategy presents
significant practical drawbacks. Abrupt truncation often interrupts reasoning mid-step,
negatively impacting model accuracy and user interpretability. Meanwhile, the repetitive
usage of special continuation tokens risks rigid and suboptimal reasoning patterns.

In contrast to these prior works, our LCPO is uniquely designed to train reasoning-specialized
models for precise and adaptive length control. LCPO uses reinforcement learning so that
models learn to dynamically allocate inference compute based on constraints provided in a
prompt. As our experiments will demonstrate, our method substantially surpasses previous
approaches in precision over length control, and performance at varying length budgets.

### 3 Method

Current reasoning language models lack an explicit mechanism for controlling the length of
their generated reasoning traces. This limitation prevents users and downstream applications from explicitly calibrating the inference compute budget (number of generated tokens)
according to task-specific requirements or available computational resources.

In this work, we address this limitation by conditioning the model on a target token length
provided in the prompt. Formally, given an input prompt x and a target length ngold,
the model is expected to generate a response y whose length ny minimizes the absolute
difference |ngold − _ny| while simultaneously producing the correct answer. This formulation_
directly couples accuracy with output length, ensuring that the generated chain-of-thoughts
adhere to user-specified constraints.

**Length Controlled Policy Optimization.** We begin with a pre-trained reasoning language
model LLMθ and a dataset D = {(xi, ygold,i)}i[N]=1[, where each instance contains only the]
input prompt and the final answer (i.e., no intermediate reasoning traces). To enable length
control, each prompt xi is augmented by appending a target length instruction. In particular,
we form
� �
_xi[new]_ = Concat _xi, “Think for ngold,i tokens.”_,

where ngold,i is sampled uniformly from Z(nmin, nmax). This augmentation yields a new
dataset D[new] = {(xi[new], ygold,i)}i[N]=1[.]


-----

We then update LLMθ using a reinforcement learning objective. In our experiments we
adopt GRPO (Shao et al., 2024) (though the method is compatible with other RL algorithms).
Our reward function combines two terms: a correctness reward rc and a length penalty
_rlength. It is defined as_

_r(y, ygold, ngold) = I(y = ygold) −_ _α ·_ ��ngold − _ny��,_ (1)

where I(·) is the indicator function, ny is the generated output length, and α is a scalar
that regulates the trade-off between generating the correct answer and meeting the target
length. In practice, a lower value of α prioritizes correctness when it is critical, whereas
a higher value enforces stricter adherence to the length constraint. Notably, the reward
function serves a dual purpose: (a) it encourages the model to produce correct answers
while implicitly favoring concise reasoning traces when shorter outputs are requested, and
(b) it consistently motivates the model to match the prescribed target length even when a
correct answer could be generated with fewer tokens. We refer to the model trained with
this objective as L1-Exact.

At inference, the output length is controlled by selecting a fixed target length ngold (or a set
of lengths) that is appended uniformly to every test prompt.

**Maximum Length Constraint Mode.** We further train a variant of L1 called L1-Max, which
flexibly generates outputs of varying lengths while respecting a maximum length constraint.
This approach is valuable when users prioritize staying within a computational budget
rather than adhering to exact generation lengths. To train L1-Max, we fine-tune the L1-Exact
model using the same RL framework but with a modified reward function:

_r(y, ygold, ngold) = I(y = ygold) · clip(α · (ngold −_ _ny) + δ, 0, 1),_ (2)

where α controls the penalty for length violations. This formulation applies a soft constraint
that (1) gradually penalizes outputs exceeding the target length rather than imposing a
hard cutoff (which is necessary to ensure gradient propagation in GRPO objective), and (2)
incentivizes the model to use fewer tokens when possible without sacrificing correctness.
The δ = 0.5 term ensures that correct answers with minor budget violations are still preferred
over incorrect answers. Further, L1-Max is trained with dual objective: when the prompt
requests an exact length, the model uses Equation 1; otherwise, it defaults to the maximum
constraint mode using Equation 2.

### 4 Experimental Setup

**Models and Datasets.** We conduct training on the DeepScaleR-Preview-Dataset (Luo
et al., 2025), a mathematics dataset consisting of 40K question-answer pairs drawn from
AIME, AMC, Omni-Math (Gao et al., 2024) and STILL (Min et al., 2024). We evaluate
our models on test sets of 4 different reasoning datasets: AIME 2025, MATH (Hendrycks
et al., 2021b), AMC, Olympiad-Bench (He et al., 2024), and additionally GPQA (Rein et al.,
2023), LSAT (Zhong et al., 2023), and MMLU (Hendrycks et al., 2021a). Our base model is
DeepScaleR-1.5B-Preview, a 1.5B-parameter model originally RL fine-tuned (from DeepSeekR1-Distill-Qwen-1.5B (DeepSeek-AI et al., 2025)) on this dataset with a 24K token context
length. Due to compute constraints, we restrict the maximum context length to 4K tokens
during training and to 8K tokens during evaluation. The model is further fine-tuned for
700 steps with LCPO-Exact objective (Equation 1), and the resulting model is referred to as
L1-Exact. The model is further RL finetuned for 120 steps with the objective mentioned in
Equation 2, and the resulting model is referred to as L1-Max.

**Baselines.** We evaluate our proposed method against the following baselines:

   - DeepSeek-R1-Distill-Qwen-1.5B: is the SFT version of Qwen-2.5-1.5B-Instruct
finetuned on reasoning traces of DeepSeek’s R1 model. For brevity, we refer to this
model as DeepSeek-R1-1.5B.

   - DeepScaleR-1.5B-Preview: the original model, evaluated without any length control modifications. For brevity, we refer this model as Agentica-24K.


-----

   - DeepScaleR-1.5B-Preview-4K: a version of Agentic-24K fine-tuned with 4K context
length. This is done due to computational constraints of training LCPO with long
sequence length (such as 24K used in Agentica-24K). The model therefore serves as
a fair comparison to L1. For brevity, we refer to this model as Agentica-4K.

   - S1: (Muennighoff et al., 2025) is a budget-forcing method, which controls reasoning
length using simple test-time interventions. We implement this method on top of
the Agentica-24K model.

**Evaluation Protocol.** We evaluate our approaches along two dimensions. First, we assess the model’s ability to adhere to the targeted length by reporting the mean deviation
between the generated token length ny and the target ngold. Second, we evaluate the overall
performance (i.e., problem-solving accuracy) when generating responses at different target
lengths. In our experiments, target lengths are selected from {512, 1024, 2048, 3600} tokens.

**Hyperparameters and Implementation Details.** For GRPO training, we adopt the same
hyperparameters as in DeepScaleR-1.5B-Preview. In particular, we use a learning rate of 1e-6
and a batch size of 128. The maximum context length is set to 4K tokens at training time and
extended to 8K tokens during evaluation. Training is performed for 700 steps using the VeRL
framework (MLSys, 2025). During training, the target length ngold is sampled uniformly
from U(nmin, nmax), where we set nmin = 100 and nmax = 4000. The balancing parameter α
in Equation 1 is fixed at 0.0003. Note that we did not conduct extensive hyperparameter
tuning, so one can expect further improvements with additional optimization.

### 5 Results and Analysis

In this section, we report and analyze the effectiveness of the proposed method (LCPO)
across various settings and benchmarks. We evaluate our method’s relative performance,
generalization capability on out-of-domain tasks, controllability of length constraints and
competitive performance in short Chain-of-Thought (CoT) setups, and examine learned
reasoning behaviors.

**L1 significantly outperforms other length-controlled models while maintaining strong**
**performance.** Figure 2 compares performance of L1-Exact and L1-Max with other baselines
across varying generation lengths. Both variants of L1 achieve superior performance across
all token budgets while maintaining precise length control. Compared to S1, the only
other method specifically designed for length control, L1 shows remarkable improvements,
over 100-150% relative and 20-25% absolute performance gains at both 512 and 1024 token
budgets. This substantial difference can be attributed to two key factors: (1) L1 intelligently
adapts its chain-of-thought to fit within specified length constraints without disrupting the
reasoning process, while S1 often truncates mid-reasoning; and (2) L1 is explicitly trained to
generate high-quality reasoning chains of varying lengths, effectively distilling reasoning
patterns from longer chains to shorter ones.

Moreover, with L1, we observe a log-linear scaling pattern, similar to the prior works O1 and
S1 by OpenAI—performance improves linearly with respect to the log-length of generated
reasoning chains. However, this scaling curve for L1 exhibits a notably smaller slope (0.24
vs. 0.37 slope of S1), indicating substantially improved effectiveness at lower token ranges.

L1-Exact performs approximately 1% below Agentica-4K, which is the same underlying
model as L1, but trained without length constraints. However, this difference is primarily
observed in the AIME dataset, where unconstrained models can generate very long chains
for complex problems. Additionally, L1-Exact allocates the same token budget to all problems regardless of difficulty, potentially using extra tokens on simpler problems. L1-Max
effectively alleviates this challenge, matching the performance of Agentica-4K by optimizing
token usage based on problem difficulty while respecting the upper ceiling. In doing so, it
outperforms even L1-Exact often by up to 2x fewer tokens. L1-Max is particularly valuable
when exact token counts are less desirable than a worst-case compute budget. Finally, the


-----

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
||||||
||||||
||||||
||||||
|512 1K 2K 4K|||||


Figure 2: Performance of L1 under varying token budgets. Pass rate (y-axis) across multiple
benchmarks as a function of tokens used (x-axis). L1-Exact and L1-Exact significantly outperform length-controlled baselines (S1) while approaching the performance of unconstrained
models.

scaling trends suggest that with longer context training, L1 would match or even surpass
Agentica-24K’s performance while maintaining a high degree of length control.

**L1 generalizes effectively to out-of-domain (OOD) tasks.** We evaluate L1’s ability to
generalize length control capabilities to domains outside its RL training distribution. We
categorize out-of-domain (OOD) datasets: general reasoning datasets GPQA and LSAT that
were not explicitly used in L1’s training but is likely within DeepSeek-R1-1.5B’s training domain and MMLU, which likely falls even outside DeepSeek-R1-1.5B’s training distribution.

Figure 3 confirms that L1 generalizes robustly to new domains: performance consistently
scales positively with token budget for OOD general reasoning datasets, approaching or
matching Agentica-4K benchmarks despite explicit length control constraints. For GPQA
and LSAT, we observe the same linear performance scaling trend as in our primary datasets,
with L1 matching Agentica-4K’s performance at comparable token budgets. This generalization is particularly impressive given that L1 was not explicitly trained on these tasks. For
MMLU, we see a less pronounced linear scaling relationship (R[2] = 0.66), likely because
these knowledge-focused questions benefit less from extended reasoning.

**L1 follows length constraints with high precision.** We quantitatively assess L1’s ability to
follow length constraints across various mathematical reasoning datasets. As shown in Figure 4, our model maintains consistent control across all token budgets (512, 1024, 2048, and
3600 tokens), with observed output lengths usually closely matching the requested lengths.

Further, in Figure 5, we show the mean error: ( _Ex∼D[ngeneratedngold_ ]−ngold ), which captures the aver
age deviation from target lengths across the dataset. The figure demonstrates that mean


-----

Figure 3: Out-of-domain (OOD) generalization of L1. L1 generalizes its length control to
out-of-domain tasks, showing linear scaling of pass rate (y-axis) over generation length
(x-axis) matching or exceeding performance of various baselines.


4000

3000


40.0%

30.0%


2000

1000


20.0%

10.0%

|Col1|Col2|Col3|Col4|Col5|O|Col7|O|OD|Col10|Col11|5%|Col13|
|---|---|---|---|---|---|---|---|---|---|---|---|---|
||||||||||||||
||||20.||8% 21.|||7%|||||
|Ma||ng|||||||||||
||th Reasoni||||||||||||
|2.2% 2.|7% 2.9|6. %|6%||||||||||


0.0%


0

|ure 3: Out-of-domain (OOD) generaliz -of-domain tasks, showing linear scali xis) matching or exceeding performanc 4648|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|
|---|---|---|---|---|---|---|---|---|---|---|---|
|MATH Olym AIME AMC||piad-Benc||h||46 4106||33||||
|Targe 1-99t||t Length h Percentil||es||||||||
|||||||3184 3261||61||||
||||||2 2458 2564|762 2640||||||
|||197 1607 1653 1025|||6 1747 1597 1561 1|616 1638||||||
|9|07 99|2|95|2 574 518 55|5 499|||||||
|241 24||9 281 321||||||||||


512 1024 2048 3600
Requested Length (tokens)


Figure 4: Length control precision across
different datasets. The plot show distribution of response lengths at different requested lengths. Our model maintains consistent length control, with output lengths
closely matching the requested lengths.


Figure 5: Mean Error in Length of gener**ated chains across different datasets.** L1
maintains around 3% average deviation on
math reasoning datasets and higher but acceptable deviation on OOD tasks.


error is low: close to 3% for all math reasoning datasets. Although OOD datasets exhibit
predictably higher errors (20-40%), these remain preferable over uncontrolled prompting.
Further Analysis in Appendix A.2 demonstrates that larger errors primarily appear at
higher token budgets on tasks like MMLU, where the longer chain of thoughts is mostly
unnecessary. Additionally, in Appendix A.1, we show ethat rror can be further reduced
significantly with extended RL training.


-----

AIME MATH AMC Olympid-Bench Average
Model

# Tokens Accuracy # Tokens Accuracy # Tokens Accuracy # Tokens Accuracy # Tokens Accuracy


Qwen-1.5 877 7.7 554 73.4 791 47.5 784 35.4 752 41.0
L1-Exact 880 9.2 583 70.6 801 49.7 770 **35.5** 758 41.2
L1-Max 858 **11.9** 541 **78.3** 757 **55.9** 722 **38.5** 720 **46.2**

Llama-3.3-70B 1024 3.8 573 74.9 838 48.6 859 37.3 824 41.2
L1-Exact 983 10.6 583 70.6 801 49.7 913 36.9 820 41.9
L1-Max 1059 **15.0** 631 **79.4** 921 **59.2** 853 **40.5** 866 **48.5**

GPT-4o 867 10.0 591 78.4 990 51.8 911 **42.4** 840 45.6
L1-Exact 880 10.0 583 70.6 1060 51.3 913 36.9 859 42.2
L1-Max 858 **11.9** 631 **79.4** 921 **59.2** 853 40.5 816 **47.8**

Table 1: Short CoT performance across math benchmarks. L1-Max surpasses its nonreasoning base model and competes with frontier models under identical short CoT lengths.

- Our analysis reveals model is significantly contaminated, See results on AIME2025 for
details.


**Long CoT Models are secretly Strong Short CoT Models.** Given L1’s strong performance
at lower token budgets, we conducted a focused evaluation comparing it to both its base nonreasoning model (Qwen-2.5-1.5B-Instruct) and significantly larger non-reasoning models
(GPT-4o and Llama-3.3-70B) at comparable generation lengths. Table 1 presents these results,
showing that L1 consistently outperforms or matches all models across all datasets despite
using equivalent token budgets. Further, on average, L1 is 5% better than its non-reasoning
counterpart, and even outperforms GPT-4o by 2% on average.

This finding is remarkable, as to the best of our knowledge, this is the first demonstration
that a 1.5B model can outperform frontier models such as GPT-4o, despite using the same
**_generation length. Overall, the results signify that with suitable RL training, long CoT_**
models can be adaptively used as short CoT models, while significantly outperforming their
base counterparts at the same generation length.


16

14


12

10


8

6


4

2


0


Keyword Usage Comparison by Reasoning Pattern: 512 vs 4096 Tokens

Model

13.9%

Self-correction & Verification Exploration & Alternatives Context Setting Conclusion

Drawing

9.1%

5.9% 6.0% 6.0% 6.2%

4.1%

3.7%

3.1% 3.1%

2.7% 2.7% 2.4%

2.0%

1.7%

1.1% 1.2%

0.2%

Keywords Grouped by Reasoning Pattern

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|Col18|Col19|Col20|Col21|Col22|Col23|Col24|Col25|Col26|Col27|Col28|Col29|Col30|Col31|Col32|l|Col34|Col35|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
||||||||||||||||||||||||||||||Mode|||l|||
|||||||||||||||||||||||||||||||||okens tokens|||
||||||||||||||||||||||||||||||||||||
||||||||||||||||||||||||||||1||3.9%||||||
|Self|-corr||ection & Ve|||rifica||tion||Explora|Explora|||tion & Alter||natives||||Co||ntext Setti|||ng||||Conc Dr|lusion awing|||||
||||||||||||||||||||||||||||||||||||
||||||||||||||||||||||||||||||||||||
||||||||||||||||||||||||||||||||||||
||||9.1%||||||||||||||||||||||||||||||||
||||||||||||||||||||||||||||||||||||
||5.6%|||||5.9%||||||||6.0%||6.0%||||||6.2%||||||||||6.7%|||
||||||||||||||||||||||||||||||||||||
|||||||||||4.1%||||||||||||||||||||3.7%|||||
|||||2.7%||||1.7%||||3.1%||||2.0%||||3.1%||2.7%|||2.4%||||||||||
||||||||||||||||||||||||||||||||||||
||||||||||||||||||||||||||||||||||||
|||||||||||||||0.2%|||||1.1%||||||||1.2%||||||||
||||||||||||||||||||||||||||||||||||
||||||||||||||||||||||||||||||||||||


Figure 6: Keyword usage by reasoning pattern at 512 vs. 4096 tokens. L1 allocates more
tokens to self-correction and conclusion drawing at higher budgets, reflecting adaptive
reasoning strategies.

**L1 employs distinct reasoning strategies at different token budgets.** To understand
how L1 changes its reasoning approach across different length constraints, we analyzed


-----

how frequently certain reasoning-related terms appear in outputs of different lengths.
Specifically, we calculated the normalized occurrence rate of most common reasoning terms
in 512-token outputs compared to 4096-token outputs, showing how the model’s reasoning
strategies shift when given different length constraints. Figure 6 organizes these keywords
into four distinct reasoning patterns: “Self-Correction and Verification,” “Exploration and
Alternatives,” “Context Setting,” and “Conclusion Drawing.”


Figure 6 shows that self-correction and
verification keywords appear approximately twice as frequently in 4096token outputs compared to 512-token
outputs. Similarly, conclusion-drawing
terms increase 2-10x with increased
token budget, indicating more thorough solution validation and completion. Interestingly, most explorationrelated keywords decrease in relative
frequency at higher token counts, with
“Alternatively” being a notable exception. Overall, we observe that smaller
CoTs have reasoning patterns similar
to their longer counterparts, but with
changed relative frequencies that favor
more self-verification and conclusion
drawing in longer chains-of-thought.


3500

3000


2500

2000


1500

1000


500

0

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Total:|Col12|Col13|3499|Col15|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
||||||||||||||||
||||Thinking L Solution L|||ength ength||||||10.|1%||
||||||||||||||||
||||||||||||||||
||||||||||||||||
|||||||Total: 16.||||1879 9%||89.|9%||
||||||||||16.|9%|||||
||||Total: 18.|||1229 7%|||||||||
||||||18.|7%|||||||||
|Total 17|||: 611 .6%||||||83.|1%|||||
|||17|.6%||||||||||||
|||82|.4%||81.|3%|||||||||


drawing in longer chains-of-thought. Figure 7: Distribution of tokens for thinking

**vs. solution across chain-of-thought lengths. L1**

Further, Figure 7 shows the ratio of

maintains a mostly consistent ratio of thinking

thinking tokens (those within <think>

tokens to solution tokens, indicating efficient scal
tags) to solution tokens for different gen
ing of intermediate steps without making final

eration lengths. We observe that the

answers overly verbose.

ratio is relatively stable across different generation lengths. This implies for
shorter CoTs, the model generally provides short solutions (often just outputting the final
answer), which helps save tokens. As generation length increases, we notice a stabilized
response length in the last two bars, implying the model scales its thinking tokens without
making the final solution overly verbose.


### 6 Conclusion

In this work, we introduced Length Controlled Policy Optimization (LCPO), a simple yet
powerful reinforcement learning-based method enabling adaptive control over the length of
reasoning chains in language models. We use LCPO to train L1, a reasoning language model,
optimized to produce outputs that adhere to length constraints given in its prompt. LCPO
significantly surpasses previous test-time scaling methods, achieving over 100% relative
and 20% absolute improvements in mathematical reasoning tasks compared to prior lengthcontrol approaches.

Further, we demonstrated that L1 generalizes robustly beyond its training distribution,
extending its length-controlled capabilities to out-of-domain tasks. Furthermore, our analysis revealed an intriguing phenomenon: models trained to generate longer reasoning
chains become unexpectedly strong short-CoT reasoners, outperforming significantly larger
frontier models such as GPT-4o at identical generation lengths. By providing length control
using simple prompt, LCPO opens promising avenues toward more efficient, flexible, and
scalable reasoning models.


-----

### References

Pranjal Aggarwal, Aman Madaan, Yiming Yang, and Mausam. Let‘s sample step by step:
Adaptive-consistency for efficient reasoning and coding with LLMs. In Houda Bouamor,
Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing, pp. 12375–12396, Singapore, December 2023.
Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.761. URL
[https://aclanthology.org/2023.emnlp-main.761/.](https://aclanthology.org/2023.emnlp-main.761/)

Daman Arora and Andrea Zanette. Training language models to reason efficiently, 2025.
[URL https://arxiv.org/abs/2502.04463.](https://arxiv.org/abs/2502.04463)

Bradley Butcher, Michael O’Keefe, and James Titchener. Precise length control in large
[language models, 2024. URL https://arxiv.org/abs/2412.11937.](https://arxiv.org/abs/2412.11937)

DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin
Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu,
Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan
Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu
Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong
Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu,
Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong
Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L.
Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin
Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang,
Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun
Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu
Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L.
Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu
Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu,
Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao,
Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An,
Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie,
Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin,
Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou,
Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao
Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong,
Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo,
Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo,
Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui
Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren,
Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao,
Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie,
Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang.
Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025.
[URL https://arxiv.org/abs/2501.12948.](https://arxiv.org/abs/2501.12948)

Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma,
Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran
Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang.
Omni-math: A universal olympiad level mathematic benchmark for large language
[models, 2024. URL https://arxiv.org/abs/2410.07985.](https://arxiv.org/abs/2410.07985)

Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi
Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong
Sun. Olympiadbench: A challenging benchmark for promoting agi with olympiad[level bilingual multimodal scientific problems, 2024. URL https://arxiv.org/abs/2402.](https://arxiv.org/abs/2402.14008)
[14008.](https://arxiv.org/abs/2402.14008)

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and
Jacob Steinhardt. Measuring massive multitask language understanding, 2021a. URL
[https://arxiv.org/abs/2009.03300.](https://arxiv.org/abs/2009.03300)


-----

Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang,
Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the
[math dataset, 2021b. URL https://arxiv.org/abs/2103.03874.](https://arxiv.org/abs/2103.03874)

Renlong Jie, Xiaojun Meng, Lifeng Shang, Xin Jiang, and Qun Liu. Prompt-based length
controlled generation with reinforcement learning. arXiv preprint arXiv:2308.12030, 2023.

Yu Kang, Xianghui Sun, Liangyu Chen, and Wei Zou. C3ot: Generating shorter chain-of[thought without compromising effectiveness, 2024. URL https://arxiv.org/abs/2412.](https://arxiv.org/abs/2412.11664)
[11664.](https://arxiv.org/abs/2412.11664)

Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang,
Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li,
Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview
with a 1.5b model by scaling rl. [https://pretty-radio-b75.notion.site/](https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2)
[DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2,](https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2)
2025. Notion Blog.

Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh,
and Peter Clark. Self-refine: Iterative refinement with self-feedback, 2023. URL
[https://arxiv.org/abs/2303.17651.](https://arxiv.org/abs/2303.17651)

Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang,
Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan
Wang, and Ji-Rong Wen. Imitate, explore, and self-improve: A reproduction report on
[slow-thinking reasoning systems, 2024. URL https://arxiv.org/abs/2412.09413.](https://arxiv.org/abs/2412.09413)

[Bytedance Seed MLSys. verl: Volcano engine reinforcement learning for llms. https:](https://github.com/volcengine/verl)
[//github.com/volcengine/verl, 2025. Accessed: 2025-02-28.](https://github.com/volcengine/verl)

Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi,
Luke Zettlemoyer, Percy Liang, Emmanuel Candes, and Tatsunori Hashimoto. s1: Simple`
[test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393.](https://arxiv.org/abs/2501.19393)

OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky,
Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex
Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei,
Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew
Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang,
Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz
Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming
Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger,
Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel
Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris
Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan
Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo,
Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming
Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman,
Hyung Won Chung, Ian Kivlichan, Ian O’Connell, Ian Osband, Ignasi Clavera Gilaberte,
Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pachocki, James Lennon,
Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu,
Joaquin Quinonero Candela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman,˜
John Rizzo, Jonathan Gordon, Jonathan Uesato, Jonathan Ward, Joost Huizinga, Julie
Wang, Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen, Karl Cobbe, Katy Shi, Kayla
Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu, Kevin Stone, Kevin Yu,
Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus, Lilian Weng,


-----

Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz
Kaiser, Luke Metz, Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko
Tintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet Yatbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael
Lampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov,
Miles Wang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat
McAleese, Neil Chowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown,
Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne,
Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo
Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown,
Roshan James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam Altman, Sam Toizer,
Sam Toyer, Samuel Miserendino, Sandhini Agarwal, Santiago Hernandez, Sasha Baker,
Scott McKinney, Scottie Yan, Shengjia Zhao, Shengli Hu, Shibani Santurkar, Shraman Ray
Chaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph Lin, Suchir Balaji, Suvansh
Sanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders,
Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng,
Timur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou,
Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko, Weiyi Zheng,
Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois, Yinghai Lu, Yining Chen,
Young Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao, and Zhuohan
[Li. Openai o1 system card, 2024a. URL https://arxiv.org/abs/2412.16720.](https://arxiv.org/abs/2412.16720)

OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat,
Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao,
Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,
Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie
Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke
Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen,
Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings,
Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien
Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty
Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simon Posada Fishman,´
Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun
Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott
Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han,
Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade
Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost
Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun
Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali
Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick,
Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt
Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis,
Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike,
Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz
Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam
Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne,
Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil,
David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela
Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg
Murk, David Mely, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan,´
Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex
Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy
Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila
Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny,
Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth
Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario


-----

Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr,
John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah
Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina
Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski
Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry
Tworek, Juan Felipe Ceron Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss,´
Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,
CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff,
Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan,
Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao
Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024b.
[URL https://arxiv.org/abs/2303.08774.](https://arxiv.org/abs/2303.08774)

Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,
Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu,
Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming
Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men,
Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang
Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan
[Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115.](https://arxiv.org/abs/2412.15115)

David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang,
Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: A graduate-level google[proof q&a benchmark, 2023. URL https://arxiv.org/abs/2311.12022.](https://arxiv.org/abs/2311.12022)

Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,
Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of
[mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/](https://arxiv.org/abs/2402.03300)
[2402.03300.](https://arxiv.org/abs/2402.03300)

Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A long way to go: Investigat[ing length correlations in rlhf, 2024. URL https://arxiv.org/abs/2310.03716.](https://arxiv.org/abs/2310.03716)

Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute
[optimally can be more effective than scaling model parameters, 2024. URL https://](https://arxiv.org/abs/2408.03314)
[arxiv.org/abs/2408.03314.](https://arxiv.org/abs/2408.03314)

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha
Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in
[language models, 2023. URL https://arxiv.org/abs/2203.11171.](https://arxiv.org/abs/2203.11171)

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi,
Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large lan[guage models, 2023. URL https://arxiv.org/abs/2201.11903.](https://arxiv.org/abs/2201.11903)

Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi,
and Yejin Choi. Generating sequences by learning to self-correct. In The Eleventh
[International Conference on Learning Representations, 2023. URL https://openreview.](https://openreview.net/forum?id=hH36JeQZDaO)
[net/forum?id=hH36JeQZDaO.](https://openreview.net/forum?id=hH36JeQZDaO)

Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham
Neubig, Ilia Kulikov, and Zaid Harchaoui. From decoding to meta-generation: Inference[time algorithms for large language models, 2024. URL https://arxiv.org/abs/2406.](https://arxiv.org/abs/2406.16838)
[16838.](https://arxiv.org/abs/2406.16838)

Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling
laws: An empirical analysis of compute-optimal inference for problem-solving with
[language models, 2024. URL https://arxiv.org/abs/2408.00724.](https://arxiv.org/abs/2408.00724)


-----

Huajian Xin, Z. Z. Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu,
Liyue Zhang, Xuan Lu, Qiushi Du, Wenjun Gao, Qihao Zhu, Dejian Yang, Zhibin Gou,
Z. F. Wu, Fuli Luo, and Chong Ruan. Deepseek-prover-v1.5: Harnessing proof assistant
[feedback for reinforcement learning and monte-carlo tree search, 2024. URL https:](https://arxiv.org/abs/2408.08152)
[//arxiv.org/abs/2408.08152.](https://arxiv.org/abs/2408.08152)

Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and
Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language
[models, 2023. URL https://arxiv.org/abs/2305.10601.](https://arxiv.org/abs/2305.10601)

Weizhe Yuan, Ilia Kulikov, Ping Yu, Kyunghyun Cho, Sainbayar Sukhbaatar, Jason Weston,
[and Jing Xu. Following length constraints in instructions, 2024. URL https://arxiv.org/](https://arxiv.org/abs/2406.17744)
[abs/2406.17744.](https://arxiv.org/abs/2406.17744)

Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin
Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating
[foundation models, 2023. URL https://arxiv.org/abs/2304.06364.](https://arxiv.org/abs/2304.06364)


-----

### A Results

**A.1** **Extended training further improves length constraint precision.**

Table 2: Length Error Comparison Between Methods

**Method** **Mean Error (%)↓** **RMSE Error (%)↓**

**Math Reasoning**
L1-Exact **3.01** 18.44
L1-Exact + 3.15 **10.04**

**OOD-1 (General Reasoning)**
L1-Exact 21.22 31.37
L1-Exact + **10.89** **16.77**

**OOD-2 (General knowledge)**
L1-Exact 40.54 42.61
L1-Exact + **23.57** **33.48**

AMC Olympiad-Bench AIME

80%

70% 50% 35.0%

30.0%

60%

40%

25.0%

50%

40% 30% 20.0%

30% 15.0%

20%

20% 10.0%

10% 10% 5.0%

0.0%

512 1K 2K 4K 512 1K 2K 4K 512 1K 2K 4K

MATH Macro Average

90%

60%

80% Our Methods:

50% Our RL-trained Model

70%

Our RL-trained Model

60% 40% (More Training)

50% 30% Baselines:

40% 20% S1 (Budget Forcing)

Agentica-4K

30% 10%

512 1K 2K 4K 512 1K 2K 4K
Tokens Used Tokens Used

Figure 8: Performance vs Length for LCPO with more training

Motivated by the remaining gap in length control precision, we investigate if further RL
training would improve length adherence. In particular, we further fine-tune L1-Exact for
500 RL steps. We refer to the derived model as L1-Exact+ Interestingly, Table 2 reveals
substantial precision improvements—particularly steep RMSE reductions—from additional
training across both math and OOD datasets. Yet, greater length precision appears to slightly
lower performance at high token ranges (Figure 8). This effect likely stems from the model’s
stricter adherence to length constraints, which reduces its flexibility to generate longer CoTs
to more challenging problems. Nevertheless, the model maintains its overall performance
trends, suggesting an interesting direction for future work: enabling users to dynamically


-----

60%

50%

|44|.7%|
|---|---|

|4|4.9%|
|---|---|

|41|.2%|
|---|---|


40%

30%

|23|.2%|
|---|---|


20%

10%

|15|.1%|
|---|---|

|9.|9%|
|---|---|

|1|.9%|
|---|---|


0%

|Col1|Col2|Col3|Col4|6|0.0%|Col7|Col8|Col9|Col10|
|---|---|---|---|---|---|---|---|---|---|
|||||||||||
||48||.9%||||L|L|GPQA SAT MMLU|
|||48|.9%|||||||
|44|.7% 41||.2%|4|4.9%|||||
||||||||||37.7%|
|23|.2%|||||||||
|9.|15 9%||.1%||||||9.0%|
|||||1|.9%|||||
|||||||||||


500 1000 1500 2000 2500 3000 3500
Requested Token Length


Figure 9: Length error vs token length for OOD datasets. Datasets like MMLU show higher
error for longer chains, likely because longer chains are not particularly useful for these
datasets.

control the tradeoff between strict length adherence and optimal performance based on
their specific requirements.


**A.2** **LCPO follows length constraints with high precision.**

In Section 5, we show that LCPO follows length constraints with high precision, although
with larger errors in OOD datasets. In this section, we particularly analyze the generation
length at which error occurs. Figure 9 shows error as a function of generated token length
for 3 different OOD datasets.


Model AIME2024 AIME2025 Delta

Qwen-Math-1.5 12.9 7.7 -5.2
GPT-4o 6.7 10.0 +3.3
LLama-3.3-70B 27.3 3.8 -23.5
DeepSeek-R1-1.5B 29.2 21.7 -7.5
Agentica-24K **40.2** 27.3 -12.9
Agentica-4K 32.1 25.4 -6.7

L1-Exact (ours) 24.8 22.5 -2.3
L1-Max (ours) 27.3 21.5 -5.8
L1-Exact (1024 tokens) 10.2 12.3 +2.1
L1-Max (1024 tokens) 16.3 11.9 -4.4


Table 3: Comparison of model performance on AIME2024 and AIME2025 benchmarks.

**Memorization or Generalization?** Given the surprisingly strong performance of LCPO,
particularly at lower token budgets, a natural question arises regarding whether the model
has memorized solutions, suggesting potential train-test leakage. To address this concern,


-----

Figure 10: Parallel vs Sequential Scaling

we specifically evaluate on the AIME2025 dataset, an exam released after the training of
all evaluated models. Results in Table 3 demonstrate that the performance drop for LCPO
is substantially smaller (≈ 2%) compared to other models such as Agentica-24K (-12.9%),
DeepSeek-R1-1.5B (-7.5%), and even Llama-3.3-70B (-23.5%). These results provide strong
evidence that despite using shorter CoT lengths, our model demonstrates generalization
rather than relying solely on memorization.

**A.3** **Parallel vs Sequential Scaling**

Figure 10 compares how sequential scaling (increasing CoT length) and parallel scaling
(Majority Voting) vary in performance for the same incurred cost (total tokens generated).
There are a few interesting observations from the graph: First, parallel scaling is almost
always worse than sequential scaling, when compared at the same token budget. Second,
parallel scaling can provide significant improvements, to all reasoning models (eg, more
than 10% in some cases). Third, while Qwen-2.5-1.5B-Math, barely benefits from majority
voting, L1-Max (1024 tokens), despite using similar number of generation tokens in each
task, benefits significantly. This is noteworthy, since for many cases, it is desirable to have
low token lengths, because of both reduced latency, and avoiding the quadratic cost of
generation. In such cases, LCPO-Max clearly severs as a better alternative to parallel scaling
of non-reasoning models. Finally, LCPO-Max (3600 tokens) matches the performance of
Agentica-4K baseline across parallel scaling. We do note a small dip in the last point, but
this is because only 1 seed was used for parallel scaling, and as such the difference is not
statistically significant.

**A.4** **Qualitative Examples**

In Figures 11 and 12, we show two examples of L1-Exact solving the same problem but with
different requested generation lengths: 512 and 3600 tokens. While the model with 512


-----

tokens is unable to solve the problem correctly, the model with longer generation arrives
at the correct answer. We refer interested readers to our website, which has more such
interesting examples.


-----

Figure 11: Example of model response with 512 tokens. Note that the model’s answer is
incorrect.


-----

Figure 12: Example of model response with 3600 tokens. With more tokens, the model
correctly calculates the answer.


-----

